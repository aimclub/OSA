{
    "albumentations/albumentations/__init__.py": [],
    "albumentations/albumentations/augmentations/__init__.py": [],
    "albumentations/albumentations/augmentations/bbox_utils.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "default_data_name",
                    "second_doc": "\"\"\"\nProvides the default key name under which bounding box data is expected or stored during image augmentation and transformation processes.\n\nReturns:\n    str: The string identifier used as the standard key for accessing bounding box information.\n\"\"\"",
                    "source_code": "return \"bboxes\""
                },
                {
                    "docstring": null,
                    "method_name": "ensure_data_valid",
                    "second_doc": "\"\"\"\nValidate that all required data fields related to bounding boxes and labels are present and correctly structured in the input data. This ensures consistency between bounding boxes and their associated labels before further processing or augmentation, helping to prevent errors caused by improperly labeled or unmatched data.\n\nArgs:\n    data (dict): The input data dictionary expected to contain bounding box and label fields as specified in the processor's parameters.\n\nRaises:\n    ValueError: If label fields are not properly specified or if bounding boxes lack the corresponding labels, or if the provided label field names do not match expected keys.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "for data_name in self.data_fields:\n            if data.get(data_name) and len(data[data_name][0]) < 5:\n                if self.params.label_fields is None:\n                    raise ValueError(\n                        \"Please specify 'label_fields' in 'bbox_params' or add labels to the end of bbox \"\n                        \"because bboxes must have labels\"\n                    )\n        if self.params.label_fields:\n            if not all(l in data.keys() for l in self.params.label_fields):\n                raise ValueError(\"Your 'label_fields' are not valid - them must have same names as params in dict\")"
                },
                {
                    "docstring": null,
                    "method_name": "filter",
                    "second_doc": "\"\"\"\nFilter bounding boxes based on specified minimum area and minimum visibility requirements.\n\nThis method ensures that only bounding boxes meeting certain criteria are retained, discarding boxes that are too small or insufficiently visible within the given image dimensions. This filtering helps maintain the quality of data used for downstream processing or model training.\n\nArgs:\n    data (Any): The data containing bounding boxes to be filtered.\n    rows (int): The number of rows (height) of the image associated with the bounding boxes.\n    cols (int): The number of columns (width) of the image associated with the bounding boxes.\n\nReturns:\n    Any: The filtered bounding boxes that satisfy the minimum area and visibility constraints.\n\"\"\"",
                    "source_code": "return filter_bboxes(\n            data, rows, cols, min_area=self.params.min_area, min_visibility=self.params.min_visibility\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "check",
                    "second_doc": "\"\"\"\nValidates the format and integrity of bounding boxes within the provided data to ensure consistent and correct object annotations during processing.\n\nArgs:\n    data: Input data containing bounding box information, typically in a structured format compatible with downstream augmentation or transformation steps.\n\nReturns:\n    bool: True if all bounding boxes meet the required criteria; otherwise, False.\n\nWhy:\n    This method checks bounding boxes to prevent downstream errors, such as invalid annotations or misaligned object locations, which could negatively impact model training and prediction reliability.\n\"\"\"",
                    "source_code": "return check_bboxes(data)"
                },
                {
                    "docstring": null,
                    "method_name": "convert_from_albumentations",
                    "second_doc": "\"\"\"\nConverts bounding box annotations from the Albumentations augmentations format to the specified target format, ensuring validity of the results using image dimensions.\n\nArgs:\n    data: A list or array containing bounding box annotations in the Albumentations format.\n    rows: The height of the reference image.\n    cols: The width of the reference image.\n\nReturns:\n    A list or array of bounding boxes converted to the target format defined by the processor, with validity checks applied.\n\nWhy:\n    This method is necessary to ensure that bounding box annotations remain accurate and compatible with downstream tasks after applying augmentation transformations, preventing misalignment or invalid box data during preprocessing.\n\"\"\"",
                    "source_code": "return convert_bboxes_from_albumentations(data, self.params.format, rows, cols, check_validity=True)"
                },
                {
                    "docstring": null,
                    "method_name": "convert_to_albumentations",
                    "second_doc": "\"\"\"\nConverts bounding box data into a format compatible with downstream image augmentation pipelines.\n\nThis method ensures that bounding boxes are adapted to the expected input structure for further processing and validation during data augmentation workflows.\n\nArgs:\n    data (list or array): Bounding box annotations to be converted.\n    rows (int): Number of rows (height) of the image.\n    cols (int): Number of columns (width) of the image.\n\nReturns:\n    list or array: Bounding boxes reformatted and validated for compatibility with augmentation operations.\n\"\"\"",
                    "source_code": "return convert_bboxes_to_albumentations(data, self.params.format, rows, cols, check_validity=True)"
                }
            ],
            "name": "BboxProcessor",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"Normalize coordinates of a bounding box. Divide x-coordinates by image width and y-coordinates\n    by image height.\n\n    Args:\n        bbox (tuple): Denormalized bounding box `(x_min, y_min, x_max, y_max)`.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        tuple: Normalized bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: If rows or cols is less or equal zero\n\n    \"\"\"",
                "first_doc": "\"\"\"\nNormalizes a bounding box by dividing its coordinates according to the number of rows and columns.\n\nArgs:\n    bbox: A sequence representing the bounding box, where the first four elements are coordinates and the remaining elements are preserved as is.\n    rows: The total number of rows to normalize the bounding box by.\n    cols: The total number of columns to normalize the bounding box by.\n\nReturns:\n    A tuple containing the normalized bounding box coordinates followed by any additional values found in the input bbox.\n\"\"\"",
                "method_name": "normalize_bbox",
                "second_doc": "\"\"\"\nScales the coordinates of a bounding box relative to the height and width of an image, ensuring the bounding box representation is independent of the image size. This allows bounding box data to remain consistent and comparable across differently sized images, which is essential for reliable geometric transformations and model training.\n\nArgs:\n    bbox: A sequence containing bounding box coordinates as the first four elements (x_min, y_min, x_max, y_max), optionally followed by additional data that will be preserved.\n    rows: Integer representing the total number of rows (image height) to normalize the bounding box by. Must be positive.\n    cols: Integer representing the total number of columns (image width) to normalize the bounding box by. Must be positive.\n\nReturns:\n    tuple: The normalized bounding box coordinates as floats, followed by any extra information present in the input bbox.\n\"\"\"",
                "source_code": "(x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n\n    if rows <= 0:\n        raise ValueError(\"Argument rows must be positive integer\")\n    if cols <= 0:\n        raise ValueError(\"Argument cols must be positive integer\")\n\n    x_min, x_max = x_min / cols, x_max / cols\n    y_min, y_max = y_min / rows, y_max / rows\n\n    return (x_min, y_min, x_max, y_max) + tail"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Denormalize coordinates of a bounding box. Multiply x-coordinates by image width and y-coordinates\n    by image height. This is an inverse operation for :func:`~albumentations.augmentations.bbox.normalize_bbox`.\n\n    Args:\n        bbox (tuple): Normalized bounding box `(x_min, y_min, x_max, y_max)`.\n        rows (int): Image height.\n        cols (int): Image widht.\n\n    Returns:\n        tuple: Denormalized bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: If rows or cols is less or equal zero\n\n    \"\"\"",
                "first_doc": "\"\"\"\nDenormalizes a bounding box from a normalized coordinate system to absolute coordinates based on the specified number of rows and columns.\n\nArgs:\n    bbox: A sequence where the first four elements represent the normalized bounding box (x_min, y_min, x_max, y_max), followed by any additional data.\n    rows: The total number of rows (height) in the target coordinate system.\n    cols: The total number of columns (width) in the target coordinate system.\n\nReturns:\n    tuple: A tuple containing the denormalized bounding box coordinates (x_min, y_min, x_max, y_max), followed by any remaining data from the input bbox.\n\"\"\"",
                "method_name": "denormalize_bbox",
                "second_doc": "\"\"\"\nConverts bounding box coordinates from a normalized scale to absolute pixel values using the specified dimensions. This enables accurate mapping of box positions within an image or feature map, which is essential for correctly processing and visualizing spatial annotations after geometric transformations.\n\nArgs:\n    bbox (sequence): The first four elements are the normalized bounding box coordinates (x_min, y_min, x_max, y_max), with optional additional data following.\n    rows (int): The height of the target coordinate system in pixels.\n    cols (int): The width of the target coordinate system in pixels.\n\nReturns:\n    tuple: The denormalized bounding box as (x_min, y_min, x_max, y_max) in absolute pixel coordinates, followed by any extra data from the input bbox.\n\"\"\"",
                "source_code": "(x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n\n    if rows <= 0:\n        raise ValueError(\"Argument rows must be positive integer\")\n    if cols <= 0:\n        raise ValueError(\"Argument cols must be positive integer\")\n\n    x_min, x_max = x_min * cols, x_max * cols\n    y_min, y_max = y_min * rows, y_max * rows\n\n    return (x_min, y_min, x_max, y_max) + tail"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Normalize a list of bounding boxes.\n\n    Args:\n        bboxes (List[tuple]): Denormalized bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        List[tuple]: Normalized bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nNormalizes a list of bounding boxes based on the specified image dimensions.\n\nArgs:\n    bboxes: A list of bounding boxes to be normalized.\n    rows: The number of rows (height) of the image.\n    cols: The number of columns (width) of the image.\n\nReturns:\n    A list of normalized bounding boxes.\n\"\"\"",
                "method_name": "normalize_bboxes",
                "second_doc": "\"\"\"\nConverts bounding box coordinates to a normalized scale relative to image dimensions, ensuring consistency across different image sizes.\n\nArgs:\n    bboxes: A list of bounding boxes to convert, where each bounding box is defined by its coordinates.\n    rows: Integer representing the image height.\n    cols: Integer representing the image width.\n\nReturns:\n    A list of bounding boxes with coordinates normalized to the [0, 1] range based on the input image size.\n\"\"\"",
                "source_code": "return [normalize_bbox(bbox, rows, cols) for bbox in bboxes]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Denormalize a list of bounding boxes.\n\n    Args:\n        bboxes (List[tuple]): Normalized bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        List[tuple]: Denormalized bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nConverts a list of normalized bounding boxes to pixel coordinates.\n\nArgs:\n    bboxes: A list of bounding boxes with normalized coordinates to be converted.\n    rows: The number of rows (height) of the image used for denormalization.\n    cols: The number of columns (width) of the image used for denormalization.\n\nReturns:\n    A list of bounding boxes with coordinates converted to pixel values.\n\"\"\"",
                "method_name": "denormalize_bboxes",
                "second_doc": "\"\"\"\nTransforms a list of bounding boxes from normalized (relative) coordinates to absolute pixel positions based on the target image dimensions. This step is necessary to accurately map predicted or annotated object locations back onto images for visualization, evaluation, or further processing in object detection workflows.\n\nArgs:\n    bboxes: List of bounding boxes with normalized coordinates [y_min, x_min, y_max, x_max].\n    rows: Integer specifying the image height in pixels for scaling purposes.\n    cols: Integer specifying the image width in pixels for scaling purposes.\n\nReturns:\n    List of bounding boxes with coordinates converted to absolute pixel values.\n\"\"\"",
                "source_code": "return [denormalize_bbox(bbox, rows, cols) for bbox in bboxes]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Calculate the area of a bounding box in pixels.\n\n    Args:\n        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Return:\n        int: Area of a bounding box in pixels.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nCalculates the area of a bounding box in pixel coordinates.\n\nArgs:\n    bbox: The normalized bounding box coordinates to be denormalized.\n    rows: The number of rows (height) in the image, used for denormalization.\n    cols: The number of columns (width) in the image, used for denormalization.\n\nReturns:\n    The area of the denormalized bounding box in pixels.\n\"\"\"",
                "method_name": "calculate_bbox_area",
                "second_doc": "\"\"\"\nComputes the area, in pixel units, of a bounding box after converting its normalized coordinates to image space. This calculation helps quantify the spatial extent of an object within the image, which is essential for evaluating and applying transformations while preserving annotation integrity.\n\nArgs:\n    bbox: A list or tuple containing the normalized coordinates (x_min, y_min, x_max, y_max) of the bounding box.\n    rows: Integer representing the image height in pixels, used for scaling the coordinates.\n    cols: Integer representing the image width in pixels, used for scaling the coordinates.\n\nReturns:\n    Float representing the area of the bounding box in pixel coordinates.\n\"\"\"",
                "source_code": "bbox = denormalize_bbox(bbox, rows, cols)\n    x_min, y_min, x_max, y_max = bbox[:4]\n    area = (x_max - x_min) * (y_max - y_min)\n    return area"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Filter bounding boxes and return only those boxes whose visibility after transformation is above\n    the threshold and minimal area of bounding box in pixels is more then min_area.\n\n    Args:\n        original_shape (tuple): Original image shape `(height, width)`.\n        bboxes (List[tuple]): Original bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n        transformed_shape (tuple): Transformed image shape `(height, width)`.\n        transformed_bboxes (List[tuple]): Transformed bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n        threshold (float): visibility threshold. Should be a value in the range [0.0, 1.0].\n        min_area (float): Minimal area threshold.\n\n    Returns:\n        List[tuple]: Filtered bounding boxes `[(x_min, y_min, x_max, y_max)]`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nFilters transformed bounding boxes based on their visibility after an image transformation.\n\nThis method evaluates each pair of original and transformed bounding boxes, checking if the transformed bounding box remains within the normalized image space and if its visible area exceeds a specified threshold. Bounding boxes that satisfy the visibility requirements are retained.\n\nArgs:\n    original_shape: The height and width of the original image as a tuple or similar sequence.\n    bboxes: The list of bounding boxes in the original image, with each box typically defined by coordinates.\n    transformed_shape: The height and width of the transformed image as a tuple or similar sequence.\n    transformed_bboxes: The list of bounding boxes in the transformed image, corresponding to the original set.\n\nReturns:\n    list: A list of transformed bounding boxes that pass the visibility and area threshold criteria.\n\"\"\"",
                "method_name": "filter_bboxes_by_visibility",
                "second_doc": "\"\"\"\nSelects transformed bounding boxes that meet defined visibility and area requirements after image augmentation.\n\nThis method ensures the quality and relevance of bounding boxes by verifying that, after transformation, they remain within the normalized image space, retain sufficient area, and are mostly visible compared to their original extent. By filtering out boxes that do not meet these criteria, the method maintains accurate object localization despite geometric or other augmentations applied to the image.\n\nArgs:\n    original_shape (tuple): Height and width of the original image.\n    bboxes (list): List of bounding boxes in the original image, each typically defined by coordinates.\n    transformed_shape (tuple): Height and width of the transformed image.\n    transformed_bboxes (list): List of bounding boxes after transformation, corresponding to the original set.\n\nReturns:\n    list: Transformed bounding boxes that remain within bounds and meet the minimum visibility and area thresholds.\n\"\"\"",
                "source_code": "img_height, img_width = original_shape[:2]\n    transformed_img_height, transformed_img_width = transformed_shape[:2]\n\n    visible_bboxes = []\n    for bbox, transformed_bbox in zip(bboxes, transformed_bboxes):\n        if not all(0.0 <= value <= 1.0 for value in transformed_bbox[:4]):\n            continue\n        bbox_area = calculate_bbox_area(bbox, img_height, img_width)\n        transformed_bbox_area = calculate_bbox_area(transformed_bbox, transformed_img_height, transformed_img_width)\n        if transformed_bbox_area < min_area:\n            continue\n        visibility = transformed_bbox_area / bbox_area\n        if visibility >= threshold:\n            visible_bboxes.append(transformed_bbox)\n    return visible_bboxes"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Convert a bounding box from a format specified in `source_format` to the format used by albumentations:\n    normalized coordinates of bottom-left and top-right corners of the bounding box in a form of\n    `(x_min, y_min, x_max, y_max)` e.g. `(0.15, 0.27, 0.67, 0.5)`.\n\n    Args:\n        bbox (tuple): A bounding box tuple.\n        source_format (str): format of the bounding box. Should be 'coco', 'pascal_voc', or 'yolo'.\n        check_validity (bool): Check if all boxes are valid boxes.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Note:\n        The `coco` format of a bounding box looks like `(x_min, y_min, width, height)`, e.g. (97, 12, 150, 200).\n        The `pascal_voc` format of a bounding box looks like `(x_min, y_min, x_max, y_max)`, e.g. (97, 12, 247, 212).\n        The `yolo` format of a bounding box looks like `(x, y, width, height)`, e.g. (0.3, 0.1, 0.05, 0.07);\n        where `x`, `y` coordinates of the center of the box, all values normalized to 1 by image height and width.\n\n    Raises:\n        ValueError: if `target_format` is not equal to `coco` or `pascal_voc`, ot `yolo`.\n        ValueError: If in YOLO format all labels not in range (0, 1).\n\n    \"\"\"",
                "first_doc": "\"\"\"\nConverts a bounding box from a specified annotation format to the normalized Albumentations format.\n\nArgs:\n    bbox: The bounding box, expected in the coordinate order and structure according to the given source_format.\n    source_format: The annotation format of the input bounding box. Supported values are 'coco', 'pascal_voc', or 'yolo'.\n    rows: The total number of rows (image height) used for normalization or denormalization.\n    cols: The total number of columns (image width) used for normalization or denormalization.\n\nReturns:\n    The bounding box converted and normalized to the Albumentations format as a tuple (x_min, y_min, x_max, y_max[, ...]), where coordinates are floats in the range [0, 1].\n\"\"\"",
                "method_name": "convert_bbox_to_albumentations",
                "second_doc": "\"\"\"\nConverts a bounding box from a specified annotation format to the normalized coordinate system expected by Albumentations.\n\nThis method ensures that bounding box annotations from different dataset formats are accurately mapped into a uniform, normalized representation. Such standardization allows further processing, transformation, and augmentation routines to operate consistently, regardless of the input source.\n\nArgs:\n    bbox: The bounding box coordinates in the structure defined by the given source_format, potentially including extra metadata.\n    source_format: The format of the input bounding box. Supported values are 'coco', 'pascal_voc', or 'yolo'.\n    rows: The image height (number of rows) used for coordinate normalization.\n    cols: The image width (number of columns) used for coordinate normalization.\n\nReturns:\n    tuple: The bounding box converted and normalized to the Albumentations format (x_min, y_min, x_max, y_max[, ...]), where all coordinates are floats within the [0, 1] interval.\n\"\"\"",
                "source_code": "if source_format not in {\"coco\", \"pascal_voc\", \"yolo\"}:\n        raise ValueError(\n            \"Unknown source_format {}. Supported formats are: 'coco', 'pascal_voc' and 'yolo'\".format(source_format)\n        )\n    if source_format == \"coco\":\n        (x_min, y_min, width, height), tail = bbox[:4], tuple(bbox[4:])\n        x_max = x_min + width\n        y_max = y_min + height\n    elif source_format == \"yolo\":\n        # https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/scripts/voc_label.py#L12\n        bbox, tail = bbox[:4], tuple(bbox[4:])\n        _bbox = np.array(bbox[:4])\n        if not np.all((0 < _bbox) & (_bbox < 1)):\n            raise ValueError(\"In YOLO format all labels must be float and in range (0, 1)\")\n\n        x, y, width, height = denormalize_bbox(bbox, rows, cols)\n\n        x_min = x - width / 2 + 1\n        x_max = x_min + width\n        y_min = y - height / 2 + 1\n        y_max = y_min + height\n    else:\n        (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n\n    bbox = (x_min, y_min, x_max, y_max) + tail\n    bbox = normalize_bbox(bbox, rows, cols)\n    if check_validity:\n        check_bbox(bbox)\n    return bbox"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Convert a bounding box from the format used by albumentations to a format, specified in `target_format`.\n\n    Args:\n        bbox (tuple): An albumentation bounding box `(x_min, y_min, x_max, y_max)`.\n        target_format (str): required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.\n        rows (int): Image height.\n        cols (int): Image width.\n        check_validity (bool): Check if all boxes are valid boxes.\n\n    Returns:\n        tuple: A bounding box.\n\n    Note:\n        The `coco` format of a bounding box looks like `[x_min, y_min, width, height]`, e.g. [97, 12, 150, 200].\n        The `pascal_voc` format of a bounding box looks like `[x_min, y_min, x_max, y_max]`, e.g. [97, 12, 247, 212].\n        The `yolo` format of a bounding box looks like `[x, y, width, height]`, e.g. [0.3, 0.1, 0.05, 0.07].\n\n    Raises:\n        ValueError: if `target_format` is not equal to `coco`, `pascal_voc` or `yolo`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nConverts a bounding box from the Albumentations format to the specified target format.\n\nThis method supports conversions to 'coco', 'pascal_voc', and 'yolo' formats, handling the appropriate coordinate transformations and normalization as needed.\n\nArgs:\n    bbox: The bounding box to be converted, represented in Albumentations format.\n    target_format: The desired target format for the output bounding box. Must be one of 'coco', 'pascal_voc', or 'yolo'.\n    rows: The number of rows (image height) for coordinate normalization or denormalization.\n    cols: The number of columns (image width) for coordinate normalization or denormalization.\n\nReturns:\n    The bounding box converted to the specified target format.\n\"\"\"",
                "method_name": "convert_bbox_from_albumentations",
                "second_doc": "\"\"\"\nTransforms a bounding box from Albumentations format to a specified standard annotation format, ensuring compatibility with various computer vision datasets and tools.\n\nThis function enables smooth interoperability between different bounding box standards by performing necessary coordinate and normalization transformations appropriate to the selected output format.\n\nArgs:\n    bbox: Bounding box in Albumentations format (normalized as [x_min, y_min, x_max, y_max] plus optional additional data).\n    target_format: Target format for conversion. Supported values are 'coco', 'pascal_voc', or 'yolo'.\n    rows: Image height in pixels, required for proper normalization or denormalization of coordinates.\n    cols: Image width in pixels, required for proper normalization or denormalization of coordinates.\n\nReturns:\n    Converted bounding box in the specified target format, with coordinates adjusted for the chosen annotation convention.\n\"\"\"",
                "source_code": "if target_format not in {\"coco\", \"pascal_voc\", \"yolo\"}:\n        raise ValueError(\n            \"Unknown target_format {}. Supported formats are: 'coco', 'pascal_voc' and 'yolo'\".format(target_format)\n        )\n    if check_validity:\n        check_bbox(bbox)\n    bbox = denormalize_bbox(bbox, rows, cols)\n    if target_format == \"coco\":\n        (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])\n        width = x_max - x_min\n        height = y_max - y_min\n        bbox = (x_min, y_min, width, height) + tail\n    elif target_format == \"yolo\":\n        # https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/scripts/voc_label.py#L12\n        (x_min, y_min, x_max, y_max), tail = bbox[:4], bbox[4:]\n        x = (x_min + x_max) / 2 - 1\n        y = (y_min + y_max) / 2 - 1\n        width = x_max - x_min\n        height = y_max - y_min\n        bbox = normalize_bbox((x, y, width, height) + tail, rows, cols)\n    return bbox"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\n    \"\"\"",
                "first_doc": "\"\"\"\nConverts a list of bounding boxes to the Albumentations format.\n\nEach bounding box in the input list is converted from the specified source format to the Albumentations format, taking into account the dimensions of the image.\n\nArgs:\n    bboxes: A list of bounding boxes to be converted.\n    source_format: The format of the input bounding boxes.\n    rows: The number of rows (height) of the image.\n    cols: The number of columns (width) of the image.\n\nReturns:\n    A list of bounding boxes converted to the Albumentations format.\n\"\"\"",
                "method_name": "convert_bboxes_to_albumentations",
                "second_doc": "\"\"\"\nTransforms a list of bounding boxes from a specified input format to the format required for consistent processing and augmentation operations, ensuring accurate spatial alignment with the image dimensions.\n\nArgs:\n    bboxes: A list of bounding box coordinates to be transformed.\n    source_format: The current format of the input bounding boxes.\n    rows: The height of the reference image in pixels.\n    cols: The width of the reference image in pixels.\n    check_validity: Boolean flag indicating whether to validate each bounding box during conversion.\n\nReturns:\n    A list of bounding boxes converted to the target format for further processing.\n    \nWhy:\n    This conversion ensures that all bounding boxes are interpreted correctly for subsequent augmentation or manipulation steps, preserving spatial consistency and data integrity during image processing workflows.\n\"\"\"",
                "source_code": "return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Convert a list of bounding boxes from the format used by albumentations to a format, specified\n    in `target_format`.\n\n    Args:\n        bboxes (List[tuple]): List of albumentation bounding box `(x_min, y_min, x_max, y_max)`.\n        target_format (str): required format of the output bounding box. Should be 'coco', 'pascal_voc' or 'yolo'.\n        rows (int): Image height.\n        cols (int): Image width.\n        check_validity (bool): Check if all boxes are valid boxes.\n\n    Returns:\n        list[tuple]: List of bounding box.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nConverts a list of bounding boxes from the Albumentations format to a specified target format.\n\nArgs:\n    bboxes: A list of bounding boxes in the Albumentations format to convert.\n    target_format: The target format to which each bounding box should be converted.\n    rows: The number of rows (height) of the image associated with the bounding boxes.\n    cols: The number of columns (width) of the image associated with the bounding boxes.\n\nReturns:\n    list: A list of bounding boxes converted to the specified target format.\n\"\"\"",
                "method_name": "convert_bboxes_from_albumentations",
                "second_doc": "\"\"\"\nConverts a list of bounding boxes from Albumentations format to a specified target format to ensure compatibility with varying requirements of downstream computer vision tasks or frameworks.\n\nArgs:\n    bboxes (list): List of bounding boxes in the Albumentations format.\n    target_format (str): Desired bounding box format to convert to.\n    rows (int): Number of rows (height) in the image corresponding to the bounding boxes.\n    cols (int): Number of columns (width) in the image corresponding to the bounding boxes.\n\nReturns:\n    list: Bounding boxes transformed into the desired format for consistent processing or evaluation.\n\"\"\"",
                "source_code": "return [convert_bbox_from_albumentations(bbox, target_format, rows, cols, check_validity) for bbox in bboxes]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums\"\"\"",
                "first_doc": "\"\"\"\nValidates that the given bounding box coordinates are properly normalized and ordered.\n\nChecks that each coordinate in the provided bounding box is within the normalized range [0.0, 1.0] and that the maximum coordinates are greater than the corresponding minimum values.\n\nArgs:\n    bbox: A sequence representing the bounding box in the format [x_min, y_min, x_max, y_max], where each coordinate should be between 0.0 and 1.0.\n\nReturns:\n    None. Raises a ValueError if any of the bounding box constraints are violated.\n\"\"\"",
                "method_name": "check_bbox",
                "second_doc": "\"\"\"\nEnsures that the provided bounding box coordinates are valid for further processing in image transformations.\n\nThe method verifies that each bounding box coordinate falls within a normalized range and that the box is properly defined with its minimum values less than its maximum values. This validation step prevents downstream errors during transformations that assume valid input boxes.\n\nArgs:\n    bbox: Sequence of four numbers in the format [x_min, y_min, x_max, y_max], where each value must be within [0.0, 1.0].\n\nReturns:\n    None.\n\nRaises:\n    ValueError: If any coordinate is outside [0.0, 1.0], or if x_max <= x_min or y_max <= y_min.\n\"\"\"",
                "source_code": "for name, value in zip([\"x_min\", \"y_min\", \"x_max\", \"y_max\"], bbox[:4]):\n        if not 0 <= value <= 1:\n            raise ValueError(\n                \"Expected {name} for bbox {bbox} \"\n                \"to be in the range [0.0, 1.0], got {value}.\".format(bbox=bbox, name=name, value=value)\n            )\n    x_min, y_min, x_max, y_max = bbox[:4]\n    if x_max <= x_min:\n        raise ValueError(\"x_max is less than or equal to x_min for bbox {bbox}.\".format(bbox=bbox))\n    if y_max <= y_min:\n        raise ValueError(\"y_max is less than or equal to y_min for bbox {bbox}.\".format(bbox=bbox))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Check if bboxes boundaries are in range 0, 1 and minimums are lesser then maximums\"\"\"",
                "first_doc": "\"\"\"\nChecks each bounding box in the provided list by applying the check_bbox function.\n\nArgs:\n    bboxes: A list containing bounding box objects to be checked.\n\nReturns:\n    None: This method does not return a value.\n\"\"\"",
                "method_name": "check_bboxes",
                "second_doc": "\"\"\"\nValidates each bounding box in the input list by applying a verification function to ensure the boxes adhere to expected constraints, facilitating reliable downstream image processing operations.\n\nArgs:\n    bboxes (list): A list of bounding box objects to be validated.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "for bbox in bboxes:\n        check_bbox(bbox)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Remove bounding boxes that either lie outside of the visible area by more then min_visibility\n    or whose area in pixels is under the threshold set by `min_area`. Also it crops boxes to final image size.\n\n    Args:\n        bboxes (List[tuple]): List of albumentation bounding box `(x_min, y_min, x_max, y_max)`.\n        rows (int): Image height.\n        cols (int): Image width.\n        min_area (float): Minimum area of a bounding box. All bounding boxes whose visible area in pixels.\n            is less than this value will be removed. Default: 0.0.\n        min_visibility (float): Minimum fraction of area for a bounding box to remain this box in list. Default: 0.0.\n\n    Returns:\n        List[tuple]: List of bounding box.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nFilters bounding boxes based on their area, clipping, and minimum visibility thresholds.\n\nArgs:\n    bboxes: List of bounding boxes to filter, where each box is represented by coordinates and additional values.\n    rows: Number of rows (height) of the image used to compute actual box areas.\n    cols: Number of columns (width) of the image used to compute actual box areas.\n\nReturns:\n    A list of bounding boxes that satisfy the visibility and minimum area requirements. Each bounding box in the list consists of its clipped coordinates along with any additional information.\n\"\"\"",
                "method_name": "filter_bboxes",
                "second_doc": "\"\"\"\nRemoves bounding boxes from a list if they do not cover enough area or are not sufficiently visible after being clipped to image boundaries. This ensures that only relevant and reliable bounding boxes are kept for further processing, leading to more accurate and robust results in subsequent image analysis.\n\nArgs:\n    bboxes: List of bounding boxes, where each is represented by coordinates followed by optional additional data.\n    rows: Integer specifying the image height, used to calculate bounding box areas.\n    cols: Integer specifying the image width, used to calculate bounding box areas.\n\nReturns:\n    List of bounding boxes that meet minimum visibility and area constraints, with their coordinates clipped to image dimensions and any extra values preserved.\n\"\"\"",
                "source_code": "resulting_boxes = []\n    for bbox in bboxes:\n        transformed_box_area = calculate_bbox_area(bbox, rows, cols)\n        bbox, tail = tuple(np.clip(bbox[:4], 0, 1.0)), tuple(bbox[4:])\n        clipped_box_area = calculate_bbox_area(bbox, rows, cols)\n        if not transformed_box_area or clipped_box_area / transformed_box_area <= min_visibility:\n            continue\n        else:\n            bbox = tuple(np.clip(bbox[:4], 0, 1.0))\n        if calculate_bbox_area(bbox, rows, cols) <= min_area:\n            continue\n        resulting_boxes.append(bbox + tail)\n    return resulting_boxes"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Calculate union of bounding boxes.\n\n    Args:\n        height (float): Height of image or space.\n        width (float): Width of image or space.\n        bboxes (List[tuple]): List like bounding boxes. Format is `[(x_min, y_min, x_max, y_max)]`.\n        erosion_rate (float): How much each bounding box can be shrinked, useful for erosive cropping.\n            Set this in range [0, 1]. 0 will not be erosive at all, 1.0 can make any bbox to lose its volume.\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nComputes the union bounding box, with optional erosion, from a list of bounding boxes.\n\nIterates through each bounding box, applies an erosion rate to shrink each box, and determines a new box that encloses all the eroded boxes.\n\nArgs:\n    height: The maximum possible height, used for initializing the union bounding box.\n    width: The maximum possible width, used for initializing the union bounding box.\n    bboxes: A list of bounding boxes, where each bounding box is an iterable containing at least four elements (x_min, y_min, x_max, y_max).\n\nReturns:\n    A tuple (x1, y1, x2, y2) representing the coordinates of the smallest bounding box that contains all eroded input bounding boxes.\n\"\"\"",
                "method_name": "union_of_bboxes",
                "second_doc": "\"\"\"\nCalculates the smallest bounding box that contains all provided bounding boxes after applying an optional erosion, ensuring robust spatial coverage even when objects may slightly shift or change size during augmentations.\n\nThis process is essential for maintaining accurate bounding box annotations after transformations that might slightly shrink the objects, thereby preventing annotation drift or overlap errors.\n\nArgs:\n    height (int or float): The maximum possible height, used to initialize the union bounding box.\n    width (int or float): The maximum possible width, used to initialize the union bounding box.\n    bboxes (list): A list of bounding boxes, where each bounding box is an iterable containing at least four elements: (x_min, y_min, x_max, y_max).\n    erosion_rate (float): The fraction by which each bounding box is eroded from its sides before computing the union.\n\nReturns:\n    tuple: A tuple (x1, y1, x2, y2) representing the coordinates of the smallest bounding box that encloses all the eroded input bounding boxes.\n\"\"\"",
                "source_code": "x1, y1 = width, height\n    x2, y2 = 0, 0\n    for bbox in bboxes:\n        x_min, y_min, x_max, y_max = bbox[:4]\n        w, h = x_max - x_min, y_max - y_min\n        lim_x1, lim_y1 = x_min + erosion_rate * w, y_min + erosion_rate * h\n        lim_x2, lim_y2 = x_max - erosion_rate * w, y_max - erosion_rate * h\n        x1, y1 = np.min([x1, lim_x1]), np.min([y1, lim_y1])\n        x2, y2 = np.max([x2, lim_x2]), np.max([y2, lim_y2])\n    return x1, y1, x2, y2"
            },
            "type": "function"
        }
    ],
    "albumentations/albumentations/augmentations/functional.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "clip",
                "second_doc": "\"\"\"\nClips the input image's pixel values to the specified range and casts the result to a desired data type.\n\nThis ensures that image values remain within valid bounds after transformations, maintaining data consistency for further processing.\n\nArgs:\n    img (np.ndarray): Input image to be clipped.\n    maxval (numeric): Upper bound to clip the image values.\n    dtype (np.dtype): Desired data type of the output image.\n\nReturns:\n    np.ndarray: Image with values clipped to [0, maxval] and cast to dtype.\n\"\"\"",
                "source_code": "return np.clip(img, 0, maxval).astype(dtype)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "clipped",
                "second_doc": "\"\"\"\nEnsures that the output of an image processing function remains within the valid value range for the image's data type by clipping values accordingly.\n\nArgs:\n    img: The input image array whose dtype determines the valid value range.\n    *args: Additional positional arguments passed to the decorated function.\n    **kwargs: Additional keyword arguments passed to the decorated function.\n\nReturns:\n    The processed image, with all values clipped to the appropriate range for its data type.\n\nWhy:\n    This is done to prevent invalid pixel values after transformations, which could otherwise lead to unexpected behavior or errors in downstream image processing tasks.\n\"\"\"",
                "source_code": "@wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        dtype = img.dtype\n        maxval = MAX_VALUES_BY_DTYPE.get(dtype, 1.0)\n        return clip(func(img, *args, **kwargs), dtype, maxval)\n\n    return wrapped_function"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "angle_to_2pi_range",
                "second_doc": "\"\"\"\nNormalizes the input angle to the range [0, 2\u03c0].\n\nThis function ensures that any given angle, regardless of its initial value, is wrapped into a standard interval commonly used in geometric computations. This normalization is necessary to avoid issues caused by angle values that fall outside the typical bounds, which can result from repeated rotations or transformations.\n\nArgs:\n    angle (float): The input angle in radians to be normalized.\n\nReturns:\n    float: The normalized angle value in the range [0, 2\u03c0].\n\"\"\"",
                "source_code": "if 0 <= angle <= 2 * np.pi:\n        return angle\n\n    if angle < 0:\n        angle += (abs(angle) // (2 * np.pi) + 1) * 2 * np.pi\n\n    return angle % (2 * np.pi)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Preserve shape of the image\n\n    \"\"\"",
                "first_doc": "\"\"\"\nA decorator that ensures the output of the decorated function has the same shape as the input image.\n\nArgs:\n    func: The function to be decorated, which should accept an image as its first argument and may alter its shape.\n\nReturns:\n    function: A wrapper function that reshapes the output of `func` to match the original image shape.\n\"\"\"",
                "method_name": "preserve_shape",
                "second_doc": "\"\"\"\nA decorator that ensures the output of the decorated function retains the original shape of the input image. This preserves compatibility with downstream processes and avoids errors related to unexpected dimension changes during image transformations.\n\nArgs:\n    func (callable): The function to be decorated. It should take an image as its first argument, potentially modifying the image but not its intended shape.\n\nReturns:\n    callable: A wrapper function that invokes `func` and reshapes its output to match the original input image's shape.\n\"\"\"",
                "source_code": "@wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        shape = img.shape\n        result = func(img, *args, **kwargs)\n        result = result.reshape(shape)\n        return result\n\n    return wrapped_function"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Preserve dummy channel dim.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nDecorator that ensures the channel dimension is preserved in image processing functions.\n\nArgs:\n    func: The image processing function to be wrapped. This function should take an image as its first argument.\n\nReturns:\n    A wrapped version of the input function that maintains a singleton channel dimension in the output if the input image had one.\n\"\"\"",
                "method_name": "preserve_channel_dim",
                "second_doc": "\"\"\"\nDecorator that ensures functions processing images retain a singleton channel dimension when present, helping maintain image data consistency through transformations.\n\nArgs:\n    func: The image processing function to be wrapped, which should accept an image as its first argument.\n\nReturns:\n    A wrapped version of the function that automatically restores a singleton channel dimension to the output if it was present in the input, ensuring seamless downstream processing that relies on expected input shapes.\n\"\"\"",
                "source_code": "@wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        shape = img.shape\n        result = func(img, *args, **kwargs)\n        if len(shape) == 3 and shape[-1] == 1 and len(result.shape) == 2:\n            result = np.expand_dims(result, axis=-1)\n        return result\n\n    return wrapped_function"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "is_rgb_image",
                "second_doc": "\"\"\"\nChecks if the input image has three channels, indicating an RGB image layout.\n\nArgs:\n    image (numpy.ndarray): Input image to check.\n\nReturns:\n    bool: True if the image has three dimensions and the last dimension size is 3 (indicative of an RGB image), otherwise False.\n\nThis check is useful to ensure that image processing operations expecting color images are applied to appropriately structured data.\n\"\"\"",
                "source_code": "return len(image.shape) == 3 and image.shape[-1] == 3"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "is_grayscale_image",
                "second_doc": "\"\"\"\nDetermine if the input image is grayscale by inspecting its shape.\n\nThis check is essential in preprocessing pipelines to ensure that images receive appropriate augmentations or processing steps based on their color format.\n\nArgs:\n    image (numpy.ndarray): Input image array to be checked.\n\nReturns:\n    bool: True if the image is grayscale (either 2D or 3D with a single channel), False otherwise.\n\"\"\"",
                "source_code": "return (len(image.shape) == 2) or (len(image.shape) == 3 and image.shape[-1] == 1)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "is_multispectral_image",
                "second_doc": "\"\"\"\nDetermine if the given image has more than one spectral channel and is not a standard grayscale or color image.\n\nThis check distinguishes multispectral images (with an unusual number of channels) from common single- or three-channel images, aiding in selecting appropriate processing or augmentation strategies.\n\nArgs:\n    image (numpy.ndarray): The input image array to be analyzed.\n\nReturns:\n    bool: True if the image appears to be multispectral (3-dimensional and number of channels is not 1 or 3), False otherwise.\n\"\"\"",
                "source_code": "return len(image.shape) == 3 and image.shape[-1] not in [1, 3]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "get_num_channels",
                "second_doc": "\"\"\"\nDetermine the number of channels in the input image by inspecting its shape. This facilitates handling of both grayscale and multi-channel images appropriately in subsequent processing steps.\n\nArgs:\n    image (numpy.ndarray): Input image array. Can be 2D (grayscale) or 3D (multi-channel).\n\nReturns:\n    int: Number of channels in the image. Returns 1 for 2D (grayscale) images, or the size of the third dimension for 3D images.\n\"\"\"",
                "source_code": "return image.shape[2] if len(image.shape) == 3 else 1"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "non_rgb_warning",
                "second_doc": "\"\"\"\nRaises a ValueError if the provided image does not have three color channels (RGB).\n\nThis function checks the number of channels in the input image to ensure compatibility with transformations that expect standard 3-channel images. If the image is grayscale (single channel), it suggests converting it to RGB; if the image is multispectral (more than 3 channels), it indicates that such images are not supported by the transformation. This ensures the applied image transformations behave as intended and prevents subtle bugs or errors during image preprocessing.\n\nArgs:\n    image: The image array to be checked, typically as a NumPy array.\n\nRaises:\n    ValueError: If the image does not have exactly 3 channels, with an explanatory message.\n\"\"\"",
                "source_code": "if not is_rgb_image(image):\n        message = \"This transformation expects 3-channel images\"\n        if is_grayscale_image(image):\n            message += \"\\nYou can convert your grayscale image to RGB using cv2.cvtColor(image, cv2.COLOR_GRAY2RGB))\"\n        if is_multispectral_image(image):  # Any image with a number of channels other than 1 and 3\n            message += \"\\nThis transformation cannot be applied to multi-spectral images\"\n\n        raise ValueError(message)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "vflip",
                "second_doc": "\"\"\"\nFlip the input image vertically (upside down) while preserving its memory layout.\n\nThis operation is commonly used in image augmentation pipelines to introduce additional variety and help models become invariant to vertical orientation changes.\n\nArgs:\n    img (np.ndarray): Input image array to be vertically flipped.\n\nReturns:\n    np.ndarray: Vertically flipped image with contiguous memory layout.\n\"\"\"",
                "source_code": "return np.ascontiguousarray(img[::-1, ...])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "hflip",
                "second_doc": "\"\"\"\nFlip the input image horizontally by reversing the order of its columns.\n\nThis function increases the diversity of training data by creating mirrored versions of the original images, which helps improve model robustness to spatial variations.\n\nArgs:\n    img (np.ndarray): Input image represented as a NumPy array.\n\nReturns:\n    np.ndarray: Horizontally flipped image as a contiguous NumPy array.\n\"\"\"",
                "source_code": "return np.ascontiguousarray(img[:, ::-1, ...])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "hflip_cv2",
                "second_doc": "\"\"\"\nHorizontally flips the input image using OpenCV.\n\nThis function produces a mirror image by flipping the input along the vertical axis. Such a transformation increases dataset diversity and helps models become invariant to the orientation of objects during training.\n\nArgs:\n    img (numpy.ndarray): Input image to be flipped.\n\nReturns:\n    numpy.ndarray: Horizontally flipped image.\n\"\"\"",
                "source_code": "return cv2.flip(img, 1)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "random_flip",
                "second_doc": "\"\"\"\nFlip the input image horizontally, vertically, or both according to the specified mode.\n\nThis operation introduces variation into input images, helping to create more diverse datasets for model training and evaluation.\n\nArgs:\n    img (np.ndarray): Input image to be flipped.\n    code (int): Flip mode; 0 for vertical, 1 for horizontal, -1 for both vertical and horizontal flips.\n\nReturns:\n    np.ndarray: The flipped image.\n\"\"\"",
                "source_code": "return cv2.flip(img, code)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "transpose",
                "second_doc": "\"\"\"\nTransposes the given image by swapping its height and width axes, preserving any additional dimensions such as channels.\n\nThis operation reorganizes the image's spatial structure, which can be useful in data preprocessing and augmentation pipelines to increase the diversity of input data or to align image dimensions with expected input formats.\n\nArgs:\n    img (numpy.ndarray): Input image array. Can be a 2D (H, W) or 3D (H, W, C) array.\n\nReturns:\n    numpy.ndarray: The transposed image with axes (W, H) for 2D images or (W, H, C) for 3D images.\n\"\"\"",
                "source_code": "return img.transpose(1, 0, 2) if len(img.shape) > 2 else img.transpose(1, 0)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "rot90",
                "second_doc": "\"\"\"\nRotate the input image by 90 degrees a specified number of times and ensure the result has contiguous memory layout.\n\nArgs:\n    img (numpy.ndarray): Input image to be rotated.\n    factor (int): Number of times the image is rotated by 90 degrees.\n\nReturns:\n    numpy.ndarray: Rotated image with contiguous memory layout.\n\nWhy:\n    Ensuring images are consistently oriented and have a standard memory structure facilitates downstream processing in image transformation pipelines and avoids unexpected issues during model training.\n\"\"\"",
                "source_code": "img = np.rot90(img, factor)\n    return np.ascontiguousarray(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "normalize",
                "second_doc": "\"\"\"\nNormalizes an image array by adjusting its mean and standard deviation.\n\nThis method standardizes pixel values of the input image according to provided mean and standard deviation, which helps bring data into a consistent numerical range for further processing and model training.\n\nArgs:\n    img (np.ndarray): Input image to be normalized.\n    mean (Sequence[float]): Mean values for each channel to be subtracted from the image.\n    std (Sequence[float]): Standard deviation values for each channel used to scale the image.\n    max_pixel_value (float): Maximum possible pixel value in the image (e.g., 255 for 8-bit images).\n\nReturns:\n    np.ndarray: Normalized image with the same shape as the input.\n\"\"\"",
                "source_code": "mean = np.array(mean, dtype=np.float32)\n    mean *= max_pixel_value\n\n    std = np.array(std, dtype=np.float32)\n    std *= max_pixel_value\n\n    denominator = np.reciprocal(std, dtype=np.float32)\n\n    img = img.astype(np.float32)\n    img -= mean\n    img *= denominator\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "cutout",
                "second_doc": "\"\"\"\nApplies rectangular mask regions to the provided image, replacing specified areas ('holes') with a given fill value. This simulates occlusions, promoting model robustness to missing or obscured parts of images during training.\n\nArgs:\n    img (numpy.ndarray): Input image to apply cutout operation on.\n    holes (list of tuple): List of rectangles to mask, each represented as (x1, y1, x2, y2) coordinates.\n    fill_value (int or float or tuple): Value to fill the cutout regions with.\n\nReturns:\n    numpy.ndarray: Image with specified regions masked out.\n\"\"\"",
                "source_code": "img = img.copy()\n    for x1, y1, x2, y2 in holes:\n        img[y1:y2, x1:x2] = fill_value\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Wrap OpenCV function to enable processing images with more than 4 channels.\n\n    Limitations:\n        This wrapper requires image to be the first argument and rest must be sent via named arguments.\n\n    Args:\n        process_fn: Transform function (e.g cv2.resize).\n        kwargs: Additional parameters.\n\n    Returns:\n        numpy.ndarray: Transformed image.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nProcesses an image using the provided function, optionally handling images with more than four channels by processing them in chunks.\n\nArgs:\n    process_fn: The function used to process each image or image chunk.\n\nReturns:\n    function: A wrapper function that takes an image as input, processes it using process_fn (splitting into channel chunks if necessary), and returns the processed image.\n\"\"\"",
                "method_name": "_maybe_process_in_chunks",
                "second_doc": "\"\"\"\nApplies a given processing function to an image, automatically splitting and processing the image in channel groups when the number of channels exceeds four. This ensures the processing function can handle images of varying channel sizes without modification, maximizing compatibility and resource efficiency.\n\nArgs:\n    process_fn (callable): The function used to process each image or channel chunk.\n    **kwargs: Additional keyword arguments passed to the processing function.\n\nReturns:\n    function: A wrapper that accepts an image, applies process_fn (optionally in channel-wise chunks), and returns the processed image.\n\"\"\"",
                "source_code": "def __process_fn(img):\n        num_channels = get_num_channels(img)\n        if num_channels > 4:\n            chunks = []\n            for index in range(0, num_channels, 4):\n                chunk = img[:, :, index : index + 4]\n                chunk = process_fn(chunk, **kwargs)\n                chunks.append(chunk)\n            img = np.dstack(chunks)\n        else:\n            img = process_fn(img, **kwargs)\n        return img\n\n    return __process_fn"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "rotate",
                "second_doc": "\"\"\"\nRotate the input image by a specified angle while preserving its center and dimensions.\n\nThis method computes an affine transformation matrix to rotate the image about its center and then applies the transformation using the specified interpolation and border handling parameters. This operation facilitates the simulation of different viewpoints and orientations in images, thus introducing variability that can be leveraged during data processing.\n\nArgs:\n    img (numpy.ndarray): Image to be rotated.\n    angle (float): The rotation angle in degrees.\n    interpolation (int): Interpolation method to use for resampling.\n    border_mode (int): Pixel extrapolation method if the image is rotated beyond its boundaries.\n    value (tuple or int): Value used for padding if border_mode is set accordingly.\n\nReturns:\n    numpy.ndarray: The rotated image.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n    matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1.0)\n\n    warp_fn = _maybe_process_in_chunks(\n        cv2.warpAffine, M=matrix, dsize=(width, height), flags=interpolation, borderMode=border_mode, borderValue=value\n    )\n    return warp_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "resize",
                "second_doc": "\"\"\"\nResize an input image to the specified width and height using the given interpolation method.\n\nThis method is used to ensure that all images processed downstream meet consistent dimensional requirements, which is crucial for batch processing in computer vision workflows.\n\nArgs:\n    img (numpy.ndarray): Input image to be resized.\n    width (int): Target width of the output image.\n    height (int): Target height of the output image.\n    interpolation (int): Interpolation method to be used for resizing.\n\nReturns:\n    numpy.ndarray: Resized image with the specified dimensions.\n\"\"\"",
                "source_code": "resize_fn = _maybe_process_in_chunks(cv2.resize, dsize=(width, height), interpolation=interpolation)\n    return resize_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "scale",
                "second_doc": "\"\"\"\nScales the input image by a given factor using the specified interpolation method.\n\nThis method resizes the image proportionally to generate additional training examples with varied object sizes and spatial layouts, which helps models become more robust to scale changes.\n\nArgs:\n    img (numpy.ndarray): Input image to be scaled.\n    scale (float): Scaling factor to resize the image.\n    interpolation (int): Interpolation method to use for resizing.\n\nReturns:\n    numpy.ndarray: The resized image with updated dimensions.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n    new_height, new_width = int(height * scale), int(width * scale)\n    return resize(img, new_height, new_width, interpolation)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "shift_scale_rotate",
                "second_doc": "\"\"\"\nApply affine transformation to the input image by shifting, scaling, and rotating it.\n\nThis method changes the spatial configuration of the image according to specified parameters, which helps increase the diversity and variability of the dataset to make models more robust to geometric distortions.\n\nArgs:\n    img (np.ndarray): Input image to be transformed.\n    angle (float): Angle (in degrees) to rotate the image.\n    scale (float): Factor by which to scale the image.\n    dx (float): Fractional shift along the horizontal axis (relative to image width).\n    dy (float): Fractional shift along the vertical axis (relative to image height).\n    interpolation (int): Interpolation method for image transformation.\n    border_mode (int): Pixel extrapolation method, used when the transformed image requires a value outside of its boundaries.\n    value (tuple or int or float): Value used for filling pixels outside the input image.\n\nReturns:\n    np.ndarray: Transformed image with applied shift, scale, and rotation.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n    center = (width / 2, height / 2)\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * width\n    matrix[1, 2] += dy * height\n\n    warp_affine_fn = _maybe_process_in_chunks(\n        cv2.warpAffine, M=matrix, dsize=(width, height), flags=interpolation, borderMode=border_mode, borderValue=value\n    )\n    return warp_affine_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "bbox_shift_scale_rotate",
                "second_doc": "\"\"\"\nApply shift, scale, and rotation transformations to a bounding box, updating its coordinates to match the applied geometric changes in image space.\n\nArgs:\n    bbox (tuple or list): Bounding box in normalized coordinates (x_min, y_min, x_max, y_max).\n    angle (float): Angle in degrees to rotate the bounding box around the image center.\n    scale (float): Scaling factor to resize the bounding box.\n    dx (float): Horizontal shift as a fraction of image width.\n    dy (float): Vertical shift as a fraction of image height.\n    rows (int): Image height in pixels.\n    cols (int): Image width in pixels.\n\nReturns:\n    tuple: Transformed bounding box coordinates (x_min, y_min, x_max, y_max) in normalized form.\n\nWhy:\n    This method recalculates bounding box positions to ensure their alignment with objects after the associated image region has been geometrically altered. This is crucial for maintaining accurate object localization when images are augmented during model training or data preparation.\n\"\"\"",
                "source_code": "x_min, y_min, x_max, y_max = bbox[:4]\n    height, width = rows, cols\n    center = (width / 2, height / 2)\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * width\n    matrix[1, 2] += dy * height\n    x = np.array([x_min, x_max, x_max, x_min])\n    y = np.array([y_min, y_min, y_max, y_max])\n    ones = np.ones(shape=(len(x)))\n    points_ones = np.vstack([x, y, ones]).transpose()\n    points_ones[:, 0] *= width\n    points_ones[:, 1] *= height\n    tr_points = matrix.dot(points_ones.T).T\n    tr_points[:, 0] /= width\n    tr_points[:, 1] /= height\n\n    x_min, x_max = min(tr_points[:, 0]), max(tr_points[:, 0])\n    y_min, y_max = min(tr_points[:, 1]), max(tr_points[:, 1])\n\n    return x_min, y_min, x_max, y_max"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "keypoint_shift_scale_rotate",
                "second_doc": "\"\"\"\nApply geometric transformations (shift, scaling, rotation) to a keypoint, updating its position, angle, and scale accordingly.\n\nThis method ensures that as the image undergoes affine transformations, the associated keypoints are correctly adjusted so that their spatial relationships and orientations remain consistent with the augmented image. Such adjustments are essential for maintaining annotation accuracy during preprocessing or data augmentation.\n\nArgs:\n    keypoint (sequence): A sequence containing at least the (x, y, angle, scale) parameters of the keypoint.\n    angle (float): Angle in degrees to rotate the keypoint around the image center.\n    scale (float): Scaling factor to apply to the keypoint.\n    dx (float): Horizontal shift as a fraction of the image width.\n    dy (float): Vertical shift as a fraction of the image height.\n    rows (int): Number of rows (height) in the image.\n    cols (int): Number of columns (width) in the image.\n\nReturns:\n    tuple: Transformed (x, y, angle, scale) of the keypoint after applying shift, scaling, and rotation.\n\"\"\"",
                "source_code": "x, y, a, s, = keypoint[:4]\n    height, width = rows, cols\n    center = (width / 2, height / 2)\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * width\n    matrix[1, 2] += dy * height\n\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    angle = a + math.radians(angle)\n    scale = s * scale\n\n    return x, y, angle, scale"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "crop",
                "second_doc": "\"\"\"\nCrop the input image to a specified rectangular region.\n\nThis method extracts a subregion from an image based on the given pixel coordinates, enabling precise control over the regions of interest for further processing or augmentation. It validates the provided coordinates to ensure they form a valid rectangle and remain within the image boundaries, preventing errors related to invalid cropping.\n\nArgs:\n    img (numpy.ndarray): Input image from which to crop a region.\n    x_min (int): Left boundary of the crop box.\n    y_min (int): Top boundary of the crop box.\n    x_max (int): Right boundary of the crop box (exclusive).\n    y_max (int): Bottom boundary of the crop box (exclusive).\n\nReturns:\n    numpy.ndarray: Cropped image region defined by the coordinates (y_min:y_max, x_min:x_max).\n\nRaises:\n    ValueError: If the crop coordinates do not define a valid rectangle or fall outside the image boundaries.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n    if x_max <= x_min or y_max <= y_min:\n        raise ValueError(\n            \"We should have x_min < x_max and y_min < y_max. But we got\"\n            \" (x_min = {x_min}, y_min = {y_min}, x_max = {x_max}, y_max = {y_max})\".format(\n                x_min=x_min, x_max=x_max, y_min=y_min, y_max=y_max\n            )\n        )\n\n    if x_min < 0 or x_max > width or y_min < 0 or y_max > height:\n        raise ValueError(\n            \"Values for crop should be non negative and equal or smaller than image sizes\"\n            \"(x_min = {x_min}, y_min = {y_min}, x_max = {x_max}, y_max = {y_max}\"\n            \"height = {height}, width = {width})\".format(\n                x_min=x_min, x_max=x_max, y_min=y_min, y_max=y_max, height=height, width=width\n            )\n        )\n\n    return img[y_min:y_max, x_min:x_max]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "get_center_crop_coords",
                "second_doc": "\"\"\"\nCalculate the coordinates required to extract a crop of specific size from the center of an image. Center cropping is commonly used to ensure that important objects, typically located near the image center, are retained while standardizing input dimensions for further processing.\n\nArgs:\n    height (int): Original image height.\n    width (int): Original image width.\n    crop_height (int): Desired height of the center crop.\n    crop_width (int): Desired width of the center crop.\n\nReturns:\n    tuple: Coordinates of the crop in the form (x1, y1, x2, y2), representing the top-left and bottom-right corners.\n\"\"\"",
                "source_code": "y1 = (height - crop_height) // 2\n    y2 = y1 + crop_height\n    x1 = (width - crop_width) // 2\n    x2 = x1 + crop_width\n    return x1, y1, x2, y2"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "center_crop",
                "second_doc": "\"\"\"\nCrop the central region of the input image to the specified size.\n\nThis method ensures that the output image retains the most spatially relevant portion by extracting a centered crop, which is particularly useful for standardizing input dimensions and focusing on the main area of interest.\n\nArgs:\n    img (numpy.ndarray): Input image to be cropped.\n    crop_height (int): Desired height of the cropped image.\n    crop_width (int): Desired width of the cropped image.\n\nReturns:\n    numpy.ndarray: Cropped image with dimensions (crop_height, crop_width).\n\nRaises:\n    ValueError: If the requested crop size is larger than the input image.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n    if height < crop_height or width < crop_width:\n        raise ValueError(\n            \"Requested crop size ({crop_height}, {crop_width}) is \"\n            \"larger than the image size ({height}, {width})\".format(\n                crop_height=crop_height, crop_width=crop_width, height=height, width=width\n            )\n        )\n    x1, y1, x2, y2 = get_center_crop_coords(height, width, crop_height, crop_width)\n    img = img[y1:y2, x1:x2]\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "get_random_crop_coords",
                "second_doc": "\"\"\"\nCalculate the coordinates for cropping a sub-region from an image based on the specified crop size and starting ratios.\n\nThis method determines the exact rectangle to crop from an image, allowing randomized cropping essential for diversifying input data during augmentation.\n\nArgs:\n    height (int): The height of the original image.\n    width (int): The width of the original image.\n    crop_height (int): The height of the crop.\n    crop_width (int): The width of the crop.\n    h_start (float): Vertical starting position as a float in [0, 1], indicating the crop's top edge.\n    w_start (float): Horizontal starting position as a float in [0, 1], indicating the crop's left edge.\n\nReturns:\n    tuple: Coordinates (x1, y1, x2, y2) defining the cropped rectangle within the image.\n\"\"\"",
                "source_code": "y1 = int((height - crop_height) * h_start)\n    y2 = y1 + crop_height\n    x1 = int((width - crop_width) * w_start)\n    x2 = x1 + crop_width\n    return x1, y1, x2, y2"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "random_crop",
                "second_doc": "\"\"\"\nPerforms a random crop of the input image to the specified height and width.\n\nThis method randomly selects a region of the given dimensions from the original image. Cropping in this way serves to produce varied image patches, which helps to simulate a wider range of possible inputs and reduce overfitting when training computer vision models.\n\nArgs:\n    img (np.ndarray): Input image to be cropped.\n    crop_height (int): Height of the cropped region.\n    crop_width (int): Width of the cropped region.\n    h_start (float, optional): Vertical starting position for the crop, as a fraction of the image height.\n    w_start (float, optional): Horizontal starting position for the crop, as a fraction of the image width.\n\nReturns:\n    np.ndarray: Cropped image region.\n\nRaises:\n    ValueError: If the requested crop size is larger than the dimensions of the input image.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n    if height < crop_height or width < crop_width:\n        raise ValueError(\n            \"Requested crop size ({crop_height}, {crop_width}) is \"\n            \"larger than the image size ({height}, {width})\".format(\n                crop_height=crop_height, crop_width=crop_width, height=height, width=width\n            )\n        )\n    x1, y1, x2, y2 = get_random_crop_coords(height, width, crop_height, crop_width, h_start, w_start)\n    img = img[y1:y2, x1:x2]\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "clamping_crop",
                "second_doc": "\"\"\"\nCrop an image to the specified bounding box coordinates, adjusting coordinates that fall outside the image boundaries to valid values.\n\nArgs:\n    img (numpy.ndarray): The input image array to be cropped.\n    x_min (int or float): The minimum x-coordinate of the crop box.\n    y_min (int or float): The minimum y-coordinate of the crop box.\n    x_max (int or float): The maximum x-coordinate of the crop box.\n    y_max (int or float): The maximum y-coordinate of the crop box.\n\nReturns:\n    numpy.ndarray: The cropped region of the input image.\n\nThis method ensures that the specified crop region remains within the image bounds to prevent errors related to out-of-bounds indexing. This allows for robust and reliable cropping during transformations on image data.\n\"\"\"",
                "source_code": "h, w = img.shape[:2]\n    if x_min < 0:\n        x_min = 0\n    if y_min < 0:\n        y_min = 0\n    if y_max >= h:\n        y_max = h - 1\n    if x_max >= w:\n        x_max = w - 1\n    return img[int(y_min) : int(y_max), int(x_min) : int(x_max)]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_shift_hsv_uint8",
                "second_doc": "\"\"\"\nAdjust the hue, saturation, and value (brightness) channels of an 8-bit RGB image by specified shift values using efficient lookup tables. This operation is intended to modify the color characteristics of images, making them more diverse for use in data preprocessing and augmentation.\n\nArgs:\n    img (np.ndarray): Input image array in uint8 RGB format.\n    hue_shift (int): Value to shift the hue channel (modulo 180).\n    sat_shift (int): Value to shift (increase or decrease) the saturation channel.\n    val_shift (int): Value to shift (increase or decrease) the value (brightness) channel.\n\nReturns:\n    np.ndarray: Output image in uint8 RGB format with hue, saturation, and value adjusted.\n\nWhy:\n    Adjusting color attributes in images increases data variety, helping models learn to recognize objects under different visual conditions and improving robustness to lighting and color variations.\n\"\"\"",
                "source_code": "dtype = img.dtype\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    hue, sat, val = cv2.split(img)\n\n    lut_hue = np.arange(0, 256, dtype=np.int16)\n    lut_hue = np.mod(lut_hue + hue_shift, 180).astype(dtype)\n\n    lut_sat = np.arange(0, 256, dtype=np.int16)\n    lut_sat = np.clip(lut_sat + sat_shift, 0, 255).astype(dtype)\n\n    lut_val = np.arange(0, 256, dtype=np.int16)\n    lut_val = np.clip(lut_val + val_shift, 0, 255).astype(dtype)\n\n    hue = cv2.LUT(hue, lut_hue)\n    sat = cv2.LUT(sat, lut_sat)\n    val = cv2.LUT(val, lut_val)\n\n    img = cv2.merge((hue, sat, val)).astype(dtype)\n    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_shift_hsv_non_uint8",
                "second_doc": "\"\"\"\nAdjusts the hue, saturation, and value (HSV) channels of an image with arbitrary numerical data types, ensuring that color manipulations remain consistent across different image formats.\n\nThis method is used to manipulate color properties in order to introduce color variation and enhance the diversity of image datasets during preprocessing or augmentation steps.\n\nArgs:\n    img (numpy.ndarray): Input image array, expected in RGB color space with a non-uint8 dtype.\n    hue_shift (float or int): Value to shift the hue channel.\n    sat_shift (float or int): Value to shift the saturation channel.\n    val_shift (float or int): Value to shift the value (brightness) channel.\n\nReturns:\n    numpy.ndarray: The RGB image after applying the specified HSV shifts, with the same dtype as the input image.\n\"\"\"",
                "source_code": "dtype = img.dtype\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    hue, sat, val = cv2.split(img)\n\n    hue = cv2.add(hue, hue_shift)\n    hue = np.where(hue < 0, hue + 180, hue)\n    hue = np.where(hue > 180, hue - 180, hue)\n    hue = hue.astype(dtype)\n    sat = clip(cv2.add(sat, sat_shift), dtype, 255 if dtype == np.uint8 else 1.0)\n    val = clip(cv2.add(val, val_shift), dtype, 255 if dtype == np.uint8 else 1.0)\n    img = cv2.merge((hue, sat, val)).astype(dtype)\n    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "shift_hsv",
                "second_doc": "\"\"\"\nApplies separate shifts to the hue, saturation, and value (HSV) color channels of an input image. The specific shifting method is chosen based on the image's data type to ensure accurate and efficient results.\n\nArgs:\n    img (np.ndarray): Input image in a format compatible with HSV manipulation, either with dtype uint8 or another type.\n    hue_shift (float or int): Amount to shift the hue channel.\n    sat_shift (float or int): Amount to shift the saturation channel.\n    val_shift (float or int): Amount to shift the value (brightness) channel.\n\nReturns:\n    np.ndarray: Image with adjusted HSV channels.\n    \nWhy:\n    Adjusting HSV channels allows for dynamic modification of color characteristics, helping prepare the image data for scenarios sensitive to color variance.\n\"\"\"",
                "source_code": "if img.dtype == np.uint8:\n        return _shift_hsv_uint8(img, hue_shift, sat_shift, val_shift)\n\n    return _shift_hsv_non_uint8(img, hue_shift, sat_shift, val_shift)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Invert all pixel values above a threshold.\n\n    Args:\n        img (numpy.ndarray): The image to solarize.\n        threshold (int): All pixels above this greyscale level are inverted.\n\n    Returns:\n        numpy.ndarray: Solarized image.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nApplies a solarization effect to the input image.\n\nThis method inverts all pixel values above a certain threshold, producing a unique visual effect known as solarization. The inversion is dependent on the image's data type and is optimized for both uint8 and other supported numerical types.\n\nArgs:\n    img: Input image to be solarized.\n\nReturns:\n    The solarized image, with pixels above the threshold inverted.\n\"\"\"",
                "method_name": "solarize",
                "second_doc": "\"\"\"\nApplies a solarization effect by inverting pixel values above a specified threshold in the input image.\n\nThis transformation is useful for introducing controlled visual alterations that increase data variability, which can help machine learning models better handle a wider range of visual scenarios. The operation adapts to the image data type for optimal efficiency and supports both uint8 and other numerical types.\n\nArgs:\n    img (np.ndarray): Input image to be solarized. The image should be a NumPy array with a supported numerical dtype.\n\nReturns:\n    np.ndarray: The solarized image where all pixel values above the threshold have been inverted, preserving the input's original shape and data type.\n\"\"\"",
                "source_code": "dtype = img.dtype\n    max_val = MAX_VALUES_BY_DTYPE[dtype]\n\n    if dtype == np.dtype(\"uint8\"):\n        lut = [(i if i < threshold else max_val - i) for i in range(max_val + 1)]\n\n        prev_shape = img.shape\n        img = cv2.LUT(img, np.array(lut, dtype=dtype))\n\n        if len(prev_shape) != len(img.shape):\n            img = np.expand_dims(img, -1)\n        return img\n\n    result_img = img.copy()\n    cond = img >= threshold\n    result_img[cond] = max_val - result_img[cond]\n    return result_img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Reduce the number of bits for each color channel.\n\n    Args:\n        img (numpy.ndarray): image to posterize.\n        bits (int): number of high bits. Must be in range [0, 8]\n\n    Returns:\n        numpy.ndarray: Image with reduced color channels.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nReduces the number of bits used to represent the color intensity in each image channel, performing a posterization effect.\n\nArgs:\n    img: Input image array to be posterized. Must have uint8 dtype.\n    bits: Desired number of bits per channel. Can be a scalar (applied to all channels) or an iterable for each channel. Values must be in range [0, 8].\n\nReturns:\n    ndarray: Posterized image with the specified number of bits per channel. Retains the shape and dtype of the input image.\n\"\"\"",
                "method_name": "posterize",
                "second_doc": "\"\"\"\nApplies a bit reduction to each image channel, limiting the number of distinct intensity levels per channel and generating a stylized posterization effect.\n\nThis transformation emphasizes large color regions and reduces subtle variations in pixel values, which can help increase the variability and robustness of training data in vision tasks.\n\nArgs:\n    img (ndarray): Input image array to be processed. Must have dtype uint8.\n    bits (int or array-like of int): Number of bits to retain per channel. May be a scalar to apply to all channels, or an iterable specifying different values for each channel. Acceptable range for each value is [0, 8].\n\nReturns:\n    ndarray: Output image where each channel has been quantized to the specified number of bits, preserving the input shape and dtype.\n\"\"\"",
                "source_code": "bits = np.uint8(bits)\n\n    assert img.dtype == np.uint8, \"Image must have uint8 channel type\"\n    assert np.all((0 <= bits) & (bits <= 8)), \"bits must be in range [0, 8]\"\n\n    if not bits.shape or len(bits) == 1:\n        if bits == 0:\n            return np.zeros_like(img)\n        elif bits == 8:\n            return img.copy()\n\n        lut = np.arange(0, 256, dtype=np.uint8)\n        mask = ~np.uint8(2 ** (8 - bits) - 1)\n        lut &= mask\n\n        return cv2.LUT(img, lut)\n\n    assert is_rgb_image(img), \"If bits is iterable image must be RGB\"\n\n    result_img = np.empty_like(img)\n    for i, channel_bits in enumerate(bits):\n        if channel_bits == 0:\n            result_img[..., i] = np.zeros_like(img[..., i])\n            continue\n        elif channel_bits == 8:\n            result_img[..., i] = img[..., i].copy()\n            continue\n\n        lut = np.arange(0, 256, dtype=np.uint8)\n        mask = ~np.uint8(2 ** (8 - channel_bits) - 1)\n        lut &= mask\n\n        result_img[..., i] = cv2.LUT(img[..., i], lut)\n\n    return result_img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_equalize_pil",
                "second_doc": "\"\"\"\nPerforms contrast equalization on a grayscale image using histogram-based intensity mapping to enhance the distribution of pixel values.\n\nArgs:\n    img (numpy.ndarray): The input grayscale image to be equalized.\n    mask (numpy.ndarray or None): Optional mask. If provided, computation limits the equalization to the masked region.\n\nReturns:\n    numpy.ndarray: The image after contrast equalization. Returns a copy of the original image if equalization is not applicable.\n\nThis method applies histogram equalization to adjust pixel intensity levels, facilitating further image transformations and improving visual distinction within the image.\n\"\"\"",
                "source_code": "histogram = cv2.calcHist([img], [0], mask, [256], (0, 256)).ravel()\n    h = [_f for _f in histogram if _f]\n\n    if len(h) <= 1:\n        return img.copy()\n\n    step = np.sum(h[:-1]) // 255\n    if not step:\n        return img.copy()\n\n    lut = np.empty(256, dtype=np.uint8)\n    n = step // 2\n    for i in range(256):\n        lut[i] = min(n // step, 255)\n        n += histogram[i]\n\n    return cv2.LUT(img, np.array(lut))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_equalize_cv",
                "second_doc": "\"\"\"\nApply histogram equalization to the input grayscale image, optionally constrained to a masked region.\n\nThis method redistributes the image intensities to improve overall contrast by stretching out the most frequent intensity values. If a mask is provided, equalization is applied only within the masked area, preserving the intensity context and enhancing local contrast where needed. This adaptation supports robust preprocessing for various vision tasks where image quality may vary or regions of interest need special treatment.\n\nArgs:\n    img (numpy.ndarray): Grayscale input image to be equalized.\n    mask (numpy.ndarray or None): Optional mask. If provided, defines the region of the image to be equalized; otherwise, the full image is processed.\n\nReturns:\n    numpy.ndarray: The equalized image, with contrast enhanced globally or within the masked region.\n\"\"\"",
                "source_code": "if mask is None:\n        return cv2.equalizeHist(img)\n\n    histogram = cv2.calcHist([img], [0], mask, [256], (0, 256)).ravel()\n    i = 0\n    for val in histogram:\n        if val > 0:\n            break\n        i += 1\n    i = min(i, 255)\n\n    total = np.sum(histogram)\n    if histogram[i] == total:\n        return np.full_like(img, i)\n\n    scale = 255.0 / (total - histogram[i])\n    _sum = 0\n\n    lut = np.zeros(256, dtype=np.uint8)\n    i += 1\n    for i in range(i, len(histogram)):\n        _sum += histogram[i]\n        lut[i] = clip(round(_sum * scale), np.dtype(\"uint8\"), 255)\n\n    return cv2.LUT(img, lut)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Equalize the image histogram.\n\n    Args:\n        img (numpy.ndarray): RGB or grayscale image.\n        mask (numpy.ndarray): An optional mask.  If given, only the pixels selected by\n            the mask are included in the analysis. Maybe 1 channel or 3 channel array.\n        mode (str): {'cv', 'pil'}. Use OpenCV or Pillow equalization method.\n        by_channels (bool): If True, use equalization by channels separately,\n            else convert image to YCbCr representation and use equalization by `Y` channel.\n\n    Returns:\n        numpy.ndarray: Equalized image.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nApplies histogram equalization to an input image using a specified mode.\n\nArgs:\n    img: The input image to be equalized. Must have dtype 'uint8'.\n\nReturns:\n    A new image array with equalized histogram, maintaining the same shape and dtype as the input image.\n\"\"\"",
                "method_name": "equalize",
                "second_doc": "\"\"\"\nEnhances the contrast of the input image by applying histogram equalization using the chosen algorithm. This process redistributes pixel intensities to improve visual clarity and highlight features, which can aid in consistent feature extraction for further processing or analysis.\n\nArgs:\n    img: numpy.ndarray\n        The input image to be equalized. Must be of dtype 'uint8'.\n    mode: str\n        The equalization mode to use; options are \"cv\" for OpenCV or \"pil\" for PIL-based implementation.\n    mask: numpy.ndarray, optional\n        If provided, restricts equalization to regions where the mask is nonzero. Must be broadcastable to image shape.\n    by_channels: bool, optional\n        If True, applies equalization independently to each channel. If False, applies jointly or with specified logic. Default is False.\n\nReturns:\n    numpy.ndarray:\n        An image array with enhanced contrast via histogram equalization, having the same shape and dtype as the input image.\n\"\"\"",
                "source_code": "assert img.dtype == np.uint8, \"Image must have uint8 channel type\"\n\n    modes = [\"cv\", \"pil\"]\n\n    if mode not in modes:\n        raise ValueError(\"Unsupported equalization mode. Supports: {}. \" \"Got: {}\".format(modes, mode))\n    if mask is not None:\n        if is_rgb_image(mask) and is_grayscale_image(img):\n            raise ValueError(\"Wrong mask shape. Image shape: {}. \" \"Mask shape: {}\".format(img.shape, mask.shape))\n        if not by_channels and not is_grayscale_image(mask):\n            raise ValueError(\n                \"When by_channels=False only 1-channel mask supports. \" \"Mask shape: {}\".format(mask.shape)\n            )\n\n    if mode == \"pil\":\n        function = _equalize_pil\n    else:\n        function = _equalize_cv\n\n    if mask is not None:\n        mask = mask.astype(np.uint8)\n\n    if is_grayscale_image(img):\n        return function(img, mask)\n\n    if not by_channels:\n        result_img = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n        result_img[..., 0] = function(result_img[..., 0], mask)\n        return cv2.cvtColor(result_img, cv2.COLOR_YCrCb2RGB)\n\n    result_img = np.empty_like(img)\n    for i in range(3):\n        if mask is None:\n            _mask = None\n        elif is_grayscale_image(mask):\n            _mask = mask\n        else:\n            _mask = mask[..., i]\n\n        result_img[..., i] = function(img[..., i], _mask)\n\n    return result_img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_shift_rgb_non_uint8",
                "second_doc": "\"\"\"\nShift the RGB channels of an image by specified amounts for each channel.\n\nThis method applies an additive shift to the red, green, and blue channels of a non-uint8 image, enabling fine control over color adjustments at the channel level. If all channels have the same shift value, the operation is optimized. This functionality facilitates systematic modification of image color distributions, which can be useful for adjusting color variations in datasets.\n\nArgs:\n    img (np.ndarray): Input image array with shape (..., 3), where channels are ordered as red, green, and blue.\n    r_shift (float or np.ndarray): Value to add to the red channel.\n    g_shift (float or np.ndarray): Value to add to the green channel.\n    b_shift (float or np.ndarray): Value to add to the blue channel.\n\nReturns:\n    np.ndarray: Image array with the same shape as input, but with shifted RGB channel values.\n\"\"\"",
                "source_code": "if r_shift == g_shift == b_shift:\n        return img + r_shift\n\n    result_img = np.empty_like(img)\n    shifts = [r_shift, g_shift, b_shift]\n    for i, shift in enumerate(shifts):\n        result_img[..., i] = img[..., i] + shift\n\n    return result_img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_shift_image_uint8",
                "second_doc": "\"\"\"\nApplies an additive shift to the pixel values of an 8-bit image by utilizing a lookup table, ensuring values remain within valid intensity bounds.\n\nThis method is used to systematically alter image intensities, which can help in simulating variations in lighting conditions or sensor sensitivity.\n\nArgs:\n    img (np.ndarray): Input image array of type uint8.\n    value (int or float): The value to add to each pixel in the image.\n\nReturns:\n    np.ndarray: The image with shifted pixel values, maintaining the same shape and type as the input.\n\"\"\"",
                "source_code": "max_value = MAX_VALUES_BY_DTYPE[img.dtype]\n\n    lut = np.arange(0, max_value + 1).astype(\"float32\")\n    lut += value\n\n    lut = np.clip(lut, 0, max_value).astype(img.dtype)\n    return cv2.LUT(img, lut)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_shift_rgb_uint8",
                "second_doc": "\"\"\"\nShift the R, G, and B channels of an 8-bit unsigned integer image by specified pixel amounts.\n\nThis method applies horizontal shifts to each color channel independently. If all channel shifts are equal, a more efficient approach is used by reshaping and shifting the entire image at once. Otherwise, each channel is shifted separately, and the results are combined into a new image. Shifting individual color channels can create diverse visual distortions, enriching the variety of augmented images.\n\nArgs:\n    img (np.ndarray): Input 3D NumPy array representing an RGB image in uint8 format.\n    r_shift (int): Number of pixels to shift the red channel horizontally.\n    g_shift (int): Number of pixels to shift the green channel horizontally.\n    b_shift (int): Number of pixels to shift the blue channel horizontally.\n\nReturns:\n    np.ndarray: The resulting image after shifting the RGB channels, with the same shape and dtype as the input.\n\"\"\"",
                "source_code": "if r_shift == g_shift == b_shift:\n        h, w, c = img.shape\n        img = img.reshape([h, w * c])\n\n        return _shift_image_uint8(img, r_shift)\n\n    result_img = np.empty_like(img)\n    shifts = [r_shift, g_shift, b_shift]\n    for i, shift in enumerate(shifts):\n        result_img[..., i] = _shift_image_uint8(img[..., i], shift)\n\n    return result_img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "shift_rgb",
                "second_doc": "\"\"\"\nApply separate shifts to the red, green, and blue channels of an image to adjust its color balance.\n\nThis method determines the proper shifting function based on the data type of the input image to ensure efficient and accurate color manipulation. Shifting individual RGB channels can create augmented samples that help models become more robust to varying lighting and color conditions.\n\nArgs:\n    img (np.ndarray): Input image array of shape (H, W, 3), either with dtype uint8 or another numeric type.\n    r_shift (float): Value to add to the red channel.\n    g_shift (float): Value to add to the green channel.\n    b_shift (float): Value to add to the blue channel.\n\nReturns:\n    np.ndarray: Image array with the RGB channels shifted accordingly, of the same shape and dtype as the input.\n\"\"\"",
                "source_code": "if img.dtype == np.uint8:\n        return _shift_rgb_uint8(img, r_shift, g_shift, b_shift)\n\n    return _shift_rgb_non_uint8(img, r_shift, g_shift, b_shift)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "clahe",
                "second_doc": "\"\"\"\nApply Contrast Limited Adaptive Histogram Equalization (CLAHE) to enhance the contrast of an input image.\n\nThis method improves the local contrast of images by redistributing lightness values, making features more distinguishable for further processing or analysis. It supports both grayscale and RGB images with uint8 data types. For RGB images, CLAHE is applied to the luminance channel in the LAB color space to preserve color fidelity.\n\nArgs:\n    img (numpy.ndarray): Input image as a uint8 NumPy array. Can be a 2D grayscale image or a 3D RGB image.\n    clip_limit (float): Threshold for contrast limiting.\n    tile_grid_size (tuple of int): Size of grid for histogram equalization (e.g., (8, 8)).\n\nReturns:\n    numpy.ndarray: Image after applying CLAHE with enhanced contrast.\n\nRaises:\n    TypeError: If the input image is not of dtype uint8.\n\"\"\"",
                "source_code": "if img.dtype != np.uint8:\n        raise TypeError(\"clahe supports only uint8 inputs\")\n\n    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n\n    if len(img.shape) == 2:\n        img = clahe.apply(img)\n    else:\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n        img[:, :, 0] = clahe.apply(img[:, :, 0])\n        img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "pad",
                "second_doc": "\"\"\"\nPads the input image to ensure its height and width are at least the specified minimum values.\nThis ensures that images meet size requirements for consistent processing in image pipelines.\n\nArgs:\n    img (numpy.ndarray): Input image.\n    min_height (int): Minimum required height for the output image.\n    min_width (int): Minimum required width for the output image.\n    border_mode (int): Pixel extrapolation method for border padding.\n    value (int, float, or tuple): Value to be used for padding if required.\n\nReturns:\n    numpy.ndarray: The padded image with dimensions at least as large as min_height and min_width.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n\n    if height < min_height:\n        h_pad_top = int((min_height - height) / 2.0)\n        h_pad_bottom = min_height - height - h_pad_top\n    else:\n        h_pad_top = 0\n        h_pad_bottom = 0\n\n    if width < min_width:\n        w_pad_left = int((min_width - width) / 2.0)\n        w_pad_right = min_width - width - w_pad_left\n    else:\n        w_pad_left = 0\n        w_pad_right = 0\n\n    img = pad_with_params(img, h_pad_top, h_pad_bottom, w_pad_left, w_pad_right, border_mode, value)\n\n    assert img.shape[0] == max(min_height, height)\n    assert img.shape[1] == max(min_width, width)\n\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "pad_with_params",
                "second_doc": "\"\"\"\nPads an image on each side according to the specified parameters, using the given border mode and value.\n\nThis function ensures that the spatial dimensions of the input image are adjusted to the desired size by adding borders, while preserving the channel dimension.\n\nArgs:\n    img (np.ndarray): Input image to be padded.\n    h_pad_top (int): Number of pixels to pad at the top.\n    h_pad_bottom (int): Number of pixels to pad at the bottom.\n    w_pad_left (int): Number of pixels to pad on the left.\n    w_pad_right (int): Number of pixels to pad on the right.\n    border_mode (int): OpenCV border type (e.g., cv2.BORDER_CONSTANT).\n    value (tuple or int, optional): Padding value if using constant border mode.\n\nReturns:\n    np.ndarray: Padded image with modified spatial dimensions and unchanged channel structure.\n\nWhy:\n    Padding images in this way allows for consistent handling of images of varying sizes, ensuring they meet the requirements for subsequent augmentation or processing steps in computer vision workflows.\n\"\"\"",
                "source_code": "img = cv2.copyMakeBorder(img, h_pad_top, h_pad_bottom, w_pad_left, w_pad_right, border_mode, value=value)\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "blur",
                "second_doc": "\"\"\"\nApply a blurring filter to the input image using a specified kernel size to reduce noise and smooth visual details.\n\nThis method blurs regions of the image in order to simulate real-world visual conditions or imperfections, which helps to improve the robustness of computer vision models by exposing them to more varied input data.\n\nArgs:\n    img (numpy.ndarray): Input image to be blurred.\n    ksize (int): Size of the square kernel to be used for blurring.\n\nReturns:\n    numpy.ndarray: The blurred image with preserved shape as the input.\n\"\"\"",
                "source_code": "blur_fn = _maybe_process_in_chunks(cv2.blur, ksize=(ksize, ksize))\n    return blur_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "gaussian_blur",
                "second_doc": "\"\"\"\nApply a Gaussian blur to the input image to reduce noise and detail, producing a smooth visual effect.\n\nArgs:\n    img (numpy.ndarray): Input image to be blurred.\n    ksize (int): Size of the Gaussian kernel used for blurring.\n\nReturns:\n    numpy.ndarray: The blurred image.\n\nWhy:\n    Blurring helps to minimize small variations and noise in images, facilitating the creation of more robust data samples for further processing or analysis.\n\"\"\"",
                "source_code": "blur_fn = _maybe_process_in_chunks(cv2.GaussianBlur, ksize=(ksize, ksize), sigmaX=0)\n    return blur_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_func_max_size",
                "second_doc": "\"\"\"\nResize the input image so that one of its dimensions matches the specified maximum size while preserving the aspect ratio.\n\nThis method ensures that the image fits within a desired size constraint, facilitating standardization and compatibility in subsequent image processing steps.\n\nArgs:\n    img (numpy.ndarray): Input image to be resized.\n    max_size (int): Desired maximum size for one of the image dimensions.\n    func (callable): Function to determine which image dimension ('height' or 'width') should be matched to 'max_size'.\n    interpolation (int, optional): Interpolation method used for resizing the image.\n\nReturns:\n    numpy.ndarray: The resized image, maintaining the original aspect ratio.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n\n    scale = max_size / float(func(width, height))\n\n    if scale != 1.0:\n        new_height, new_width = tuple(py3round(dim * scale) for dim in (height, width))\n        img = resize(img, height=new_height, width=new_width, interpolation=interpolation)\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "longest_max_size",
                "second_doc": "\"\"\"\nResize the input image so that its largest dimension matches the specified max_size, preserving the aspect ratio. This ensures that images are standardized in size for consistent processing in downstream computer vision workflows.\n\nArgs:\n    img (np.ndarray): Input image to be resized.\n    max_size (int): Desired maximum size for the largest image dimension.\n    interpolation (int): Interpolation method used for resizing.\n\nReturns:\n    np.ndarray: The resized image with the largest side equal to max_size, maintaining aspect ratio.\n\"\"\"",
                "source_code": "return _func_max_size(img, max_size, interpolation, max)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "smallest_max_size",
                "second_doc": "\"\"\"\nResize the input image so that its smallest dimension becomes equal to the specified max_size, preserving the aspect ratio.\n\nArgs:\n    img (np.ndarray): Input image to be resized.\n    max_size (int): Desired size for the smallest dimension of the image.\n    interpolation (int): Interpolation method to be used for resizing.\n\nReturns:\n    np.ndarray: The resized image with its smallest dimension equal to max_size.\n\nWhy:\n    Resizing images to a consistent minimum dimension helps ensure uniformity across datasets, facilitating more effective training of computer vision models.\n\"\"\"",
                "source_code": "return _func_max_size(img, max_size, interpolation, min)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "median_blur",
                "second_doc": "\"\"\"\nApplies a median blur filter to the input image, reducing noise while preserving edges. The kernel size must be 3 or 5 for images of type float32, ensuring compatibility and optimal performance. Processing may be performed in chunks for efficiency with large images.\n\nArgs:\n    img (numpy.ndarray): Input image on which to apply the median blur.\n    ksize (int): Size of the kernel window; must be an odd integer. For float32 images, only 3 or 5 are allowed.\n\nReturns:\n    numpy.ndarray: Blurred image of the same shape and dtype as the input.\n    \nRaises:\n    ValueError: If ksize is not 3 or 5 for float32 images.\n\"\"\"",
                "source_code": "if img.dtype == np.float32 and ksize not in {3, 5}:\n        raise ValueError(\n            \"Invalid ksize value {}. For a float32 image the only valid ksize values are 3 and 5\".format(ksize)\n        )\n\n    blur_fn = _maybe_process_in_chunks(cv2.medianBlur, ksize=ksize)\n    return blur_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "motion_blur",
                "second_doc": "\"\"\"\nApplies a motion blur effect to the input image using the specified convolution kernel. This simulates the visual effect of movement, which helps diversify image samples and supports the creation of robust vision models.\n\nArgs:\n    img (numpy.ndarray): The input image to which the motion blur will be applied.\n    kernel (numpy.ndarray): The convolution kernel that defines the strength and direction of the blur.\n\nReturns:\n    numpy.ndarray: The processed image with the motion blur effect applied.\n\"\"\"",
                "source_code": "blur_fn = _maybe_process_in_chunks(cv2.filter2D, ddepth=-1, kernel=kernel)\n    return blur_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "image_compression",
                "second_doc": "\"\"\"\nCompresses an image using the specified format and quality setting, simulating real-world compression artifacts to modify the image for augmentation purposes.\n\nArgs:\n    img (numpy.ndarray): Input image in uint8 or float32 format.\n    image_type (str): Image encoding format, either '.jpg', '.jpeg', or '.webp'.\n    quality (int): Compression quality level; higher values mean better image quality.\n\nReturns:\n    numpy.ndarray: The compressed and decompressed image, preserving the original shape and data type where possible.\n\nRaises:\n    ValueError: If the input image has an unsupported dtype.\n    NotImplementedError: If the specified image_type is not '.jpg', '.jpeg', or '.webp'.\n\nWhy:\n    This method applies compression and decompression to simulate common artifacts and quality loss encountered in practical scenarios, thereby helping models become more robust to such variations during training.\n\"\"\"",
                "source_code": "if image_type == \".jpeg\" or image_type == \".jpg\":\n        quality_flag = cv2.IMWRITE_JPEG_QUALITY\n    elif image_type == \".webp\":\n        quality_flag = cv2.IMWRITE_WEBP_QUALITY\n    else:\n        NotImplementedError(\"Only '.jpg' and '.webp' compression transforms are implemented. \")\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        warn(\n            \"Image compression augmentation \"\n            \"is most effective with uint8 inputs, \"\n            \"{} is used as input.\".format(input_dtype),\n            UserWarning,\n        )\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for image augmentation\".format(input_dtype))\n\n    _, encoded_img = cv2.imencode(image_type, img, (int(quality_flag), quality))\n    img = cv2.imdecode(encoded_img, cv2.IMREAD_UNCHANGED)\n\n    if needs_float:\n        img = to_float(img, max_value=255)\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Bleaches out pixels, imitation snow.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (numpy.ndarray): Image.\n        snow_point: Number of show points.\n        brightness_coeff: Brightness coefficient.\n\n    Returns:\n        numpy.ndarray: Image.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nAdds a snow effect to an RGB image by modifying its brightness in the HLS color space.\n\nArgs:\n    img: The input RGB image to which the snow effect will be applied.\n    snow_point: A threshold value influencing the regions of the image brightened to create the snow effect.\n    brightness_coeff: The brightness multiplier applied to the relevant portions of the image simulating snow.\n\nReturns:\n    The processed image with a simulated snow effect, in the same dtype as the input.\n\"\"\"",
                "method_name": "add_snow",
                "second_doc": "\"\"\"\nSimulates a snowy weather condition in an RGB image by increasing the brightness of darker regions in the HLS color space.\n\nThis transformation is performed to introduce realistic environmental variation into images, which helps models become more robust to conditions encountered in real-world scenarios.\n\nArgs:\n    img (np.ndarray): The input RGB image to be transformed.\n    snow_point (float): Threshold influencing which areas are brightened to mimic snowfall.\n    brightness_coeff (float): Factor by which the lightness channel is increased in selected areas.\n\nReturns:\n    np.ndarray: The transformed image with brightened regions simulating snow, with the same data type as the input.\n\"\"\"",
                "source_code": "non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    snow_point *= 127.5  # = 255 / 2\n    snow_point += 85  # = 255 / 3\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for RandomSnow augmentation\".format(input_dtype))\n\n    image_HLS = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    image_HLS = np.array(image_HLS, dtype=np.float32)\n\n    image_HLS[:, :, 1][image_HLS[:, :, 1] < snow_point] *= brightness_coeff\n\n    image_HLS[:, :, 1] = clip(image_HLS[:, :, 1], np.uint8, 255)\n\n    image_HLS = np.array(image_HLS, dtype=np.uint8)\n\n    image_RGB = cv2.cvtColor(image_HLS, cv2.COLOR_HLS2RGB)\n\n    if needs_float:\n        image_RGB = to_float(image_RGB, max_value=255)\n\n    return image_RGB"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (numpy.ndarray): Image.\n        slant (int):\n        drop_length:\n        drop_width:\n        drop_color:\n        blur_value (int): Rainy view are blurry.\n        brightness_coefficient (float): Rainy days are usually shady.\n        rain_drops:\n\n    Returns:\n        numpy.ndarray: Image.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nAdds a rain effect to the input image by drawing lines that simulate raindrops and adjusting image brightness.\n\nArgs:\n    img: The input image to apply the rain effect to.\n    slant: The horizontal displacement of raindrops, affecting the angle of each drop.\n    drop_length: The length of each simulated raindrop.\n    drop_width: The width of each raindrop line.\n    drop_color: The color used to draw the raindrops.\n    blur_value: The kernel size for blurring the image to intensify the rainy effect.\n    brightness_coefficient: Coefficient to adjust the brightness of the image after simulating rain.\n    rain_drops: List of tuples specifying the (x, y) coordinates where each raindrop begins.\n\nReturns:\n    The image with simulated rain effect applied, in the same type as the input (uint8 or float32).\n\"\"\"",
                "method_name": "add_rain",
                "second_doc": "\"\"\"\nSimulates rainy weather in an image by drawing angled lines to mimic raindrops, applying blur for atmospheric realism, and reducing overall brightness. This modification serves to expose visual models to challenging weather-induced variations, aiding robustness in downstream tasks.\n\nArgs:\n    img: Input image to be augmented.\n    slant: Horizontal offset for each raindrop, determining its angle.\n    drop_length: Length of each raindrop line.\n    drop_width: Width (thickness) of raindrop lines.\n    drop_color: Color value for drawing the raindrops.\n    blur_value: Size of the blur kernel to reproduce rainy conditions.\n    brightness_coefficient: Factor to decrease image brightness after rain simulation.\n    rain_drops: List of (x, y) tuples marking the starting position of each raindrop.\n\nReturns:\n    Augmented image with simulated rain effect, of the same data type as the original (uint8 or float32).\n\"\"\"",
                "source_code": "non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for RandomSnow augmentation\".format(input_dtype))\n\n    image = img.copy()\n\n    for (rain_drop_x0, rain_drop_y0) in rain_drops:\n        rain_drop_x1 = rain_drop_x0 + slant\n        rain_drop_y1 = rain_drop_y0 + drop_length\n\n        cv2.line(image, (rain_drop_x0, rain_drop_y0), (rain_drop_x1, rain_drop_y1), drop_color, drop_width)\n\n    image = cv2.blur(image, (blur_value, blur_value))  # rainy view are blurry\n    image_hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS).astype(np.float32)\n    image_hls[:, :, 1] *= brightness_coefficient\n\n    image_rgb = cv2.cvtColor(image_hls.astype(np.uint8), cv2.COLOR_HLS2RGB)\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Add fog to the image.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (numpy.ndarray): Image.\n        fog_coef (float): Fog coefficient.\n        alpha_coef (float): Alpha coefficient.\n        haze_list (list):\n\n    Returns:\n        numpy.ndarray: Image.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nApplies a fog effect to the input image using specified parameters.\n\nArgs:\n    img: Input image to which the fog effect will be applied.\n    fog_coef: Coefficient that controls the intensity of the fog effect.\n    alpha_coef: Coefficient that adjusts the alpha blending for the fog overlay.\n    haze_list: List of (x, y) point tuples indicating the centers of fog patches to apply.\n\nReturns:\n    The processed image with the fog effect applied. The data type of the returned image matches the conventions of the input and the applied conversion logic.\n\"\"\"",
                "method_name": "add_fog",
                "second_doc": "\"\"\"\nSimulates foggy conditions on the input image by overlaying semi-transparent white patches and applying a blur effect, thereby introducing challenging visual artifacts. This operation increases visual variability, which is valuable for preparing image datasets for robust model training under diverse environmental conditions.\n\nArgs:\n    img: Input image (NumPy array) to modify.\n    fog_coef: Float that determines the size and intensity of the fog patches.\n    alpha_coef: Float that regulates how much the white overlay blends with the image.\n    haze_list: List of (x, y) tuples specifying the centers for fog patches.\n\nReturns:\n    Processed image with fog effect simulated. The returned image matches the type conventions set by the input and automatic float/integer conversions within the method.\n\"\"\"",
                "source_code": "non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for RandomFog augmentation\".format(input_dtype))\n\n    height, width = img.shape[:2]\n\n    hw = max(int(width // 3 * fog_coef), 10)\n\n    for haze_points in haze_list:\n        x, y = haze_points\n        overlay = img.copy()\n        output = img.copy()\n        alpha = alpha_coef * fog_coef\n        rad = hw // 2\n        point = (x + hw // 2, y + hw // 2)\n        cv2.circle(overlay, point, int(rad), (255, 255, 255), -1)\n        cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n\n        img = output.copy()\n\n    image_rgb = cv2.blur(img, (hw // 10, hw // 10))\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Add sun flare.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (numpy.ndarray):\n        flare_center_x (float):\n        flare_center_y (float):\n        src_radius:\n        src_color (int, int, int):\n        circles (list):\n\n    Returns:\n        numpy.ndarray:\n\n    \"\"\"",
                "first_doc": "\"\"\"\nAdds a sun flare effect to the given image using specified flare parameters.\n\nThe method applies multiple bright circles to mimic a realistic sun flare effect. Flare circles and a central flare source are blended into the image with adjustable alpha blending. The function can handle input images in both uint8 and float32 formats.\n\nArgs:\n    img: The input image to which the sun flare will be applied.\n    flare_center_x: The x-coordinate of the sun flare's central position.\n    flare_center_y: The y-coordinate of the sun flare's central position.\n    src_radius: The radius of the central flare source.\n    src_color: The color of the central flare source, typically an RGB tuple.\n    circles: A list of tuples where each tuple defines a halo in the flare. Each tuple contains (alpha, (x, y), radius, (r_color, g_color, b_color)) representing the blend weight, position, size, and color.\n\nReturns:\n    The image with the sun flare effect applied, in the same dtype as the input image.\n\"\"\"",
                "method_name": "add_sun_flare",
                "second_doc": "\"\"\"\nSimulates lens flare artifacts by overlaying bright circles on the input image according to specified parameters.\n\nThis method introduces simulated optical effects that can help increase the visual diversity of image datasets, particularly by mimicking real-world lighting conditions, which can make models more resilient to such variations.\n\nArgs:\n    img: The input image to which the flare effect is added. Can be either uint8 or float32 format.\n    flare_center_x: X-coordinate for the central position of the main flare source.\n    flare_center_y: Y-coordinate for the central position of the main flare source.\n    src_radius: Radius of the central flare source.\n    src_color: RGB tuple specifying the color of the central flare.\n    circles: List of tuples representing individual flare halos. Each tuple contains:\n        - alpha (float): Blend weight for the halo.\n        - (x, y): Position of the halo center.\n        - radius (int): Radius of the halo.\n        - (r_color, g_color, b_color): RGB color values for the halo.\n\nReturns:\n    Image of the same type as the input, but with the flare effect applied.\n\"\"\"",
                "source_code": "non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for RandomSunFlareaugmentation\".format(input_dtype))\n\n    overlay = img.copy()\n    output = img.copy()\n\n    for (alpha, (x, y), rad3, (r_color, g_color, b_color)) in circles:\n        cv2.circle(overlay, (x, y), rad3, (r_color, g_color, b_color), -1)\n\n        cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n\n    point = (int(flare_center_x), int(flare_center_y))\n\n    overlay = output.copy()\n    num_times = src_radius // 10\n    alpha = np.linspace(0.0, 1, num=num_times)\n    rad = np.linspace(1, src_radius, num=num_times)\n    for i in range(num_times):\n        cv2.circle(overlay, point, int(rad[i]), src_color, -1)\n        alp = alpha[num_times - i - 1] * alpha[num_times - i - 1] * alpha[num_times - i - 1]\n        cv2.addWeighted(overlay, alp, output, 1 - alp, 0, output)\n\n    image_rgb = output\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Add shadows to the image.\n\n    From https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library\n\n    Args:\n        img (numpy.ndarray):\n        vertices_list (list):\n\n    Returns:\n        numpy.ndarray:\n\n    \"\"\"",
                "first_doc": "\"\"\"\nAdds shadow polygons to an image to simulate lighting effects.\n\nThis method takes an input image and a list of polygon vertex arrays (shadow regions), and overlays them as shadows onto the image. Pixels under these shadow regions will have their brightness (in the Lightness channel of HLS color space) reduced, simulating the effect of a shadow overlay.\n\nArgs:\n    img: The input RGB image to which shadows will be applied.\n    vertices_list: A list of arrays, each representing the vertices of a polygon where a shadow should be added.\n\nReturns:\n    The image with simulated shadows applied in the specified regions. The returned image has the same shape as the input and preserves the input's floating point or uint8 format.\n\"\"\"",
                "method_name": "add_shadow",
                "second_doc": "\"\"\"\nSimulates the effect of localized shadows within specified polygonal regions of an image.\n\nThis method processes an input RGB image and overlays shadow effects by darkening defined polygonal areas. By reducing the brightness in the affected regions, the method introduces realistic lighting variations, which serve to increase the visual diversity of the dataset. This helps models become more robust against changes in illumination conditions.\n\nArgs:\n    img: The input RGB image (numpy array) to be modified.\n    vertices_list: A list of numpy arrays, each representing the vertices of a polygon where the shadow effect will be applied.\n\nReturns:\n    Numpy array representing the image with shadow effects applied to the specified polygonal regions. The returned image preserves the shape and original data type (float32 or uint8) of the input.\n\"\"\"",
                "source_code": "non_rgb_warning(img)\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for RandomSnow augmentation\".format(input_dtype))\n\n    image_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    mask = np.zeros_like(img)\n\n    # adding all shadow polygons on empty mask, single 255 denotes only red channel\n    for vertices in vertices_list:\n        cv2.fillPoly(mask, vertices, 255)\n\n    # if red channel is hot, image's \"Lightness\" channel's brightness is lowered\n    red_max_value_ind = mask[:, :, 0] == 255\n    image_hls[:, :, 1][red_max_value_ind] = image_hls[:, :, 1][red_max_value_ind] * 0.5\n\n    image_rgb = cv2.cvtColor(image_hls, cv2.COLOR_HLS2RGB)\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Barrel / pincushion distortion. Unconventional augment.\n\n    Reference:\n        |  https://stackoverflow.com/questions/6199636/formulas-for-barrel-pincushion-distortion\n        |  https://stackoverflow.com/questions/10364201/image-transformation-in-opencv\n        |  https://stackoverflow.com/questions/2477774/correcting-fisheye-distortion-programmatically\n        |  http://www.coldvision.io/2017/03/02/advanced-lane-finding-using-opencv/\n    \"\"\"",
                "first_doc": "\"\"\"\nApplies optical distortion to an input image using a camera matrix and distortion coefficients.\n\nArgs:\n    img: The input image to which optical distortion will be applied.\n\nReturns:\n    The distorted image as a NumPy array, after applying the optical distortion transformation.\n\"\"\"",
                "method_name": "optical_distortion",
                "second_doc": "\"\"\"\nSimulates the effect of lens distortion on an image by applying a transformation based on camera intrinsic parameters and distortion coefficients. This introduces geometric variations, helping to create a more diverse set of images for improved robustness of computer vision models.\n\nArgs:\n    img (numpy.ndarray): The input image to be distorted.\n\nReturns:\n    numpy.ndarray: The image after applying optical distortion, with its original shape preserved.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n\n    fx = width\n    fy = width\n\n    cx = width * 0.5 + dx\n    cy = height * 0.5 + dy\n\n    camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32)\n\n    distortion = np.array([k, k, 0, 0, 0], dtype=np.float32)\n    map1, map2 = cv2.initUndistortRectifyMap(camera_matrix, distortion, None, None, (width, height), cv2.CV_32FC1)\n    img = cv2.remap(img, map1, map2, interpolation=interpolation, borderMode=border_mode, borderValue=value)\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Reference:\n        http://pythology.blogspot.sg/2014/03/interpolation-on-regular-distorted-grid.html\n    \"\"\"",
                "first_doc": "\"\"\"\nApplies a grid distortion effect to the input image by remapping its pixels according to computed mapping grids.\n\nArgs:\n    img: Input image on which to apply the grid distortion.\n\nReturns:\n    The distorted image resulting from the remapping operation.\n\"\"\"",
                "method_name": "grid_distortion",
                "second_doc": "\"\"\"\nDistorts the input image by recalculating its pixel positions through interpolated horizontal and vertical mapping grids, creating a warped visual effect.\n\nThis method manipulates the image geometry to artificially expand the diversity of visual patterns exposed to models during processing, supporting robust feature learning and improved generalization by introducing spatial variability.\n\nArgs:\n    img (numpy.ndarray): The input image to be transformed.\n    num_steps (int): Number of grid sections for the distortion mapping.\n    xsteps (list[float]): Distortion factors for each horizontal section.\n    ysteps (list[float]): Distortion factors for each vertical section.\n    interpolation (int): Interpolation method used during remapping (e.g., cv2.INTER_LINEAR).\n    border_mode (int): Pixel extrapolation method for areas outside the image (e.g., cv2.BORDER_REFLECT_101).\n    value (tuple[int, int, int] or int): Value used for filling pixels outside image if border_mode is cv2.BORDER_CONSTANT.\n\nReturns:\n    numpy.ndarray: The image after applying the grid distortion effect.\n\"\"\"",
                "source_code": "height, width = img.shape[:2]\n\n    x_step = width // num_steps\n    xx = np.zeros(width, np.float32)\n    prev = 0\n    for idx, x in enumerate(range(0, width, x_step)):\n        start = x\n        end = x + x_step\n        if end > width:\n            end = width\n            cur = width\n        else:\n            cur = prev + x_step * xsteps[idx]\n\n        xx[start:end] = np.linspace(prev, cur, end - start)\n        prev = cur\n\n    y_step = height // num_steps\n    yy = np.zeros(height, np.float32)\n    prev = 0\n    for idx, y in enumerate(range(0, height, y_step)):\n        start = y\n        end = y + y_step\n        if end > height:\n            end = height\n            cur = height\n        else:\n            cur = prev + y_step * ysteps[idx]\n\n        yy[start:end] = np.linspace(prev, cur, end - start)\n        prev = cur\n\n    map_x, map_y = np.meshgrid(xx, yy)\n    map_x = map_x.astype(np.float32)\n    map_y = map_y.astype(np.float32)\n\n    remap_fn = _maybe_process_in_chunks(\n        cv2.remap, map1=map_x, map2=map_y, interpolation=interpolation, borderMode=border_mode, borderValue=value\n    )\n    return remap_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Elastic deformation of images as described in [Simard2003]_ (with modifications).\n    Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n\n    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n         Convolutional Neural Networks applied to Visual Document Analysis\", in\n         Proc. of the International Conference on Document Analysis and\n         Recognition, 2003.\n    \"\"\"",
                "first_doc": "\"\"\"\nApplies an elastic transformation to the input image, introducing random distortions to augment the data.\n\nArgs:\n    img: The input image to be transformed.\n    alpha: The scaling factor for the intensity of the deformation.\n    sigma: The standard deviation for the Gaussian filter used to smooth the displacement maps.\n    alpha_affine: The intensity of random affine transformation applied to the image.\n\nReturns:\n    The elastically transformed image as a numpy array with the same shape as the input.\n\"\"\"",
                "method_name": "elastic_transform",
                "second_doc": "\"\"\"\nApplies a random elastic deformation and affine transformation to the input image, generating non-rigid variations that increase the diversity of the data. This helps expose models to a broader range of plausible geometric distortions, making them more robust to variations in real-world images.\n\nArgs:\n    img (numpy.ndarray): The input image to be transformed.\n    alpha (float): Scaling factor that controls the intensity of the elastic deformation.\n    sigma (float): Standard deviation for the Gaussian filter, smoothing the displacement fields.\n    alpha_affine (float): Intensity of the random affine transformation applied before elastic deformation.\n\nReturns:\n    numpy.ndarray: The image after elastic transformation, with the same shape as the input.\n\"\"\"",
                "source_code": "if random_state is None:\n        random_state = np.random.RandomState(1234)\n\n    height, width = img.shape[:2]\n\n    # Random affine\n    center_square = np.float32((height, width)) // 2\n    square_size = min((height, width)) // 3\n    alpha = float(alpha)\n    sigma = float(sigma)\n    alpha_affine = float(alpha_affine)\n\n    pts1 = np.float32(\n        [\n            center_square + square_size,\n            [center_square[0] + square_size, center_square[1] - square_size],\n            center_square - square_size,\n        ]\n    )\n    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n    matrix = cv2.getAffineTransform(pts1, pts2)\n\n    warp_fn = _maybe_process_in_chunks(\n        cv2.warpAffine, M=matrix, dsize=(width, height), flags=interpolation, borderMode=border_mode, borderValue=value\n    )\n    img = warp_fn(img)\n\n    if approximate:\n        # Approximate computation smooth displacement map with a large enough kernel.\n        # On large images (512+) this is approximately 2X times faster\n        dx = random_state.rand(height, width).astype(np.float32) * 2 - 1\n        cv2.GaussianBlur(dx, (17, 17), sigma, dst=dx)\n        dx *= alpha\n\n        dy = random_state.rand(height, width).astype(np.float32) * 2 - 1\n        cv2.GaussianBlur(dy, (17, 17), sigma, dst=dy)\n        dy *= alpha\n    else:\n        dx = np.float32(gaussian_filter((random_state.rand(height, width) * 2 - 1), sigma) * alpha)\n        dy = np.float32(gaussian_filter((random_state.rand(height, width) * 2 - 1), sigma) * alpha)\n\n    x, y = np.meshgrid(np.arange(width), np.arange(height))\n\n    map_x = np.float32(x + dx)\n    map_y = np.float32(y + dy)\n\n    remap_fn = _maybe_process_in_chunks(\n        cv2.remap, map1=map_x, map2=map_y, interpolation=interpolation, borderMode=border_mode, borderValue=value\n    )\n    return remap_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Elastic deformation of images as described in [Simard2003]_ (with modifications for speed).\n    Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n\n    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n         Convolutional Neural Networks applied to Visual Document Analysis\", in\n         Proc. of the International Conference on Document Analysis and\n         Recognition, 2003.\n    \"\"\"",
                "first_doc": "\"\"\"\nApplies an approximate elastic transformation with random affine to the input image.\n\nThis method distorts the input image by applying a random affine transformation followed by a smooth elastic deformation, approximated for efficiency.\n\nArgs:\n    img: The input image on which to perform the elastic transformation.\n    alpha: The scaling factor that controls the intensity of the deformation.\n    sigma: The Gaussian filter standard deviation, controlling the smoothness of the displacement fields.\n    alpha_affine: The scaling factor for the strength of the affine transformation applied before the elastic deformation.\n\nReturns:\n    The transformed image after applying the approximate elastic and affine transformations, with the same shape as the input.\n\"\"\"",
                "method_name": "elastic_transform_approx",
                "second_doc": "\"\"\"\nApplies a combination of random affine and approximate elastic transformations to an image.\n\nThis method introduces spatial distortions by first applying a random affine transformation and then following it with a computationally efficient, smooth elastic deformation. Such complex transformations are performed to diversify the input distribution, making the model more robust to geometric variations and local warping effects that might occur in real-world imagery.\n\nArgs:\n    img: Input image to be transformed.\n    alpha: Float value controlling the magnitude of the elastic deformation.\n    sigma: Float, standard deviation of the Gaussian filter determining the smoothness of deformation fields.\n    alpha_affine: Float defining the range of random affine transformation prior to elastic deformation.\n    interpolation: Interpolation method used for warping and remapping.\n    border_mode: Pixel extrapolation method if image is sampled outside its boundaries.\n    value: Value used for pixels outside the borders if border_mode is constant.\n    random_state: np.random.RandomState instance for reproducibility. If None, a default seed is used.\n\nReturns:\n    Image with the same shape as the input, transformed by random affine followed by smooth, randomized elastic warping.\n\"\"\"",
                "source_code": "if random_state is None:\n        random_state = np.random.RandomState(1234)\n\n    height, width = img.shape[:2]\n\n    # Random affine\n    center_square = np.float32((height, width)) // 2\n    square_size = min((height, width)) // 3\n    alpha = float(alpha)\n    sigma = float(sigma)\n    alpha_affine = float(alpha_affine)\n\n    pts1 = np.float32(\n        [\n            center_square + square_size,\n            [center_square[0] + square_size, center_square[1] - square_size],\n            center_square - square_size,\n        ]\n    )\n    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n    matrix = cv2.getAffineTransform(pts1, pts2)\n\n    warp_fn = _maybe_process_in_chunks(\n        cv2.warpAffine, M=matrix, dsize=(width, height), flags=interpolation, borderMode=border_mode, borderValue=value\n    )\n    img = warp_fn(img)\n\n    dx = random_state.rand(height, width).astype(np.float32) * 2 - 1\n    cv2.GaussianBlur(dx, (17, 17), sigma, dst=dx)\n    dx *= alpha\n\n    dy = random_state.rand(height, width).astype(np.float32) * 2 - 1\n    cv2.GaussianBlur(dy, (17, 17), sigma, dst=dy)\n    dy *= alpha\n\n    x, y = np.meshgrid(np.arange(width), np.arange(height))\n\n    map_x = np.float32(x + dx)\n    map_y = np.float32(y + dy)\n\n    remap_fn = _maybe_process_in_chunks(\n        cv2.remap, map1=map_x, map2=map_y, interpolation=interpolation, borderMode=border_mode, borderValue=value\n    )\n    return remap_fn(img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "invert",
                "second_doc": "\"\"\"\nInverts the pixel values of an input image, producing a negative of the original image. This operation is often used to introduce variation in image datasets, helping models learn features that are invariant to pixel intensity transformations.\n\nArgs:\n    img (np.ndarray): Input image array with pixel values in the range [0, 255].\n\nReturns:\n    np.ndarray: Image array with inverted pixel values, where each value is computed as 255 minus the original value.\n\"\"\"",
                "source_code": "return 255 - img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "channel_shuffle",
                "second_doc": "\"\"\"\nReorders the channels of the input image according to a specified permutation.\n\nThis operation introduces channel-level variation, which can help models learn more robust representations by reducing sensitivity to specific channel arrangements.\n\nArgs:\n    img (numpy.ndarray): The input image array whose channels will be reordered.\n    channels_shuffled (sequence of int): The desired order of the image channels.\n\nReturns:\n    numpy.ndarray: The image with channels permuted according to channels_shuffled.\n\"\"\"",
                "source_code": "img = img[..., channels_shuffled]\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "channel_dropout",
                "second_doc": "\"\"\"\nRandomly sets the specified image channels to a constant value, effectively simulating missing or corrupted sensor data and promoting model robustness.\n\nArgs:\n    img (np.ndarray): The input image with multiple channels (e.g., RGB), represented as a NumPy array.\n    channels_to_drop (slice or int or list of int): Index or indices specifying which channel(s) to drop.\n    fill_value (numeric): The value to assign to the dropped channel(s).\n\nReturns:\n    np.ndarray: A copy of the input image with the specified channel(s) set to the given fill_value.\n\nRaises:\n    NotImplementedError: If the input image has only one channel.\n\"\"\"",
                "source_code": "if len(img.shape) == 2 or img.shape[2] == 1:\n        raise NotImplementedError(\"Only one channel. ChannelDropout is not defined.\")\n\n    img = img.copy()\n\n    img[..., channels_to_drop] = fill_value\n\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "gamma_transform",
                "second_doc": "\"\"\"\nApplies a gamma correction to the input image by adjusting its intensity values according to a specified gamma value. This non-linear operation modifies the overall brightness and contrast of the image, which helps simulate different lighting conditions and improve the effectiveness of subsequent image processing or analysis.\n\nArgs:\n    img (np.ndarray): Input image array, either with dtype np.uint8 or a floating-point type.\n    gamma (float): The gamma value used for correction. A value greater than 1 darkens the image, while a value less than 1 lightens it.\n    eps (float): A small value added to gamma to prevent division by zero.\n\nReturns:\n    np.ndarray: The gamma-corrected image, with the same shape as the input.\n\"\"\"",
                "source_code": "if img.dtype == np.uint8:\n        invGamma = 1.0 / (gamma + eps)\n        table = (np.arange(0, 256.0 / 255, 1.0 / 255) ** invGamma) * 255\n        img = cv2.LUT(img, table.astype(np.uint8))\n    else:\n        img = np.power(img, gamma)\n\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "gauss_noise",
                "second_doc": "\"\"\"\nAdd Gaussian noise to the input image to simulate real-world variations and increase data diversity for robust machine learning model training.\n\nArgs:\n    image (numpy.ndarray): Input image to which noise will be added.\n    gauss (numpy.ndarray): Gaussian noise array to be added to the image.\n\nReturns:\n    numpy.ndarray: Image with added Gaussian noise, in float32 format.\n\"\"\"",
                "source_code": "image = image.astype(\"float32\")\n    return image + gauss"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_brightness_contrast_adjust_non_uint",
                "second_doc": "\"\"\"\nApply brightness and contrast adjustments to a non-uint image array, altering pixel intensities based on specified scaling (alpha) and shifting (beta) parameters. This modification helps simulate variations in lighting and exposure conditions, diversifying image data for downstream processing.\n\nArgs:\n    img (np.ndarray): Input image to be adjusted, with a non-uint dtype.\n    alpha (float): Multiplicative factor applied to scale the image contrast.\n    beta (float): Additive factor for adjusting image brightness.\n    beta_by_max (bool): If True, brightness adjustment uses the max possible value for the dtype; otherwise, uses image mean.\n\nReturns:\n    np.ndarray: The image with adjusted brightness and/or contrast, in float32 format.\n\"\"\"",
                "source_code": "dtype = img.dtype\n    img = img.astype(\"float32\")\n\n    if alpha != 1:\n        img *= alpha\n    if beta != 0:\n        if beta_by_max:\n            max_value = MAX_VALUES_BY_DTYPE[dtype]\n            img += beta * max_value\n        else:\n            img += beta * np.mean(img)\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "_brightness_contrast_adjust_uint",
                "second_doc": "\"\"\"\nAdjusts the brightness and contrast of an unsigned 8-bit integer image using a lookup table (LUT) for efficient pixel-wise transformation.\n\nArgs:\n    img (np.ndarray): Input image with dtype uint8.\n    alpha (float): Contrast adjustment factor. Values greater than 1 increase contrast, values between 0 and 1 decrease it.\n    beta (float): Brightness adjustment factor. Positive values increase brightness, negative values decrease it.\n    beta_by_max (bool): If True, brightness is scaled by the maximum possible pixel value; otherwise, it is scaled by the mean pixel value of the input image.\n\nReturns:\n    np.ndarray: The image after applying brightness and contrast adjustments, with shape preserved and dtype uint8.\n\nWhy:\n    This method applies intensity adjustments efficiently through a LUT to rapidly simulate various lighting and contrast conditions, which helps diversify dataset appearance and improve the robustness of vision models during training.\n\"\"\"",
                "source_code": "dtype = np.dtype(\"uint8\")\n\n    max_value = MAX_VALUES_BY_DTYPE[dtype]\n\n    lut = np.arange(0, max_value + 1).astype(\"float32\")\n\n    if alpha != 1:\n        lut *= alpha\n    if beta != 0:\n        if beta_by_max:\n            lut += beta * max_value\n        else:\n            lut += beta * np.mean(img)\n\n    lut = np.clip(lut, 0, max_value).astype(dtype)\n    img = cv2.LUT(img, lut)\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "brightness_contrast_adjust",
                "second_doc": "\"\"\"\nAdjust the brightness and contrast of an input image using parameters appropriate for its data type.\n\nThis method selects a type-specific algorithm to efficiently modify the image's intensity values, ensuring compatibility and optimal processing for both integer and non-integer image formats. By adapting the adjustment method, it enables consistent augmentation outcomes regardless of input image representation.\n\nArgs:\n    img (np.ndarray): Input image to be processed.\n    alpha (float): Scaling factor for contrast adjustment.\n    beta (float): Additive factor for brightness adjustment.\n    beta_by_max (bool): Whether to normalize the brightness adjustment by the maximum possible value of the image data type.\n\nReturns:\n    np.ndarray: Image with adjusted brightness and contrast.\n\"\"\"",
                "source_code": "if img.dtype == np.uint8:\n        return _brightness_contrast_adjust_uint(img, alpha, beta, beta_by_max)\n    else:\n        return _brightness_contrast_adjust_non_uint(img, alpha, beta, beta_by_max)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Apply poisson noise to image to simulate camera sensor noise.\n\n    Args:\n        image (numpy.ndarray): Input image, currently, only RGB, uint8 images are supported.\n        color_shift (float):\n        intensity (float): Multiplication factor for noise values. Values of ~0.5 are produce noticeable,\n                   yet acceptable level of noise.\n        random_state:\n        **kwargs:\n\n    Returns:\n        numpy.ndarray: Noised image\n\n    \"\"\"",
                "first_doc": "\"\"\"\nApplies simulated ISO noise to an RGB image using HLS conversion.\n\nThis method introduces luminance and color noise to an input image in the RGB color space to simulate the effect of ISO noise typically encountered in digital photography. The image is converted to the HLS color space, noise is added to the hue and luminance channels, and the result is converted back to the RGB color space.\n\nArgs:\n    image: The input RGB image as a NumPy array with dtype uint8 and 3 channels.\n\nReturns:\n    The noisy RGB image as a NumPy array with dtype uint8, maintaining the same shape as the input.\n\"\"\"",
                "method_name": "iso_noise",
                "second_doc": "\"\"\"\nIntroduces realistic luminance and color noise to an input RGB image by manipulating its hue and luminance in the HLS color space. This process mimics variations found in naturally noisy images due to sensor imperfections, thereby increasing the variety of visual patterns available for downstream processing and helping models become less sensitive to noise-related artifacts.\n\nArgs:\n    image: NumPy array of shape (H, W, 3) and dtype uint8, representing an RGB image.\n    intensity: Float specifying the scale of luminance noise applied.\n    color_shift: Float specifying the scale of hue (color) noise applied.\n    random_state: Optional instance of np.random.RandomState to ensure reproducible noise patterns; if None, a default seed is used.\n\nReturns:\n    NumPy array of shape (H, W, 3) and dtype uint8, containing the transformed RGB image with simulated noise.\n\"\"\"",
                "source_code": "assert image.dtype == np.uint8, \"Image must have uint8 channel type\"\n    assert image.shape[2] == 3, \"Image must be RGB\"\n\n    if random_state is None:\n        random_state = np.random.RandomState(42)\n\n    one_over_255 = float(1.0 / 255.0)\n    image = np.multiply(image, one_over_255, dtype=np.float32)\n    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n    mean, stddev = cv2.meanStdDev(hls)\n\n    luminance_noise = random_state.poisson(stddev[1] * intensity * 255, size=hls.shape[:2])\n    color_noise = random_state.normal(0, color_shift * 360 * intensity, size=hls.shape[:2])\n\n    hue = hls[..., 0]\n    hue += color_noise\n    hue[hue < 0] += 360\n    hue[hue > 360] -= 360\n\n    luminance = hls[..., 1]\n    luminance += (luminance_noise / 255) * (1.0 - luminance)\n\n    image = cv2.cvtColor(hls, cv2.COLOR_HLS2RGB) * 255\n    return image.astype(np.uint8)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "to_gray",
                "second_doc": "\"\"\"\nConverts an input RGB image to grayscale and then restores it to RGB format, effectively removing color information while maintaining the original image shape and channel structure.\n\nThis allows subsequent image processing operations to expect standard RGB input shapes without color information, supporting consistency within complex image augmentation pipelines.\n\nArgs:\n    img (numpy.ndarray): Input image in RGB format with shape (H, W, 3).\n\nReturns:\n    numpy.ndarray: The grayscale version of the input image, converted back to RGB format with shape (H, W, 3).\n\"\"\"",
                "source_code": "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    return cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "downscale",
                "second_doc": "\"\"\"\nApplies a downscale-upscale operation to an image, effectively simulating image quality degradation or resolution change by resizing the image to a smaller size and then restoring it to its original dimensions. This approach is commonly used to introduce realistic visual artifacts for data augmentation during model training.\n\nArgs:\n    img (numpy.ndarray): Input image to be processed.\n    scale (float): Downscaling factor determining the intermediate size of the image.\n    interpolation (int): Interpolation method used for resizing (e.g., cv2.INTER_NEAREST, cv2.INTER_LINEAR).\n\nReturns:\n    numpy.ndarray: Image of the original shape, processed to mimic lower resolution or compression effects.\n\"\"\"",
                "source_code": "h, w = img.shape[:2]\n\n    need_cast = interpolation != cv2.INTER_NEAREST and img.dtype == np.uint8\n    if need_cast:\n        img = to_float(img)\n    downscaled = cv2.resize(img, None, fx=scale, fy=scale, interpolation=interpolation)\n    upscaled = cv2.resize(downscaled, (w, h), interpolation=interpolation)\n    if need_cast:\n        upscaled = from_float(np.clip(upscaled, 0, 1), dtype=np.dtype(\"uint8\"))\n    return upscaled"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "to_float",
                "second_doc": "\"\"\"\nConverts an input image to a float32 array and normalizes its values based on the maximum allowed value for the image's data type.\n\nThis standardization ensures consistency in downstream processing steps, especially for operations that expect normalized float-valued images.\n\nArgs:\n    img (np.ndarray): Input image array.\n    max_value (float, optional): Maximum value for normalization. If not provided, it is inferred from the image's data type.\n\nReturns:\n    np.ndarray: The normalized image as a float32 numpy array.\n\nRaises:\n    RuntimeError: If max_value is not specified and cannot be inferred for the given image dtype.\n\"\"\"",
                "source_code": "if max_value is None:\n        try:\n            max_value = MAX_VALUES_BY_DTYPE[img.dtype]\n        except KeyError:\n            raise RuntimeError(\n                \"Can't infer the maximum value for dtype {}. You need to specify the maximum value manually by \"\n                \"passing the max_value argument\".format(img.dtype)\n            )\n    return img.astype(\"float32\") / max_value"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "from_float",
                "second_doc": "\"\"\"\nConverts a floating-point image array to a specified data type by scaling its values according to an appropriate maximum value.\n\nThis method ensures that images stored as floating-point arrays (typically in the [0, 1] range) are correctly transformed into an integer format (e.g., uint8 or uint16) suitable for further processing or storage, preserving the intended value range and image fidelity.\n\nArgs:\n    img (np.ndarray): Input image array with floating-point values, assumed to be in the range [0, 1].\n    dtype (np.dtype): Desired target data type for the output image (e.g., np.uint8, np.uint16).\n    max_value (Optional[int], optional): Maximum value corresponding to the target data type. If not provided, it attempts to infer the maximum from predefined values.\n\nReturns:\n    np.ndarray: Image array converted to the specified dtype, with its values appropriately scaled.\n\nRaises:\n    RuntimeError: If max_value is not provided and cannot be inferred for the given dtype.\n\"\"\"",
                "source_code": "if max_value is None:\n        try:\n            max_value = MAX_VALUES_BY_DTYPE[dtype]\n        except KeyError:\n            raise RuntimeError(\n                \"Can't infer the maximum value for dtype {}. You need to specify the maximum value manually by \"\n                \"passing the max_value argument\".format(dtype)\n            )\n    return (img * max_value).astype(dtype)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Flip a bounding box vertically around the x-axis.\n\n    Args:\n        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n        rows (int): Image rows.\n        cols (int): Image cols.\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nVertically flips a bounding box within normalized coordinate space.\n\nArgs:\n    bbox: The bounding box, where the first four elements represent x_min, y_min, x_max, and y_max.\n    rows: The number of rows in the image; not used in this method.\n    cols: The number of columns in the image; not used in this method.\n\nReturns:\n    tuple: The vertically flipped bounding box as (x_min, new_y_min, x_max, new_y_max), where new_y_min and new_y_max are calculated with respect to a vertical flip.\n\"\"\"",
                "method_name": "bbox_vflip",
                "second_doc": "\"\"\"\nComputes the vertical flip of a bounding box within normalized coordinates. This operation is important for ensuring bounding boxes remain consistent with image content after flipping transformations are applied.\n\nArgs:\n    bbox (tuple): The bounding box, where the first four elements are (x_min, y_min, x_max, y_max) in normalized coordinates.\n    rows (int): The number of rows in the image; not used in this method.\n    cols (int): The number of columns in the image; not used in this method.\n\nReturns:\n    tuple: The bounding box after vertical flipping, represented as (x_min, 1 - y_max, x_max, 1 - y_min).\n\"\"\"",
                "source_code": "x_min, y_min, x_max, y_max = bbox[:4]\n    return x_min, 1 - y_max, x_max, 1 - y_min"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Flip a bounding box horizontally around the y-axis.\n\n    Args:\n        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n        rows (int): Image rows.\n        cols (int): Image cols.\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nHorizontally flips the coordinates of a bounding box within a normalized coordinate system.\n\nArgs:\n    bbox: A sequence representing the bounding box, where the first four elements correspond to (x_min, y_min, x_max, y_max).\n    rows: The number of rows in the image grid (unused).\n    cols: The number of columns in the image grid (unused).\n\nReturns:\n    tuple: The horizontally flipped bounding box as a tuple (x_min, y_min, x_max, y_max), with x coordinates reflected across the vertical axis in the normalized space.\n\"\"\"",
                "method_name": "bbox_hflip",
                "second_doc": "\"\"\"\nReflects a bounding box horizontally within a normalized coordinate system by inverting its x-coordinates. \n\nThis operation is commonly used to ensure that annotations remain accurate when performing horizontal image flips during data augmentation, helping maintain consistency between transformed images and their corresponding bounding boxes.\n\nArgs:\n    bbox (Sequence[float]): A sequence where the first four elements are (x_min, y_min, x_max, y_max), representing the bounding box in normalized coordinates.\n    rows (int): Number of rows in the image grid (not used in this operation).\n    cols (int): Number of columns in the image grid (not used in this operation).\n\nReturns:\n    tuple: The horizontally reflected bounding box as (x_min, y_min, x_max, y_max), where x_min and x_max are swapped and mirrored over the vertical axis.\n\"\"\"",
                "source_code": "x_min, y_min, x_max, y_max = bbox[:4]\n    return 1 - x_max, y_min, 1 - x_min, y_max"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Flip a bounding box either vertically, horizontally or both depending on the value of `d`.\n\n    Args:\n        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n        d (int):\n        rows (int): Image rows.\n        cols (int): Image cols.\n\n    Returns:\n        tuple: A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: if value of `d` is not -1, 0 or 1.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nFlips a bounding box according to the specified direction.\n\nThis method flips the input bounding box either vertically, horizontally, or both, based on the value of the direction parameter.\n\nArgs:\n    bbox: The bounding box to be flipped.\n    d: The flip direction. Valid values are 0 for vertical flip, 1 for horizontal flip, and -1 for both vertical and horizontal flip.\n    rows: The number of rows in the image, used for determining the flip axis.\n    cols: The number of columns in the image, used for determining the flip axis.\n\nReturns:\n    The flipped bounding box as per the specified direction.\n\"\"\"",
                "method_name": "bbox_flip",
                "second_doc": "\"\"\"\nAdjusts the location of a bounding box by flipping it in the chosen direction to maintain correct correspondence with the image after a flip operation.\n\nThis method ensures that bounding box annotations remain accurate and aligned with the image contents after the image is flipped, preserving data consistency during augmentation processes.\n\nArgs:\n    bbox: The bounding box to flip.\n    d: Direction of the flip \u2014 0 for vertical, 1 for horizontal, -1 for both.\n    rows: Height of the image, used for vertical flipping calculations.\n    cols: Width of the image, used for horizontal flipping calculations.\n\nReturns:\n    The bounding box with coordinates updated according to the specified flip direction.\n\"\"\"",
                "source_code": "if d == 0:\n        bbox = bbox_vflip(bbox, rows, cols)\n    elif d == 1:\n        bbox = bbox_hflip(bbox, rows, cols)\n    elif d == -1:\n        bbox = bbox_hflip(bbox, rows, cols)\n        bbox = bbox_vflip(bbox, rows, cols)\n    else:\n        raise ValueError(\"Invalid d value {}. Valid values are -1, 0 and 1\".format(d))\n    return bbox"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Crop a bounding box using the provided coordinates of bottom-left and top-right corners in pixels and the\n    required height and width of the crop.\n\n    Args:\n        bbox (tuple): A cropped box `(x_min, y_min, x_max, y_max)`.\n        crop_coords (tuple): Crop coordinates `(x1, y1, x2, y2)`.\n        crop_height (int):\n        crop_width (int):\n        rows (int): Image rows.\n        cols (int): Image cols.\n\n    Returns:\n        tuple: A cropped bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nCrops and normalizes a bounding box based on specified crop coordinates and image dimensions.\n\nThis method first denormalizes the given bounding box using the full image dimensions, then adjusts the box relative to the provided crop coordinates, and finally re-normalizes the resulting bounding box to the new cropped area's dimensions.\n\nArgs:\n    bbox: The normalized coordinates of the bounding box to be cropped.\n    crop_coords: The coordinates defining the crop region in the format (x1, y1, x2, y2).\n    crop_height: The height of the crop region.\n    crop_width: The width of the crop region.\n    rows: The number of rows (height) in the original image.\n    cols: The number of columns (width) in the original image.\n\nReturns:\n    The normalized coordinates of the bounding box within the cropped image region.\n\"\"\"",
                "method_name": "crop_bbox_by_coords",
                "second_doc": "\"\"\"\nAdjusts a bounding box to match a specific cropped area of an image, converting coordinates for seamless integration with further processing.\n\nBy recalculating bounding box coordinates relative to the crop and normalizing them to the new region, this method ensures that object annotations remain accurate after image transformations. This is crucial for maintaining correct data representation throughout augmentation workflows.\n\nArgs:\n    bbox: The normalized coordinates of the bounding box to be transformed.\n    crop_coords: Tuple of (x1, y1, x2, y2) specifying the crop region in the original image.\n    crop_height: Height of the cropped area.\n    crop_width: Width of the cropped area.\n    rows: The height (number of rows) of the original image.\n    cols: The width (number of columns) of the original image.\n\nReturns:\n    The bounding box coordinates, normalized to the dimensions of the cropped image region.\n\"\"\"",
                "source_code": "bbox = denormalize_bbox(bbox, rows, cols)\n    x_min, y_min, x_max, y_max = bbox[:4]\n    x1, y1, x2, y2 = crop_coords\n    cropped_bbox = x_min - x1, y_min - y1, x_max - x1, y_max - y1\n    return normalize_bbox(cropped_bbox, crop_height, crop_width)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Crop a bounding box.\n\n    Args:\n        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n        x_min (int):\n        y_min (int):\n        x_max (int):\n        y_max (int):\n        rows (int): Image rows.\n        cols (int): Image cols.\n\n    Returns:\n        tuple: A cropped bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nCrops a bounding box to a specified rectangular region and adjusts its coordinates accordingly.\n\nArgs:\n    bbox: The bounding box to be cropped.\n    x_min: The minimum x-coordinate of the cropping region.\n    y_min: The minimum y-coordinate of the cropping region.\n    x_max: The maximum x-coordinate of the cropping region.\n    y_max: The maximum y-coordinate of the cropping region.\n    rows: The number of rows (height) of the original image or coordinate space.\n    cols: The number of columns (width) of the original image or coordinate space.\n\nReturns:\n    The adjusted bounding box after cropping, with coordinates remapped to the new region.\n\"\"\"",
                "method_name": "bbox_crop",
                "second_doc": "\"\"\"\nAdjusts a bounding box's coordinates to fit within a defined cropping rectangle, ensuring its position and size are accurately recalculated for further image processing.\n\nArgs:\n    bbox: The bounding box to be adjusted.\n    x_min: The minimum x-coordinate of the crop region.\n    y_min: The minimum y-coordinate of the crop region.\n    x_max: The maximum x-coordinate of the crop region.\n    y_max: The maximum y-coordinate of the crop region.\n    rows: The number of rows (height) of the original image or coordinate space.\n    cols: The number of columns (width) of the original image or coordinate space.\n\nReturns:\n    The bounding box with coordinates adapted to the cropped region, preserving correct spatial alignment for subsequent transformations or analysis.\n\"\"\"",
                "source_code": "crop_coords = x_min, y_min, x_max, y_max\n    crop_height = y_max - y_min\n    crop_width = x_max - x_min\n    return crop_bbox_by_coords(bbox, crop_coords, crop_height, crop_width, rows, cols)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "bbox_center_crop",
                "second_doc": "\"\"\"\nCenters and crops a bounding box within an image, recalculating its coordinates to correspond with the cropped central region.\n\nThis is done to ensure bounding box annotations remain accurate and aligned with the portion of the image used for further processing or model training.\n\nArgs:\n    bbox (tuple): The bounding box coordinates in the format (x_min, y_min, x_max, y_max).\n    rows (int): The number of rows (height) in the original image.\n    cols (int): The number of columns (width) in the original image.\n    crop_height (int): The height of the central crop.\n    crop_width (int): The width of the central crop.\n\nReturns:\n    tuple: The adjusted bounding box coordinates relative to the cropped area.\n\"\"\"",
                "source_code": "crop_coords = get_center_crop_coords(rows, cols, crop_height, crop_width)\n    return crop_bbox_by_coords(bbox, crop_coords, crop_height, crop_width, rows, cols)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "bbox_random_crop",
                "second_doc": "\"\"\"\nAdjusts bounding boxes according to randomly selected crop coordinates within an image. This method ensures that bounding boxes remain accurate after a random crop is applied to an image, thus maintaining the correctness of annotations for subsequent processing or training.\n\nArgs:\n    bbox (list or np.ndarray): Bounding box coordinates to be adjusted.\n    rows (int): Number of rows (height) of the source image.\n    cols (int): Number of columns (width) of the source image.\n    crop_height (int): Target height for the cropped region.\n    crop_width (int): Target width for the cropped region.\n    h_start (float): Vertical position to start the crop (as a proportion of total height).\n    w_start (float): Horizontal position to start the crop (as a proportion of total width).\n\nReturns:\n    list or np.ndarray: Adjusted bounding box coordinates corresponding to the cropped image area.\n\"\"\"",
                "source_code": "crop_coords = get_random_crop_coords(rows, cols, crop_height, crop_width, h_start, w_start)\n    return crop_bbox_by_coords(bbox, crop_coords, crop_height, crop_width, rows, cols)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Rotates a bounding box by 90 degrees CCW (see np.rot90)\n\n    Args:\n        bbox (tuple): A bounding box tuple (x_min, y_min, x_max, y_max).\n        factor (int): Number of CCW rotations. Must be in set {0, 1, 2, 3} See np.rot90.\n        rows (int): Image rows.\n        cols (int): Image cols.\n\n    Returns:\n        tuple: A bounding box tuple (x_min, y_min, x_max, y_max).\n\n    \"\"\"",
                "first_doc": "\"\"\"\nRotates a bounding box by a specified multiple of 90 degrees.\n\nThis method applies a 90-degree rotation to the input bounding box based on the provided rotation factor. The bounding box is specified with normalized coordinates, and the rotation is performed within the unit square.\n\nArgs:\n    bbox: The bounding box defined as a list or tuple with at least four elements corresponding to (x_min, y_min, x_max, y_max).\n    factor: The number of times to rotate 90 degrees counter-clockwise. Must be an integer in the set {0, 1, 2, 3}.\n    rows: Number of rows in the image (unused in this method).\n    cols: Number of columns in the image (unused in this method).\n\nReturns:\n    A tuple representing the rotated bounding box with coordinates (x_min, y_min, x_max, y_max).\n\nRaises:\n    ValueError: If the rotation factor is not in the set {0, 1, 2, 3}.\n\"\"\"",
                "method_name": "bbox_rot90",
                "second_doc": "\"\"\"\nAdjusts the coordinates of a normalized bounding box to remain accurate after rotating its corresponding image by a multiple of 90 degrees.\n\nWhen images are rotated during data preprocessing or augmentation, associated bounding boxes need to be transformed as well to ensure they still properly enclose their intended regions. This method recalculates and returns the new bounding box coordinates after applying the specified number of 90-degree counter-clockwise rotations within normalized image space.\n\nArgs:\n    bbox: Sequence of at least four floats (x_min, y_min, x_max, y_max) specifying the bounding box with normalized coordinates relative to the image.\n    factor: Integer in {0, 1, 2, 3} indicating how many times the bounding box should be rotated by 90 degrees counter-clockwise.\n    rows: Number of rows in the image (not used in the computation).\n    cols: Number of columns in the image (not used in the computation).\n\nReturns:\n    tuple: The recalculated bounding box coordinates (x_min, y_min, x_max, y_max) after rotation.\n\nRaises:\n    ValueError: If the rotation factor is not one of {0, 1, 2, 3}.\n\"\"\"",
                "source_code": "if factor not in {0, 1, 2, 3}:\n        raise ValueError(\"Parameter n must be in set {0, 1, 2, 3}\")\n    x_min, y_min, x_max, y_max = bbox[:4]\n    if factor == 1:\n        bbox = y_min, 1 - x_max, y_max, 1 - x_min\n    elif factor == 2:\n        bbox = 1 - x_max, 1 - y_max, 1 - x_min, 1 - y_min\n    elif factor == 3:\n        bbox = 1 - y_max, x_min, 1 - y_min, x_max\n    return bbox"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Rotates a bounding box by angle degrees.\n\n    Args:\n        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n        angle (int): Angle of rotation in degrees.\n        rows (int): Image rows.\n        cols (int): Image cols.\n        interpolation (int): Interpolation method. TODO: Fix this, tt's not used in function\n\n    Returns:\n        A bounding box `(x_min, y_min, x_max, y_max)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nRotates a bounding box by a specified angle and returns the new axis-aligned bounding box.\n\nArgs:\n    bbox: Bounding box specified as [x_min, y_min, x_max, y_max, ...].\n    angle: The rotation angle in degrees, counter-clockwise.\n    rows: Number of rows (height) in the image or coordinate space.\n    cols: Number of columns (width) in the image or coordinate space.\n    interpolation: Not used in this function, but provided for compatibility.\n\nReturns:\n    tuple: A tuple (x_min, y_min, x_max, y_max) representing the rotated bounding box's coordinates.\n\"\"\"",
                "method_name": "bbox_rotate",
                "second_doc": "\"\"\"\nRotates the coordinates of a given bounding box by a specified angle and returns the smallest axis-aligned bounding box that encloses the rotated box. This is necessary to maintain accurate localization of objects after geometric transformations are applied to images, ensuring consistency between augmented images and their associated annotations.\n\nArgs:\n    bbox (list or tuple): Bounding box specified as [x_min, y_min, x_max, y_max, ...].\n    angle (float): The angle in degrees to rotate the bounding box, counter-clockwise.\n    rows (int): The number of rows (image height) in the coordinate space.\n    cols (int): The number of columns (image width) in the coordinate space.\n    interpolation: Placeholder for compatibility; not used in this function.\n\nReturns:\n    tuple: (x_min, y_min, x_max, y_max) representing the coordinates of the new rotated axis-aligned bounding box.\n\"\"\"",
                "source_code": "x_min, y_min, x_max, y_max = bbox[:4]\n    scale = cols / float(rows)\n    x = np.array([x_min, x_max, x_max, x_min]) - 0.5\n    y = np.array([y_min, y_min, y_max, y_max]) - 0.5\n    angle = np.deg2rad(angle)\n    x_t = (np.cos(angle) * x * scale + np.sin(angle) * y) / scale\n    y_t = -np.sin(angle) * x * scale + np.cos(angle) * y\n    x_t = x_t + 0.5\n    y_t = y_t + 0.5\n\n    x_min, x_max = min(x_t), max(x_t)\n    y_min, y_max = min(y_t), max(y_t)\n\n    return x_min, y_min, x_max, y_max"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Transposes a bounding box along given axis.\n\n    Args:\n        bbox (tuple): A bounding box `(x_min, y_min, x_max, y_max)`.\n        axis (int): 0 - main axis, 1 - secondary axis.\n        rows (int): Image rows.\n        cols (int): Image cols.\n\n    Returns:\n        tuple: A bounding box tuple `(x_min, y_min, x_max, y_max)`.\n\n    Raises:\n        ValueError: If axis not equal to 0 or 1.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nTransposes the bounding box along the specified axis.\n\nThis method modifies the input bounding box coordinates according to the given axis. If axis is 0, it swaps the x and y coordinates. If axis is 1, it flips the bounding box across the main diagonal within a normalized coordinate system.\n\nArgs:\n    bbox: The bounding box represented as a sequence where the first four elements are (x_min, y_min, x_max, y_max).\n    axis: The axis along which to transpose the bounding box. Must be 0 or 1.\n    rows: The total number of rows in the image or grid (not directly used in this method, included for interface compatibility).\n    cols: The total number of columns in the image or grid (not directly used in this method, included for interface compatibility).\n\nReturns:\n    The transposed bounding box as a tuple (x_min, y_min, x_max, y_max).\n\"\"\"",
                "method_name": "bbox_transpose",
                "second_doc": "\"\"\"\nTransforms the bounding box coordinates according to the specified axis, enabling consistent augmentation of associated spatial labels with geometric image transformations.\n\nBy transposing bounding boxes, this method ensures that the bounding box annotations remain accurate and aligned with the modified images after transformations that alter spatial arrangement.\n\nArgs:\n    bbox: Sequence containing at least four elements (x_min, y_min, x_max, y_max) representing the coordinates of the bounding box in normalized form.\n    axis: Integer specifying the axis along which to transpose the bounding box. Must be 0 (swap x and y) or 1 (flip across the main diagonal).\n    rows: Total number of rows in the image or grid (included for interface consistency, not used within the function).\n    cols: Total number of columns in the image or grid (included for interface consistency, not used within the function).\n\nReturns:\n    tuple: The bounding box coordinates after transposition, in the form (x_min, y_min, x_max, y_max).\n\"\"\"",
                "source_code": "x_min, y_min, x_max, y_max = bbox[:4]\n    if axis not in {0, 1}:\n        raise ValueError(\"Axis must be either 0 or 1.\")\n    if axis == 0:\n        bbox = (y_min, x_min, y_max, x_max)\n    if axis == 1:\n        bbox = (1 - y_max, 1 - x_max, 1 - y_min, 1 - x_min)\n    return bbox"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Flip a keypoint vertically around the x-axis.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        rows (int): Image height.\n        cols( int): Image width.\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nVertically flips the given keypoint coordinates.\n\nArgs:\n    keypoint: A sequence containing keypoint information; expected to include at least x, y, angle, and scale in the first four elements.\n    rows: The total number of rows (height) in the image.\n    cols: The total number of columns (width) in the image.\n\nReturns:\n    tuple: A tuple containing the vertically flipped x coordinate, flipped y coordinate, adjusted angle, and the original scale.\n\"\"\"",
                "method_name": "keypoint_vflip",
                "second_doc": "\"\"\"\nFlips keypoint coordinates vertically to maintain correct keypoint locations after vertical image flipping. Adjusts the angle to reflect the flipped orientation while preserving the keypoint's scale.\n\nArgs:\n    keypoint (Sequence): A sequence containing keypoint information; the first four elements should represent x, y, angle, and scale.\n    rows (int): The total number of rows (image height).\n    cols (int): The total number of columns (image width).\n\nReturns:\n    tuple: (flipped_x, flipped_y, adjusted_angle, original_scale) after applying a vertical flip.\n\"\"\"",
                "source_code": "x, y, angle, scale = keypoint[:4]\n    c = math.cos(angle)\n    s = math.sin(angle)\n    angle = math.atan2(-s, c)\n    return x, (rows - 1) - y, angle, scale"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Flip a keypoint horizontally around the y-axis.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nHorizontally flips a keypoint within an image of specified dimensions.\n\nArgs:\n    keypoint: The keypoint to flip, typically a sequence or tuple containing at least four elements (x, y, angle, scale).\n    rows: The number of rows in the image. (Unused in computation.)\n    cols: The number of columns in the image.\n\nReturns:\n    tuple: A tuple representing the horizontally flipped keypoint (x, y, angle, scale).\n\"\"\"",
                "method_name": "keypoint_hflip",
                "second_doc": "\"\"\"\nComputes the horizontal mirror of a keypoint within an image while preserving its geometric characteristics.\n\nThis operation ensures that the keypoint's position and orientation remain consistent with a mirrored version of the image, which is necessary for applying data augmentation techniques that expose models to a wider range of possible input arrangements.\n\nArgs:\n    keypoint: A sequence or tuple containing at least four elements (x, y, angle, scale) that defines the keypoint's location and properties.\n    rows: Number of rows in the image (not used in this operation).\n    cols: Number of columns in the image, used to determine the new x-coordinate after flipping.\n\nReturns:\n    tuple: The updated keypoint as a tuple (x, y, angle, scale), reflecting its position and orientation after a horizontal flip.\n\"\"\"",
                "source_code": "x, y, angle, scale = keypoint[:4]\n    c = math.cos(angle)\n    s = math.sin(angle)\n    angle = math.atan2(s, -c)\n    return (cols - 1) - x, y, angle, scale"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Flip a keypoint either vertically, horizontally or both depending on the value of `d`.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        d (int): Number of flip. Must be -1, 0 or 1:\n            * 0 - vertical flip,\n            * 1 - horizontal flip,\n            * -1 - vertical and horizontal flip.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: if value of `d` is not -1, 0 or 1.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nFlips a keypoint vertically, horizontally, or both, based on the specified direction.\n\nArgs:\n    keypoint: The input keypoint to be flipped.\n    d: The direction of the flip. Use 0 for vertical, 1 for horizontal, or -1 for both.\n    rows: The number of rows in the image containing the keypoint.\n    cols: The number of columns in the image containing the keypoint.\n\nReturns:\n    The flipped keypoint. Raises a ValueError if the direction parameter 'd' is invalid.\n\"\"\"",
                "method_name": "keypoint_flip",
                "second_doc": "\"\"\"\nAdjusts the location of a keypoint by flipping it vertically, horizontally, or both, depending on the specified direction. This transformation helps preserve the spatial consistency between images and their associated keypoints during augmentation, ensuring accurate learning in vision tasks.\n\nArgs:\n    keypoint: The coordinates of the keypoint to flip.\n    d: The direction for the flip. Use 0 for vertical, 1 for horizontal, or -1 for both directions.\n    rows: The total number of rows (height) of the image containing the keypoint.\n    cols: The total number of columns (width) of the image containing the keypoint.\n\nReturns:\n    The keypoint after applying the specified flip operation.\n\nRaises:\n    ValueError: If 'd' is not one of the valid values (-1, 0, or 1).\n\"\"\"",
                "source_code": "if d == 0:\n        keypoint = keypoint_vflip(keypoint, rows, cols)\n    elif d == 1:\n        keypoint = keypoint_hflip(keypoint, rows, cols)\n    elif d == -1:\n        keypoint = keypoint_hflip(keypoint, rows, cols)\n        keypoint = keypoint_vflip(keypoint, rows, cols)\n    else:\n        raise ValueError(\"Invalid d value {}. Valid values are -1, 0 and 1\".format(d))\n    return keypoint"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Rotates a keypoint by 90 degrees CCW (see np.rot90)\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        factor (int): Number of CCW rotations. Must be in range [0;3] See np.rot90.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    Raises:\n        ValueError: if factor not in set {0, 1, 2, 3}\n\n    \"\"\"",
                "first_doc": "\"\"\"\nRotates a keypoint by a multiple of 90 degrees within an image of specified size.\n\nThis method adjusts the (x, y) position, angle, and scale of a given keypoint according to a rotation factor (number of 90-degree increments), taking into account the dimensions of the image (rows and columns).\n\nArgs:\n    keypoint: The keypoint represented as a sequence (typically (x, y, angle, scale)).\n    factor: The number of 90-degree rotations to perform; must be one of {0, 1, 2, 3}.\n    rows: The number of rows in the image (image height).\n    cols: The number of columns in the image (image width).\n\nReturns:\n    tuple: A tuple containing the rotated keypoint's (x, y, angle, scale).\n\"\"\"",
                "method_name": "keypoint_rot90",
                "second_doc": "\"\"\"\nAdjusts the position, orientation, and scale of a keypoint by rotating it in 90-degree increments to maintain geometric consistency after image transformations.\n\nThis function ensures that keypoint annotations remain accurately aligned with their associated image regions after a 90-degree rotation, which is essential for precise localization and further processing in vision pipelines.\n\nArgs:\n    keypoint (tuple): Sequence representing the keypoint's (x, y, angle, scale).\n    factor (int): Number of 90-degree rotations to apply; must be one of {0, 1, 2, 3}.\n    rows (int): Height of the image in pixels.\n    cols (int): Width of the image in pixels.\n\nReturns:\n    tuple: The keypoint's updated (x, y, angle, scale) after applying the rotation.\n\"\"\"",
                "source_code": "x, y, angle, scale = keypoint[:4]\n\n    if factor not in {0, 1, 2, 3}:\n        raise ValueError(\"Parameter n must be in set {0, 1, 2, 3}\")\n\n    if factor == 1:\n        x, y, angle = y, (cols - 1) - x, angle - math.pi / 2\n    elif factor == 2:\n        x, y, angle = (cols - 1) - x, (rows - 1) - y, angle - math.pi\n    elif factor == 3:\n        x, y, angle = (rows - 1) - y, x, angle + math.pi / 2\n\n    return x, y, angle, scale"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Rotate a keypoint by angle.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        angle (float): Rotation angle.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nRotates a keypoint by a specified angle around the center of the image.\n\nArgs:\n    keypoint: The keypoint to be rotated, containing coordinates, angle, and scale information.\n    angle: The rotation angle in degrees to apply to the keypoint.\n    rows: The number of rows in the image (image height).\n    cols: The number of columns in the image (image width).\n\nReturns:\n    tuple: The rotated keypoint as a tuple containing the updated x and y coordinates, angle (in radians), and scale.\n\"\"\"",
                "method_name": "keypoint_rotate",
                "second_doc": "\"\"\"\nApplies a geometric transformation to relocate a keypoint by rotating it around the image center by a specified angle. This ensures the keypoint's properties remain consistent with augmented image data, supporting accurate model training and evaluation in vision applications.\n\nArgs:\n    keypoint (tuple): The keypoint to be rotated, containing x and y coordinates, angle (in radians), and scale.\n    angle (float): The rotation angle in degrees to apply to the keypoint.\n    rows (int): The number of rows in the image (image height).\n    cols (int): The number of columns in the image (image width).\n\nReturns:\n    tuple: The rotated keypoint as a tuple containing the updated x and y coordinates, the new angle (in radians), and scale.\n\"\"\"",
                "source_code": "matrix = cv2.getRotationMatrix2D(((cols - 1) * 0.5, (rows - 1) * 0.5), angle, 1.0)\n    x, y, a, s = keypoint[:4]\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a + math.radians(angle), s"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Scales a keypoint by scale_x and scale_y.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        scale_x (int): Scale coefficient x-axis.\n        scale_y (int): Scale coefficient y-axis.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nScales the coordinates and scale of a keypoint according to provided scaling factors.\n\nArgs:\n    keypoint: A sequence containing keypoint coordinates, angle, and scale (expects at least four elements).\n    scale_x: Scaling factor for the x-coordinate.\n    scale_y: Scaling factor for the y-coordinate.\n\nReturns:\n    tuple: A tuple containing the scaled x-coordinate, scaled y-coordinate, original angle, and the scaled keypoint scale.\n\"\"\"",
                "method_name": "keypoint_scale",
                "second_doc": "\"\"\"\nAdjusts the position and scale of a keypoint by specific scaling factors, ensuring geometric consistency during coordinate-space transformations.\n\nThis method applies scaling to the x and y coordinates and updates the keypoint's scale according to the maximum scaling ratio, while keeping the angle unchanged. This is necessary to maintain the correct relative location and size of keypoints after an image or object is resized or warped.\n\nArgs:\n    keypoint (Sequence[float]): A sequence representing a keypoint, containing at least four elements: x-coordinate, y-coordinate, angle, and scale.\n    scale_x (float): Factor by which the x-coordinate is scaled.\n    scale_y (float): Factor by which the y-coordinate is scaled.\n\nReturns:\n    tuple: A tuple of the form (scaled_x, scaled_y, original_angle, scaled_scale) where:\n        - scaled_x (float): The x-coordinate after scaling.\n        - scaled_y (float): The y-coordinate after scaling.\n        - original_angle (float): The unchanged angle of the keypoint.\n        - scaled_scale (float): The scale, adjusted by the larger of scale_x or scale_y.\n\"\"\"",
                "source_code": "x, y, angle, scale = keypoint[:4]\n    return x * scale_x, y * scale_y, angle, scale * max(scale_x, scale_y)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Crop a keypoint using the provided coordinates of bottom-left and top-right corners in pixels and the\n    required height and width of the crop.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        crop_coords (tuple): Crop box coords `(x1, x2, y1, y2)`.\n        crop height (int): Crop height.\n        crop_width (int): Crop width.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nAdjusts the coordinates of a keypoint relative to a cropped region in an image.\n\nArgs:\n    keypoint: A list or tuple representing the keypoint location and properties. The first four elements correspond to x-coordinate, y-coordinate, angle, and scale.\n    crop_coords: The coordinates of the cropping rectangle, typically in the format (x1, y1, x2, y2), where (x1, y1) is the top-left and (x2, y2) is the bottom-right of the crop.\n    crop_height: The height of the cropped region, not directly used in the computation but included for compatibility.\n    crop_width: The width of the cropped region, not directly used in the computation but included for compatibility.\n    rows: The total number of rows in the image, not directly used in the computation but may be relevant for bounds checking or additional processing.\n    cols: The total number of columns in the image, not directly used in the computation but may be relevant for bounds checking or additional processing.\n\nReturns:\n    A tuple containing the keypoint's (x, y) coordinates adjusted to the cropped region, along with its original angle and scale.\n\"\"\"",
                "method_name": "crop_keypoint_by_coords",
                "second_doc": "\"\"\"\nUpdates a keypoint's coordinates to match its new position within a cropped area of an image. This ensures the keypoint remains accurately aligned with the underlying visual features after cropping, supporting precise localization for downstream tasks.\n\nArgs:\n    keypoint: A list or tuple with at least four elements representing the keypoint's x-coordinate, y-coordinate, angle, and scale.\n    crop_coords: A tuple (x1, y1, x2, y2) indicating the top-left and bottom-right coordinates of the crop rectangle.\n    crop_height: The height of the cropped region (included for interface consistency; not used in calculation).\n    crop_width: The width of the cropped region (included for interface consistency; not used in calculation).\n    rows: Total number of rows in the original image (may be relevant for external validation; not used here).\n    cols: Total number of columns in the original image (may be relevant for external validation; not used here).\n\nReturns:\n    tuple: The adjusted keypoint as (x, y, angle, scale), where (x, y) are the coordinates relative to the cropped region, and angle and scale remain unchanged.\n\"\"\"",
                "source_code": "x, y, angle, scale = keypoint[:4]\n    x1, y1, x2, y2 = crop_coords\n    return x - x1, y - y1, angle, scale"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Keypoint random crop.\n\n    Args:\n        keypoint: (tuple): A keypoint `(x, y, angle, scale)`.\n        crop_height (int): Crop height.\n        crop_width (int): Crop width.\n        h_start (int): Crop height start.\n        w_start (int): Crop width start.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nRandomly crops a keypoint within the specified image dimensions.\n\nThis method calculates random cropping coordinates based on the input start positions and crop size, \nand then crops the provided keypoint accordingly.\n\nArgs:\n    keypoint: The keypoint to be cropped.\n    crop_height: The height of the crop box.\n    crop_width: The width of the crop box.\n    h_start: The starting vertical position for the crop.\n    w_start: The starting horizontal position for the crop.\n    rows: The total number of rows (image height).\n    cols: The total number of columns (image width).\n\nReturns:\n    The cropped keypoint with coordinates adjusted according to the random crop.\n\"\"\"",
                "method_name": "keypoint_random_crop",
                "second_doc": "\"\"\"\nRandomly crops a keypoint within given image boundaries by generating crop coordinates based on input parameters and adjusting the keypoint's position to fit within the new crop region.\n\nThis method ensures that keypoints remain accurately represented and aligned with their corresponding image sections after a random cropping operation, which is essential for maintaining data consistency during augmentation and model training.\n\nArgs:\n    keypoint: The keypoint to adjust, usually represented as a coordinate or tuple.\n    crop_height: The height of the desired crop.\n    crop_width: The width of the desired crop.\n    h_start: The top pixel position from which to start the crop.\n    w_start: The left pixel position from which to start the crop.\n    rows: The original image height (in pixels).\n    cols: The original image width (in pixels).\n\nReturns:\n    The keypoint with coordinates updated and mapped to the randomly selected crop region.\n\"\"\"",
                "source_code": "crop_coords = get_random_crop_coords(rows, cols, crop_height, crop_width, h_start, w_start)\n    return crop_keypoint_by_coords(keypoint, crop_coords, crop_height, crop_width, rows, cols)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Keypoint center crop.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n        crop_height (int): Crop height.\n        crop_width (int): Crop width.\n        h_start (int): Crop height start.\n        w_start (int): Crop width start.\n        rows (int): Image height.\n        cols (int): Image width.\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nCrops the given keypoint to a centered region of the specified size within an image.\n\nArgs:\n    keypoint: The keypoint to be cropped, typically in coordinate form.\n    crop_height: The height of the crop window.\n    crop_width: The width of the crop window.\n    rows: The total number of rows (height) in the original image.\n    cols: The total number of columns (width) in the original image.\n\nReturns:\n    The cropped keypoint, adjusted to the coordinates of the centered crop region.\n\"\"\"",
                "method_name": "keypoint_center_crop",
                "second_doc": "\"\"\"\nAdjusts a keypoint's coordinates to correspond to a central crop of the specified size within an image, ensuring keypoint positions remain consistent after spatial modifications.\n\nArgs:\n    keypoint: The input keypoint, represented by its coordinates in the original image.\n    crop_height: Height of the centered crop window.\n    crop_width: Width of the centered crop window.\n    rows: Total height (number of rows) of the source image.\n    cols: Total width (number of columns) of the source image.\n\nReturns:\n    The adjusted keypoint with coordinates relative to the central crop region.\n    \nWhy:\n    This adjustment ensures keypoints remain correctly aligned and meaningful after cropping, which is essential for maintaining accurate spatial relationships when transforming images for further processing or model training.\n\"\"\"",
                "source_code": "crop_coords = get_center_crop_coords(rows, cols, crop_height, crop_width)\n    return crop_keypoint_by_coords(keypoint, crop_coords, crop_height, crop_width, rows, cols)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Unified rounding in all python versions.\"\"\"",
                "first_doc": "\"\"\"\nRounds a number to the nearest integer using Python 3's rounding rules.\n\nIf the fractional part of the number is exactly 0.5, the function rounds to the nearest even integer (banker's rounding). Otherwise, it rounds normally to the nearest integer.\n\nArgs:\n    number: The number to be rounded.\n\nReturns:\n    int: The number rounded to the nearest integer using Python 3's rounding policy.\n\"\"\"",
                "method_name": "py3round",
                "second_doc": "\"\"\"\nRounds a given number to the nearest integer following Python 3's rounding convention, ensuring consistency in how midpoint values are handled.\n\nThis method is used to avoid bias in rounding operations, especially when repeated rounding could impact aggregate results, which is important for tasks that require deterministic and statistically balanced results.\n\nArgs:\n    number (float): The numeric value to round.\n\nReturns:\n    int: The result of rounding the input to the nearest integer, with ties rounded to the nearest even integer.\n\"\"\"",
                "source_code": "if abs(round(number) - number) == 0.5:\n        return int(2.0 * round(number / 2.0))\n\n    return int(round(number))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "noop",
                "second_doc": "\"\"\"\nReturns the input object unchanged, allowing it to pass through transformation pipelines without modification.\n\nThis method is useful when building flexible data processing workflows that may conditionally apply or skip certain operations.\n\nArgs:\n    input_obj: The object to be returned as-is. This can be an image, annotation, or any other data type handled by the processing pipeline.\n\nReturns:\n    The same input_obj, unaltered.\n\"\"\"",
                "source_code": "return input_obj"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Swap tiles on image.\n\n    Args:\n        image (np.ndarray): Input image.\n        tiles (np.ndarray): array of tuples(\n            current_left_up_corner_row, current_left_up_corner_col,\n            old_left_up_corner_row, old_left_up_corner_col,\n            height_tile, width_tile)\n\n    Returns:\n        np.ndarray: Output image.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nSwaps specified rectangular regions (tiles) between different locations on an image.\n\nArgs:\n    image: The input image represented as a numpy array.\n    tiles: A list of tile specifications. Each specification should be an iterable with six elements: (source_row, source_col, target_row, target_col, height, width).\n\nReturns:\n    A new numpy array representing the image with the specified tiles swapped to their target locations.\n\"\"\"",
                "method_name": "swap_tiles_on_image",
                "second_doc": "\"\"\"\nReplaces specified rectangular regions in an image by copying pixel values from other regions within the same image.\n\nThis method allows for rearranging selected areas (tiles) to new positions, which can introduce controlled spatial variations for further image processing or transformation tasks.\n\nArgs:\n    image (numpy.ndarray): Input image in the form of a NumPy array.\n    tiles (Iterable[Iterable[int]]): List of tile specifications, where each tile is defined by six integers: (source_row, source_col, target_row, target_col, height, width), specifying the region to copy from and its destination.\n\nReturns:\n    numpy.ndarray: A new image array with specified tiles moved to their target positions.\n\"\"\"",
                "source_code": "new_image = image.copy()\n\n    for tile in tiles:\n        new_image[tile[0] : tile[0] + tile[4], tile[1] : tile[1] + tile[5]] = image[\n            tile[2] : tile[2] + tile[4], tile[3] : tile[3] + tile[5]\n        ]\n\n    return new_image"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Rotate a keypoint by angle.\n\n    Args:\n        keypoint (tuple): A keypoint `(x, y, angle, scale)`.\n\n    Returns:\n        tuple: A keypoint `(x, y, angle, scale)`.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nTransforms the given keypoint by transposing its coordinates and adjusting its angle.\n\nThis method takes a keypoint, swaps its x and y coordinates, and modifies its angle to account for transposition. The angle is first normalized to the [0, 2\u03c0) range, and then transformed based on its current value to ensure proper orientation after transposition.\n\nArgs:\n    keypoint: A sequence containing at least four elements representing the keypoint's x-coordinate, y-coordinate, angle, and scale.\n\nReturns:\n    tuple: A tuple containing the transposed y and x coordinates, the adjusted angle, and the original scale.\n\"\"\"",
                "method_name": "keypoint_transpose",
                "second_doc": "\"\"\"\nTransposes the spatial coordinates of a keypoint and recalculates its angle to maintain accurate geometric representation after transposition.\n\nSwapping the x and y coordinates and adjusting the orientation is necessary to keep keypoint alignment consistent with other transformed image data, ensuring geometric transformations remain coherent when augmenting images for computer vision tasks.\n\nArgs:\n    keypoint (Sequence[float]): A sequence of at least four elements representing the keypoint's x-coordinate, y-coordinate, angle (in radians), and scale.\n\nReturns:\n    tuple: A tuple of (y, x, adjusted_angle, scale), where the coordinates are transposed and the angle is recalibrated to reflect the new orientation.\n\"\"\"",
                "source_code": "x, y, angle, scale = keypoint[:4]\n    angle = angle_to_2pi_range(angle)\n\n    if angle <= np.pi:\n        angle = np.pi - angle\n    else:\n        angle = 3 * np.pi - angle\n\n    return y, x, angle, scale"
            },
            "type": "function"
        }
    ],
    "albumentations/albumentations/augmentations/keypoints_utils.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "default_data_name",
                    "second_doc": "\"\"\"\nProvides the standard data field name used for storing or accessing keypoints within the processor. \n\nReturns:\n    str: The default key under which keypoint data is expected, ensuring consistent handling of keypoints during data augmentation pipelines.\n\"\"\"",
                    "source_code": "return \"keypoints\""
                },
                {
                    "docstring": null,
                    "method_name": "ensure_data_valid",
                    "second_doc": "\"\"\"\nValidate that all required label fields specified in the parameters are present in the provided data dictionary.\n\nThis ensures consistency between the expected and actual data structure before proceeding with keypoint processing, helping prevent downstream errors due to missing or misnamed fields.\n\nArgs:\n    data (dict): Input data containing keypoint information and associated fields.\n\nRaises:\n    ValueError: If any of the required label fields specified in the parameters are missing from the data.\n\"\"\"",
                    "source_code": "if self.params.label_fields:\n            if not all(l in data.keys() for l in self.params.label_fields):\n                raise ValueError(\n                    \"Your 'label_fields' are not valid - them must have same names as params in \"\n                    \"'keypoint_params' dict\"\n                )"
                },
                {
                    "docstring": null,
                    "method_name": "ensure_transforms_valid",
                    "second_doc": "\"\"\"\nChecks whether the provided list of transforms is compatible with the current keypoints format and issues a warning if a transformation does not fully support the format. This validation helps avoid unintentional data inconsistencies and ensures the correct application of transformations.\n\nArgs:\n    transforms (list): List of transformation objects to validate against the current keypoints format.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "from albumentations.imgaug.transforms import DualIAATransform\n\n        if self.params.format is not None and self.params.format != \"xy\":\n            for transform in transforms:\n                if isinstance(transform, DualIAATransform):\n                    warnings.warn(\n                        \"{} transformation supports only 'xy' keypoints \"\n                        \"augmentation. You have '{}' keypoints format. Scale \"\n                        \"and angle WILL NOT BE transformed.\".format(transform.__class__.__name__, self.params.format)\n                    )\n                    break"
                },
                {
                    "docstring": null,
                    "method_name": "filter",
                    "second_doc": "\"\"\"\nFilter keypoints based on image dimensions and visibility settings to ensure only valid keypoints are retained for further processing.\n\nArgs:\n    data: List or array of keypoints to be filtered.\n    rows: The number of rows (height) of the image.\n    cols: The number of columns (width) of the image.\n\nReturns:\n    A filtered list or array of keypoints that are within image bounds and, optionally, only those that are visible.\n\nWhy:\n    Filtering keypoints in this way helps maintain the integrity of transformed data and prevents invalid or out-of-bounds keypoints from affecting downstream processing.\n\"\"\"",
                    "source_code": "return filter_keypoints(data, rows, cols, remove_invisible=self.params.remove_invisible)"
                },
                {
                    "docstring": null,
                    "method_name": "check",
                    "second_doc": "\"\"\"\nChecks the validity and placement of keypoints with respect to the specified image dimensions.\n\nArgs:\n    data (Any): The keypoints data to be validated.\n    rows (int): The number of rows (height) of the image.\n    cols (int): The number of columns (width) of the image.\n\nReturns:\n    Any: The result of the keypoint validation, typically indicating whether the keypoints are within the desired bounds.\n\nWhy:\n    This method ensures that keypoints are correctly positioned within the image boundaries, which is essential for maintaining data integrity and avoiding errors in subsequent processing or augmentation steps.\n\"\"\"",
                    "source_code": "return check_keypoints(data, rows, cols)"
                },
                {
                    "docstring": null,
                    "method_name": "convert_from_albumentations",
                    "second_doc": "\"\"\"\nConverts keypoints from the Albumentations format to the internal representation used by the processor.\n\nArgs:\n    data (list): List of keypoints in the format produced by Albumentations' transformations.\n    rows (int): Number of rows (height) in the image.\n    cols (int): Number of columns (width) in the image.\n\nReturns:\n    list: Keypoints converted to the target format, validated and reprojected as necessary.\n\nThis method ensures that keypoint annotations remain accurate and usable after data augmentation operations, maintaining consistency with expected input formats for further processing or model training.\n\"\"\"",
                    "source_code": "return convert_keypoints_from_albumentations(\n            data,\n            self.params.format,\n            rows,\n            cols,\n            check_validity=self.params.remove_invisible,\n            angle_in_degrees=self.params.angle_in_degrees,\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "convert_to_albumentations",
                    "second_doc": "\"\"\"\nConverts keypoints and related metadata into a format compatible with advanced image augmentation workflows, ensuring data consistency and supporting further spatial transformations.\n\nArgs:\n    data: The keypoint data to be converted.\n    rows (int): The height of the corresponding image.\n    cols (int): The width of the corresponding image.\n\nReturns:\n    Converted keypoint representations suitable for downstream augmentation operations.\n\"\"\"",
                    "source_code": "return convert_keypoints_to_albumentations(\n            data,\n            self.params.format,\n            rows,\n            cols,\n            check_validity=self.params.remove_invisible,\n            angle_in_degrees=self.params.angle_in_degrees,\n        )"
                }
            ],
            "name": "KeypointsProcessor",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"Check if keypoint coordinates are less than image shapes\"\"\"",
                "first_doc": "\"\"\"\nChecks if the given keypoint coordinates are within the valid image boundaries.\n\nArgs:\n    kp: The keypoint coordinates as a sequence (typically of length 2).\n    rows: The total number of rows in the image, defining the vertical boundary.\n    cols: The total number of columns in the image, defining the horizontal boundary.\n\nReturns:\n    None. Raises a ValueError if any keypoint coordinate is out of bounds.\n\"\"\"",
                "method_name": "check_keypoint",
                "second_doc": "\"\"\"\nValidates that the provided keypoint coordinates fall within the defined image boundaries to prevent processing invalid or out-of-range points.\n\nArgs:\n    kp: Sequence containing the keypoint coordinates (expected as at least two values for x and y positions).\n    rows: Integer specifying the number of rows (vertical boundary) of the image.\n    cols: Integer specifying the number of columns (horizontal boundary) of the image.\n\nReturns:\n    None\n\nRaises:\n    ValueError: If either of the keypoint coordinates lies outside the image boundaries.\n\"\"\"",
                "source_code": "for name, value, size in zip([\"x\", \"y\"], kp[:2], [cols, rows]):\n        if not 0 <= value < size:\n            raise ValueError(\n                \"Expected {name} for keypoint {kp} \"\n                \"to be in the range [0.0, {size}], got {value}.\".format(kp=kp, name=name, value=value, size=size)\n            )"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Check if keypoints boundaries are less than image shapes\"\"\"",
                "first_doc": "\"\"\"\nChecks the validity of each keypoint in the provided list using the specified image dimensions.\n\nArgs:\n    keypoints: A list containing the keypoints to be checked.\n    rows: The number of rows representing the image height.\n    cols: The number of columns representing the image width.\n\nReturns:\n    None: This method does not return a value.\n\"\"\"",
                "method_name": "check_keypoints",
                "second_doc": "\"\"\"\nValidates each keypoint in the input list to ensure it falls within the specified image boundaries. This helps maintain data consistency and prevents downstream errors during image augmentation operations.\n\nArgs:\n    keypoints: List of keypoints to validate.\n    rows: Integer representing the image height.\n    cols: Integer representing the image width.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "for kp in keypoints:\n        check_keypoint(kp, rows, cols)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "filter_keypoints",
                "second_doc": "\"\"\"\nFilters out keypoints that fall outside the valid image boundaries if specified.\n\nThis method ensures that only keypoints located within the dimensions of the image are retained, which helps maintain data integrity during further processing steps.\n\nArgs:\n    keypoints (list): A list of keypoint coordinates, where each keypoint is represented as a sequence (x, y, ...).\n    cols (int): Number of columns (width) of the image.\n    rows (int): Number of rows (height) of the image.\n    remove_invisible (bool): Whether to filter out keypoints located outside the image area.\n\nReturns:\n    list: The filtered list of keypoints, containing only those within image boundaries if removal is enabled; otherwise, returns all provided keypoints.\n\"\"\"",
                "source_code": "if not remove_invisible:\n        return keypoints\n\n    resulting_keypoints = []\n    for kp in keypoints:\n        x, y = kp[:2]\n        if x < 0 or x >= cols:\n            continue\n        if y < 0 or y >= rows:\n            continue\n        resulting_keypoints.append(kp)\n    return resulting_keypoints"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "keypoint_has_extra_data",
                "second_doc": "\"\"\"\nChecks whether a given keypoint contains additional data fields beyond those specified by the format.\n\nArgs:\n    kp (tuple): The keypoint data to be checked.\n    format (tuple): The expected format of the keypoint.\n\nReturns:\n    bool: True if the keypoint includes more elements than the format specifies, indicating the presence of extra data; False otherwise.\n\nThis function ensures compatibility and consistency when handling keypoint data, which may sometimes include auxiliary information not originally defined in the format specification.\n\"\"\"",
                "source_code": "return len(kp) > len(format)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "convert_keypoint_to_albumentations",
                "second_doc": "\"\"\"\nConverts a keypoint from a specified format to the internal (x, y, a, s, ...) format expected by augmentation routines, ensuring compatibility and correctness for subsequent image transformations.\n\nThis function ensures that keypoints provided in various commonly used coordinate formats are standardized, validates them if required, and converts angle units if necessary, thereby supporting a seamless augmentation workflow.\n\nArgs:\n    keypoint (tuple): The keypoint coordinates in the given source_format.\n    source_format (str): The format of the input keypoint (e.g., 'xy', 'yx', 'xya', etc.).\n    angle_in_degrees (bool): If True, converts angle values from degrees to radians.\n    check_validity (bool): If True, validates the resulting keypoint against image boundaries.\n    rows (int): The number of rows in the image (height), used for validity checking.\n    cols (int): The number of columns in the image (width), used for validity checking.\n\nReturns:\n    tuple: The keypoint converted to the standardized (x, y, a, s, ...) format compatible with further image augmentations.\n\"\"\"",
                "source_code": "if source_format not in keypoint_formats:\n        raise ValueError(\"Unknown target_format {}. Supported formats are: {}\".format(source_format, keypoint_formats))\n\n    if source_format == \"xy\":\n        (x, y), tail = keypoint[:2], tuple(keypoint[2:])\n        a, s = 0, 0\n    elif source_format == \"yx\":\n        (y, x), tail = keypoint[:2], tuple(keypoint[2:])\n        a, s = 0, 0\n    elif source_format == \"xya\":\n        (x, y, a), tail = keypoint[:3], tuple(keypoint[3:])\n        s = 0\n    elif source_format == \"xys\":\n        (x, y, s), tail = keypoint[:3], tuple(keypoint[3:])\n        a = 0\n    elif source_format == \"xyas\":\n        (x, y, a, s), tail = keypoint[:4], tuple(keypoint[4:])\n    elif source_format == \"xysa\":\n        (x, y, s, a), tail = keypoint[:4], tuple(keypoint[4:])\n\n    if angle_in_degrees:\n        a = math.radians(a)\n\n    keypoint = (x, y, a, s) + tail\n    if check_validity:\n        check_keypoint(keypoint, rows, cols)\n    return keypoint"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "normalize_angle",
                "second_doc": "\"\"\"\nAdjusts the angle value to ensure it falls within the interval [0, 2\u03c0].\n\nThis function is useful in contexts where angle consistency is necessary, such as when processing geometric transformations. By constraining the angle to a canonical range, it helps prevent computational errors or ambiguities that might arise from out-of-bound values.\n\nArgs:\n    a (float): Angle value in radians to be normalized.\n\nReturns:\n    float: The normalized angle within the range [0, 2\u03c0].\n\"\"\"",
                "source_code": "two_pi = 2.0 * math.pi\n    while a < 0:\n        a += two_pi\n    while a > two_pi:\n        a -= two_pi\n    return a"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "convert_keypoint_from_albumentations",
                "second_doc": "\"\"\"\nConvert a keypoint from the default internal representation to a specified output format, optionally validating and adjusting angle representation.\n\nArgs:\n    keypoint (tuple): Keypoint coordinates and attributes in the format (x, y, angle, scale, ...).\n    target_format (str): Desired output format for the keypoint (e.g., 'xy', 'yx', 'xya', etc.).\n    rows (int): Number of image rows, used for validity checking.\n    cols (int): Number of image columns, used for validity checking.\n    check_validity (bool): If True, validate that the keypoint is within image boundaries.\n    angle_in_degrees (bool): If True, convert angle component to degrees.\n\nReturns:\n    tuple: Reordered and potentially validated keypoint in the requested format.\n\nRaises:\n    ValueError: If the target_format is not recognized.\n\nWhy:\n    To ensure keypoints are correctly formatted and validated for further processing or output, allowing seamless interoperability with different consumers or downstream pipelines that may expect various keypoint ordering or angle units.\n\"\"\"",
                "source_code": "if target_format not in keypoint_formats:\n        raise ValueError(\"Unknown target_format {}. Supported formats are: {}\".format(target_format, keypoint_formats))\n    if check_validity:\n        check_keypoint(keypoint, rows, cols)\n\n    (x, y, angle, scale), tail = keypoint[:4], tuple(keypoint[4:])\n    angle = normalize_angle(angle)\n    if angle_in_degrees:\n        angle = math.degrees(angle)\n\n    if target_format == \"xy\":\n        kp = (x, y)\n    elif target_format == \"yx\":\n        kp = (y, x)\n    elif target_format == \"xya\":\n        kp = (x, y, angle)\n    elif target_format == \"xys\":\n        kp = (x, y, scale)\n    elif target_format == \"xyas\":\n        kp = (x, y, angle, scale)\n    elif target_format == \"xysa\":\n        kp = (x, y, scale, angle)\n\n    return kp + tail"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "convert_keypoints_to_albumentations",
                "second_doc": "\"\"\"\nTransforms a list of keypoints from the specified source format into the format required by the internal processing pipeline, applying any necessary geometric adjustments and validity checks.\n\nThis method ensures keypoints are appropriately prepared for consistent downstream processing, typically after image transformations.\n\nArgs:\n    keypoints (list): List of keypoints to be converted.\n    source_format (str): Format of the source keypoints (e.g., 'xy', 'yx').\n    rows (int): Image height, used for coordinate normalization or transformation.\n    cols (int): Image width, used for coordinate normalization or transformation.\n    check_validity (bool): Whether to validate keypoints after conversion.\n    angle_in_degrees (bool): Indicates if rotation angles should be interpreted in degrees.\n\nReturns:\n    list: Converted keypoints formatted for subsequent processing and augmentation steps.\n\"\"\"",
                "source_code": "return [\n        convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "convert_keypoints_from_albumentations",
                "second_doc": "\"\"\"\nConverts a batch of keypoints from the Albumentations format to the specified target format. This ensures that keypoint data remains compatible with downstream processing and model requirements after transformations.\n\nArgs:\n    keypoints (list): A list of keypoints in the Albumentations format.\n    target_format (str): The desired format to convert each keypoint to.\n    rows (int): The number of rows (height) in the image for normalization or coordinate calculations.\n    cols (int): The number of columns (width) in the image for normalization or coordinate calculations.\n    check_validity (bool): Whether to validate each converted keypoint.\n    angle_in_degrees (bool): Whether angle values should be in degrees or radians.\n\nReturns:\n    list: A list of converted keypoints in the specified target format.\n\"\"\"",
                "source_code": "return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]"
            },
            "type": "function"
        }
    ],
    "albumentations/albumentations/augmentations/transforms.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the PadIfNeeded transformation, which ensures an image reaches at least the specified minimum height and width by applying padding if necessary.\n\nThis helps maintain consistent input dimensions for downstream processing, reducing the risk of errors during model training or inference when input images are smaller than expected.\n\nArgs:\n    always_apply (bool): Whether to always apply this transformation.\n    p (float): Probability of applying the transformation.\n    min_height (int): Minimum allowed image height after padding.\n    min_width (int): Minimum allowed image width after padding.\n    border_mode (int): Pixel extrapolation method for padding.\n    value (float, tuple, or int): Padding value for image areas.\n    mask_value (float, tuple, or int): Padding value for mask areas.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(PadIfNeeded, self).__init__(always_apply, p)\n        self.min_height = min_height\n        self.min_width = min_width\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value"
                },
                {
                    "docstring": null,
                    "method_name": "update_params",
                    "second_doc": "\"\"\"\nUpdate the padding parameters in order to ensure that the image dimensions meet the specified minimum height and width. This is necessary to guarantee that subsequent transformations or model inputs receive images of at least the required size, thereby preserving consistency and preventing errors that can occur when processing undersized images.\n\nArgs:\n    params (dict): Dictionary containing current image transformation parameters, including image dimensions.\n    **kwargs: Additional keyword arguments.\n\nReturns:\n    dict: Updated parameters dictionary with calculated values for 'pad_top', 'pad_bottom', 'pad_left', and 'pad_right' to satisfy minimum size constraints.\n\"\"\"",
                    "source_code": "params = super(PadIfNeeded, self).update_params(params, **kwargs)\n        rows = params[\"rows\"]\n        cols = params[\"cols\"]\n\n        if rows < self.min_height:\n            h_pad_top = int((self.min_height - rows) / 2.0)\n            h_pad_bottom = self.min_height - rows - h_pad_top\n        else:\n            h_pad_top = 0\n            h_pad_bottom = 0\n\n        if cols < self.min_width:\n            w_pad_left = int((self.min_width - cols) / 2.0)\n            w_pad_right = self.min_width - cols - w_pad_left\n        else:\n            w_pad_left = 0\n            w_pad_right = 0\n\n        params.update(\n            {\"pad_top\": h_pad_top, \"pad_bottom\": h_pad_bottom, \"pad_left\": w_pad_left, \"pad_right\": w_pad_right}\n        )\n        return params"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nPads the input image with the specified number of pixels on each side using given border mode and fill value.\n\nThis method ensures that the resulting image meets the required spatial dimensions, which can be necessary for consistent processing in image pipelines that expect inputs of a particular size.\n\nArgs:\n    img (numpy.ndarray): The input image to be padded.\n    pad_top (int): Number of pixels to add to the top of the image.\n    pad_bottom (int): Number of pixels to add to the bottom of the image.\n    pad_left (int): Number of pixels to add to the left side of the image.\n    pad_right (int): Number of pixels to add to the right side of the image.\n\nReturns:\n    numpy.ndarray: The padded image.\n\"\"\"",
                    "source_code": "return F.pad_with_params(\n            img, pad_top, pad_bottom, pad_left, pad_right, border_mode=self.border_mode, value=self.value\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nPads a mask array with the specified padding values and mode.\n\nThis method applies padding to a mask to ensure it matches the required spatial dimensions after image transformations, which maintains alignment between the data and associated labels or masks.\n\nArgs:\n    img (numpy.ndarray): The mask array to pad.\n    pad_top (int): Number of pixels to pad at the top.\n    pad_bottom (int): Number of pixels to pad at the bottom.\n    pad_left (int): Number of pixels to pad on the left.\n    pad_right (int): Number of pixels to pad on the right.\n\nReturns:\n    numpy.ndarray: The padded mask array.\n\"\"\"",
                    "source_code": "return F.pad_with_params(\n            img, pad_top, pad_bottom, pad_left, pad_right, border_mode=self.border_mode, value=self.mask_value\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nAdjusts the bounding box coordinates to account for padding applied to the image.\n\nThis method recalculates bounding box positions after the image undergoes padding, ensuring that the boxes remain accurately aligned with their corresponding objects in the transformed image space.\n\nArgs:\n    bbox (tuple): Bounding box in normalized coordinates (x_min, y_min, x_max, y_max).\n    rows (int): Number of original image rows (height).\n    cols (int): Number of original image columns (width).\n    pad_top (int): Amount of padding added to the top of the image.\n    pad_bottom (int): Amount of padding added to the bottom of the image.\n    pad_left (int): Amount of padding added to the left of the image.\n    pad_right (int): Amount of padding added to the right of the image.\n\nReturns:\n    tuple: Bounding box in normalized coordinates, updated to match the padded image dimensions.\n\"\"\"",
                    "source_code": "x_min, y_min, x_max, y_max = denormalize_bbox(bbox, rows, cols)\n        bbox = x_min + pad_left, y_min + pad_top, x_max + pad_left, y_max + pad_top\n        return normalize_bbox(bbox, rows + pad_top + pad_bottom, cols + pad_left + pad_right)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nAdjusts the position of a keypoint by adding padding offsets to its x and y coordinates.\n\nThis method is used to ensure that keypoint annotations remain correctly aligned with the image when padding is applied. It helps maintain the spatial accuracy of keypoints after augmenting the image with additional borders.\n\nArgs:\n    keypoint (tuple): A keypoint specified as (x, y, angle, scale).\n    pad_left (int): Amount of padding added to the left side of the image.\n    pad_top (int): Amount of padding added to the top of the image.\n\nReturns:\n    tuple: The adjusted keypoint as (x + pad_left, y + pad_top, angle, scale).\n\"\"\"",
                    "source_code": "x, y, angle, scale = keypoint\n        return x + pad_left, y + pad_top, angle, scale"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the arguments required to initialize the padding transformation, ensuring consistent storage and reconstruction of the transformation's configuration.\n\nReturns:\n    tuple: Names of the initialization parameters ('min_height', 'min_width', 'border_mode', 'value', 'mask_value').\n\"\"\"",
                    "source_code": "return (\"min_height\", \"min_width\", \"border_mode\", \"value\", \"mask_value\")"
                }
            ],
            "name": "PadIfNeeded",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Crop transformation by specifying the coordinates of the rectangular region to extract from an image.  \nThis allows selective region-based augmentation, enabling models to learn from localized parts of the input.\n\nArgs:\n    x_min (int): The minimum x-coordinate (left) of the crop region.\n    y_min (int): The minimum y-coordinate (top) of the crop region.\n    x_max (int): The maximum x-coordinate (right) of the crop region.\n    y_max (int): The maximum y-coordinate (bottom) of the crop region.\n    always_apply (bool, optional): If True, apply the crop to every input. Defaults to False.\n    p (float, optional): Probability of applying the crop. Defaults to 1.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Crop, self).__init__(always_apply, p)\n        self.x_min = x_min\n        self.y_min = y_min\n        self.x_max = x_max\n        self.y_max = y_max"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nCrop the input image to the region specified by the bounding box coordinates.\n\nThis method extracts a specific region from the input image according to predefined coordinates, allowing for precise selection of a sub-area.\n\nArgs:\n    img: The input image to be cropped.\n\nReturns:\n    The cropped image, corresponding to the region defined by (x_min, y_min, x_max, y_max).\n\"\"\"",
                    "source_code": "return F.crop(img, x_min=self.x_min, y_min=self.y_min, x_max=self.x_max, y_max=self.y_max)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nCrop a bounding box to fit within the specified crop region parameters.\n\nThis method adjusts the given bounding box so that it aligns with the cropping area defined by the instance's coordinates. Cropping bounding boxes ensures that object localization remains accurate after image transformations.\n\nArgs:\n    bbox: The bounding box to adjust, typically specified as a sequence of coordinates.\n    **params: Additional parameters that may be needed for bounding box cropping.\n\nReturns:\n    The cropped bounding box, with coordinates updated to match the crop region.\n\"\"\"",
                    "source_code": "return F.bbox_crop(bbox, x_min=self.x_min, y_min=self.y_min, x_max=self.x_max, y_max=self.y_max, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nAdjusts the coordinates of a keypoint to reflect its position within the cropped region of the image.\n\nThis method recalculates a keypoint's location after an image crop, ensuring that the keypoint remains accurately mapped within the new image boundaries. This is necessary to keep annotation data synchronized with the modified image content when applying crops.\n\nArgs:\n    keypoint (tuple): The original coordinates of the keypoint in the format (x, y, ...).\n    params (dict): Dictionary containing image parameters, specifically 'rows' and 'cols' which describe the image dimensions.\n\nReturns:\n    tuple: The transformed keypoint coordinates adjusted to the cropped area.\n\"\"\"",
                    "source_code": "return F.crop_keypoint_by_coords(\n            keypoint,\n            crop_coords=(self.x_min, self.y_min, self.x_max, self.y_max),\n            crop_height=self.y_max - self.y_min,\n            crop_width=self.x_max - self.x_min,\n            rows=params[\"rows\"],\n            cols=params[\"cols\"],\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initial arguments required to define the cropping region for an image transformation.\n\nThis method enables consistent handling and reconstruction of transformation parameters, ensuring that image augmentations can be serialized, deserialized, or reapplied accurately in different settings.\n\nReturns:\n    tuple: A tuple containing the names ('x_min', 'y_min', 'x_max', 'y_max') of the parameters that specify the crop boundaries.\n\"\"\"",
                    "source_code": "return (\"x_min\", \"y_min\", \"x_max\", \"y_max\")"
                }
            ],
            "name": "Crop",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nFlip the given image vertically.\n\nThis operation is performed to increase dataset diversity by introducing mirrored versions of existing images, which can help reduce overfitting and improve the robustness of computer vision models during training.\n\nArgs:\n    img: Input image to be flipped. Should be a numpy array or a compatible image object.\n\nReturns:\n    Flipped image with its content mirrored along the vertical axis.\n\"\"\"",
                    "source_code": "return F.vflip(img)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nFlip bounding box coordinates vertically.\n\nThis method transforms bounding box coordinates to reflect a vertical flip of the corresponding image, ensuring spatial annotations remain consistent after augmentation.\n\nArgs:\n    bbox (Any): The bounding box to be vertically flipped. The format depends on the transform pipeline and configuration.\n    **params: Additional parameters necessary for the vertical flipping operation.\n\nReturns:\n    Any: The vertically flipped bounding box in the same format as the input.\n\"\"\"",
                    "source_code": "return F.bbox_vflip(bbox, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nFlip keypoint coordinates vertically to match the transformation applied to the associated image.\n\nThis method ensures that when an image is flipped vertically, the keypoint annotations remain accurate and aligned with the augmented image.\n\nArgs:\n    keypoint (tuple): A tuple representing a single keypoint's (x, y, angle, scale) parameters.\n    **params: Additional keyword arguments required for the vertical flip operation.\n\nReturns:\n    tuple: The keypoint tuple updated to reflect the vertical flip.\n\"\"\"",
                    "source_code": "return F.keypoint_vflip(keypoint, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of initialization arguments required to reconstruct this transformation.\n\nReturns:\n    tuple: An empty tuple, indicating that this transformation does not require any initialization arguments.\n\nWhy:\n    This method allows for consistent reconstruction and serialization of transformation pipelines by specifying the required parameters for each transform. For this particular transformation, no arguments are needed for initialization, so an empty tuple is returned.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "VerticalFlip",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nHorizontally flips the input image, selecting the most efficient flipping method based on the image's properties.\n\nArgs:\n    img (np.ndarray): Image to be flipped. Should be a NumPy array.\n\nReturns:\n    np.ndarray: The horizontally flipped image.\n    \nThe method determines whether to use the OpenCV-based or NumPy-based implementation to optimize performance for different image types, ensuring efficient augmentation during image preprocessing.\n\"\"\"",
                    "source_code": "if img.ndim == 3 and img.shape[2] > 1 and img.dtype == np.uint8:\n            # Opencv is faster than numpy only in case of\n            # non-gray scale 8bits images\n            return F.hflip_cv2(img)\n        else:\n            return F.hflip(img)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nHorizontally flips the given bounding box coordinates according to specified parameters.\n\nThis operation ensures that bounding boxes remain accurately aligned with their corresponding images when a horizontal flip transformation is applied, which is essential for maintaining the integrity of training data during augmentation.\n\nArgs:\n    bbox: The bounding box to transform.\n    **params: Additional parameters required for the horizontal flip operation.\n\nReturns:\n    The horizontally flipped bounding box.\n\"\"\"",
                    "source_code": "return F.bbox_hflip(bbox, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nApplies a horizontal flip transformation to a given keypoint using the specified parameters. This ensures that the keypoint annotation remains consistent with the horizontally flipped image, maintaining the integrity of labels for downstream tasks such as detection or pose estimation.\n\nArgs:\n    keypoint (Tuple[float, float, ...]): The keypoint coordinates (and optionally additional keypoint data) to be transformed.\n    **params: Additional parameters required for the horizontal flip operation.\n\nReturns:\n    Tuple[float, float, ...]: The horizontally flipped keypoint coordinates, adjusted as needed to match the flipped image orientation.\n\"\"\"",
                    "source_code": "return F.keypoint_hflip(keypoint, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the parameters required to initialize this transform.\n\nThis method provides necessary information for serialization and reproducibility of image augmentations that require initial arguments, facilitating consistent pipeline construction across different environments.\n\nReturns:\n    tuple: An empty tuple, indicating that this transform does not require any initialization arguments.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "HorizontalFlip",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": "\"\"\"Args:\n        d (int): code that specifies how to flip the input. 0 for vertical flipping, 1 for horizontal flipping,\n                -1 for both vertical and horizontal flipping (which is also could be seen as rotating the input by\n                180 degrees).\n        \"\"\"",
                    "first_doc": "\"\"\"\nApplies a random flip transformation to the input image.\n\nArgs:\n    img: The input image to be randomly flipped.\n\nReturns:\n    The flipped image after applying the random flip transformation.\n\"\"\"",
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nRandomly flips the input image along a specified axis, introducing variability in spatial orientation.\n\nThis transformation is used to expand the diversity of image data, which helps prevent models from overfitting to a particular image orientation during training.\n\nArgs:\n    img: The input image to be transformed.\n    d: The axis or direction along which the random flip is to be applied.\n\nReturns:\n    The image after applying a random flip transformation along the chosen axis.\n\"\"\"",
                    "source_code": "return F.random_flip(img, d)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate a random direction value used to determine how to apply the flip transformation to an input image, ensuring variations in the augmentation process for increased dataset diversity.\n\nReturns:\n    dict: A dictionary with key 'd' mapping to a randomly selected integer (-1, 0, or 1) representing the flip direction.\n\"\"\"",
                    "source_code": "return {\"d\": random.randint(-1, 1)}"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nFlip the input bounding box according to the specified parameters.\n\nThis method transforms the bounding box coordinates to match the orientation of a flipped image, ensuring that object annotations remain consistent with the spatial transformation. This is essential for maintaining correct bounding box labels during data augmentation processes.\n\nArgs:\n    bbox (tuple or list): The bounding box to be transformed.\n    **params: Additional parameters determining the direction and nature of the flip.\n\nReturns:\n    The flipped bounding box with updated coordinates.\n\"\"\"",
                    "source_code": "return F.bbox_flip(bbox, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nFlip the coordinates of a keypoint according to the specified flipping parameters.\n\nThis method ensures that keypoints are spatially adjusted to remain accurate and meaningful when an image is flipped, preserving the correct spatial relationships required for further processing or model training.\n\nArgs:\n    keypoint (tuple): The input keypoint represented as a tuple of coordinates.\n    **params: Additional parameters controlling the flip operation (such as flip direction or image shape).\n\nReturns:\n    tuple: The transformed keypoint with updated coordinates after flipping.\n\"\"\"",
                    "source_code": "return F.keypoint_flip(keypoint, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturn the names of initialization arguments required to reconstruct the Flip transformation. \n\nThis method returns an empty tuple, indicating that this particular transformation does not require storing any explicit parameters for re-initialization. This allows consistent behavior when serializing or composing image augmentation pipelines.\n\nReturns:\n    tuple: An empty tuple, as no initialization arguments are needed for the Flip transform.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "Flip",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nTranspose the given image by swapping its height and width dimensions.\n\nThis method enhances variability in the training data by altering the spatial arrangement of pixels, which helps improve the robustness and generalization of computer vision models.\n\nArgs:\n    img (numpy.ndarray): Input image to be transposed.\n\nReturns:\n    numpy.ndarray: The transposed image with swapped height and width.\n\"\"\"",
                    "source_code": "return F.transpose(img)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nApply the transpose transformation to the given bounding box according to the specified parameters. This ensures that the bounding box remains accurately aligned with the image after a transpose operation.\n\nArgs:\n    bbox (tuple): A tuple representing the bounding box coordinates to be transposed.\n    **params: Additional parameters required for the transpose operation.\n\nReturns:\n    tuple: Transposed bounding box coordinates.\n\"\"\"",
                    "source_code": "return F.bbox_transpose(bbox, 0, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nTransforms the input keypoint coordinates using a transposition operation, ensuring consistency between image and annotation modifications during augmentation.\n\nArgs:\n    keypoint (tuple): The keypoint coordinates to be transformed, typically in the form (x, y, ...).\n\nReturns:\n    tuple: The transposed keypoint coordinates, reflecting the transformation applied to the image.\n\"\"\"",
                    "source_code": "return F.keypoint_transpose(keypoint)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns a tuple of argument names required to initialize the transform.\n\nThis method provides a standardized way to communicate which constructor arguments are relevant for serialization or reproducibility. Returning an empty tuple indicates that this transform does not require any specific initialization parameters.\n\nReturns:\n    tuple: An empty tuple, as no initialization arguments are needed.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "Transpose",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an instance by setting up parameters for resizing images to ensure none of their dimensions exceed a specified maximum size.\n\nArgs:\n    always_apply (bool): Whether to always apply this transformation.\n    p (float): Probability of applying the transformation.\n    interpolation (int): Interpolation method used during resizing.\n    max_size (int): The maximum allowed size for any spatial dimension of the image.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(LongestMaxSize, self).__init__(always_apply, p)\n        self.interpolation = interpolation\n        self.max_size = max_size"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nResize the input image so that its longest side does not exceed the specified maximum size, maintaining the original aspect ratio.\n\nThis method ensures that images conform to a consistent dimension limit without distortion, which is important for standardizing input data and optimizing downstream processing steps.\n\nArgs:\n    img: The input image to be resized.\n    interpolation: The interpolation method to use when resizing.\n\nReturns:\n    The resized image with the longest side equal to or less than max_size, preserving the aspect ratio.\n\"\"\"",
                    "source_code": "return F.longest_max_size(img, max_size=self.max_size, interpolation=interpolation)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nReturns the input bounding box unchanged. This method ensures that bounding box coordinates are preserved during transformations, maintaining consistency with the augmented image data.\n\nArgs:\n    bbox: The bounding box to be processed, typically represented as a tuple or list of coordinates.\n\nReturns:\n    The input bounding box, unmodified.\n\"\"\"",
                    "source_code": "return bbox"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nScales a keypoint's coordinates so that the largest side of the image matches the specified maximum size, preserving spatial alignment with the corresponding image transformation.\n\nArgs:\n    keypoint (tuple): The (x, y) coordinates of the keypoint to transform.\n    params (dict): Dictionary containing 'rows' and 'cols' specifying the image's current height and width.\n\nReturns:\n    tuple: The transformed (x, y) coordinates of the keypoint after scaling.\n    \nWhy:\n    This ensures that keypoints remain accurately positioned relative to the resized image, maintaining data consistency during preprocessing for tasks such as detection or pose estimation.\n\"\"\"",
                    "source_code": "height = params[\"rows\"]\n        width = params[\"cols\"]\n\n        scale = self.max_size / max([height, width])\n        return F.keypoint_scale(keypoint, scale, scale)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required for configuring this image transformation. This allows for consistent tracking, serialization, and reproducibility of transformation parameters.\n\nReturns:\n    tuple: A tuple containing the names of the arguments (\"max_size\", \"interpolation\") used to initialize the transformation.\n\"\"\"",
                    "source_code": "return (\"max_size\", \"interpolation\")"
                }
            ],
            "name": "LongestMaxSize",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the SmallestMaxSize transform with parameters to control the resizing of images to a specified maximum size using a chosen interpolation method.\n\nArgs:\n    always_apply (bool): Whether to always apply the transformation.\n    p (float): Probability of applying the transformation.\n    interpolation (int): Interpolation method to use when resizing.\n    max_size (int): The maximum size to which the image's longest side will be resized.\n\nReturns:\n    None\n\nWhy:\n    This initialization enables the transformation to systematically resize images in preprocessing, ensuring consistency in image dimensions for further computer vision processing or model training.\n\"\"\"",
                    "source_code": "super(SmallestMaxSize, self).__init__(always_apply, p)\n        self.interpolation = interpolation\n        self.max_size = max_size"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nResizes the input image so that its largest dimension does not exceed the specified maximum size, while maintaining the original aspect ratio.\n\nThis helps standardize input image dimensions for further processing or model training, ensuring efficiency and consistency across a dataset.\n\nArgs:\n    img (PIL.Image or np.ndarray): The input image to be resized.\n    interpolation (int, optional): Interpolation method used during resizing.\n\nReturns:\n    The resized image of the same type as the input.\n\"\"\"",
                    "source_code": "return F.smallest_max_size(img, max_size=self.max_size, interpolation=interpolation)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nReturns the input bounding box unchanged. This behavior ensures that the bounding box coordinates remain aligned with the original image when a transformation does not affect spatial attributes, maintaining consistency in object localization.\n\nArgs:\n    bbox (tuple): Bounding box coordinates.\n\nReturns:\n    tuple: The same bounding box coordinates, unmodified.\n\"\"\"",
                    "source_code": "return bbox"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nScales the given keypoint coordinates according to the minimum dimension of the input, ensuring that its smallest side matches a specified maximum size.\n\nArgs:\n    keypoint (tuple): The coordinates of the keypoint to be scaled.\n    params (dict): Dictionary containing image dimensions under the keys \"rows\" (height) and \"cols\" (width).\n\nReturns:\n    tuple: The scaled keypoint coordinates.\n\nWhy:\n    This method standardizes the scale of keypoints relative to the resized image, preserving their correct location after the image's smallest side is adjusted to a specific maximum size. This consistency is essential for maintaining the spatial correspondence between keypoints and images during data processing and augmentation.\n\"\"\"",
                    "source_code": "height = params[\"rows\"]\n        width = params[\"cols\"]\n\n        scale = self.max_size / min([height, width])\n        return F.keypoint_scale(keypoint, scale, scale)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the arguments required to initialize the transformation, allowing for serialization and reproducibility of the transformation pipeline.\n\nReturns:\n    tuple: A tuple containing the names of the arguments ('max_size', 'interpolation') used for initializing the transformation.\n\"\"\"",
                    "source_code": "return (\"max_size\", \"interpolation\")"
                }
            ],
            "name": "SmallestMaxSize",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Resize transformation with specified target dimensions and interpolation method.\n\nArgs:\n    height (int): Desired output height for the resized image.\n    width (int): Desired output width for the resized image.\n    interpolation (int or cv2 interpolation flag): Interpolation method used for resizing.\n    always_apply (bool, optional): If True, always apply the transformation. Defaults to False.\n    p (float, optional): Probability of applying the transformation. Defaults to 1.0.\n\nReturns:\n    None\n\nWhy:  \nThis method sets up the necessary parameters to ensure images are consistently resized, enabling uniform input dimensions for downstream processing and improving the reliability of image data handling.\n\"\"\"",
                    "source_code": "super(Resize, self).__init__(always_apply, p)\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nResize the input image to the target height and width using the specified interpolation method.\n\nArgs:\n    img: The input image to be resized.\n    interpolation: Interpolation method to use for resizing.\n\nReturns:\n    The resized image.\n    \nWhy:\n    Resizing enables unified image dimensions across a dataset, which is essential for consistent model input requirements and reliable data processing in computer vision pipelines.\n\"\"\"",
                    "source_code": "return F.resize(img, height=self.height, width=self.width, interpolation=interpolation)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nProcess and return the provided bounding box during resizing operations, ensuring compatibility and consistency within augmentation pipelines.\n\nArgs:\n    bbox (tuple or list): Bounding box coordinates to be processed.\n\nReturns:\n    tuple or list: The (possibly unchanged) bounding box coordinates.\n\"\"\"",
                    "source_code": "return bbox"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nScales the given keypoint coordinates according to the resizing parameters.\n\nThis method adjusts the location of a keypoint to reflect the scaling factors applied during image resizing, ensuring spatial correspondences between the original and transformed data are maintained.\n\nArgs:\n    keypoint: The keypoint to be transformed, typically as a tuple (x, y, ...).\n    params: A dictionary containing the original image dimensions under the keys \"rows\" and \"cols\".\n\nReturns:\n    The transformed keypoint with coordinates scaled to match the resized image dimensions.\n\"\"\"",
                    "source_code": "height = params[\"rows\"]\n        width = params[\"cols\"]\n        scale_x = self.width / width\n        scale_y = self.height / height\n        return F.keypoint_scale(keypoint, scale_x, scale_y)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required for the resize transformation. This enables serialization and reproducibility when applying resizing operations as part of an image augmentation pipeline.\n\nReturns:\n    tuple: A tuple containing the names of the arguments: \"height\", \"width\", and \"interpolation\".\n\"\"\"",
                    "source_code": "return (\"height\", \"width\", \"interpolation\")"
                }
            ],
            "name": "Resize",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": "\"\"\"\n        Args:\n            factor (int): number of times the input will be rotated by 90 degrees.\n        \"\"\"",
                    "first_doc": "\"\"\"\nRotates the input image by 90 degrees the specified number of times.\n\nArgs:\n    img: The image to be rotated.\n\nReturns:\n    The rotated image as a contiguous NumPy array.\n\"\"\"",
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies a 90-degree rotation to the input image a specified number of times to introduce geometric variation.\n\nThis operation helps simulate a wider range of orientations during image preprocessing, promoting better model robustness and generalization to unseen data.\n\nArgs:\n    img: numpy.ndarray\n        The image to be rotated. Must be a NumPy array.\n\nReturns:\n    numpy.ndarray:\n        The rotated image as a contiguous NumPy array.\n\"\"\"",
                    "source_code": "return np.ascontiguousarray(np.rot90(img, factor))"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate randomly-chosen parameters for applying a 90-degree rotation transformation to an image.\n\nThis method determines the specific rotation (0, 90, 180, or 270 degrees) to be applied, introducing variability in how images are transformed, which helps create diverse augmented data samples for training robust models.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary with the key 'factor', whose value is an integer in the range [0, 3] representing the number of 90-degree rotations to apply.\n\"\"\"",
                    "source_code": "return {\"factor\": random.randint(0, 3)}"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nRotate a bounding box by 90 degrees a specified number of times.\n\nThis method adjusts the bounding box coordinates to match the rotated image orientation, ensuring that object localization remains accurate after the image transformation. Rotating bounding boxes is essential for maintaining the consistency of annotated data during spatial augmentations.\n\nArgs:\n    bbox (tuple or list): The bounding box coordinates to be rotated.\n    factor (int): Number of times to rotate 90 degrees counterclockwise.\n    **params: Additional parameters required for the bounding box transformation.\n\nReturns:\n    tuple or list: The rotated bounding box coordinates.\n\"\"\"",
                    "source_code": "return F.bbox_rot90(bbox, factor, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nRotate a keypoint by 90 degrees a specified number of times using the same transformation parameters as applied to the image.\n\nArgs:\n    keypoint (tuple): The coordinates and angle of the keypoint to be rotated.\n    factor (int): Number of times to rotate the keypoint by 90 degrees.\n    **params: Additional parameters used for the rotation.\n\nReturns:\n    tuple: The rotated keypoint with updated coordinates and angle.\n\nWhy:\n    Ensures keypoints remain correctly aligned and valid after the corresponding image rotation, maintaining consistency and accuracy in data used for computer vision tasks.\n\"\"\"",
                    "source_code": "return F.keypoint_rot90(keypoint, factor, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments for this transformation.\n\nThis method is used to facilitate serialization and reproducibility of augmentation pipelines by providing a standardized interface for transform classes to specify which arguments are necessary to fully reconstruct the transform instance.\n\nReturns:\n    tuple: An empty tuple, indicating that this transformation does not require any initialization arguments.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "RandomRotate90",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Rotate transformation with parameters that control rotation range, pixel interpolation, handling of image borders, and fill values. These settings allow users to configure how rotation is applied to images, masks, or other associated data elements, ensuring flexibility and preserving data integrity during augmentation.\n\nArgs:\n    always_apply (bool): Whether to always apply the transform or use the given probability `p`.\n    p (float): Probability of applying the transform.\n    limit (float or tuple of float): Range from which the rotation angle will be randomly selected.\n    interpolation (int): Interpolation method used for rotating the image.\n    border_mode (int): Pixel extrapolation method for border pixels.\n    value (int, float, or sequence): Value used for filling pixels outside the input image.\n    mask_value (int, float, or sequence): Value used for filling pixels outside mask boundaries.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Rotate, self).__init__(always_apply, p)\n        self.limit = to_tuple(limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies a rotation transformation to the given image using specified parameters.\n\nThis method is used to augment input images by rotating them, which helps increase dataset variability and improve the robustness of computer vision models to orientation changes.\n\nArgs:\n    img: The input image to be rotated.\n    angle: The angle by which to rotate the image.\n    interpolation: The interpolation method to use for resampling.\n    \nReturns:\n    The rotated image, with border handling and padding values applied as defined by the object's configuration.\n\"\"\"",
                    "source_code": "return F.rotate(img, angle, interpolation, self.border_mode, self.value)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nRotate the input mask image by a specified angle using nearest-neighbor interpolation, applying predefined border and fill values.\n\nThis method ensures that the corresponding mask stays consistent with its augmented image during geometric transformations, which is crucial for maintaining accurate labels in tasks like segmentation.\n\nArgs:\n    img (numpy.ndarray): The mask image to be rotated.\n    angle (float): The angle, in degrees, to rotate the image.\n\nReturns:\n    numpy.ndarray: The rotated mask image, with borders and fill value applied as specified.\n\"\"\"",
                    "source_code": "return F.rotate(img, angle, cv2.INTER_NEAREST, self.border_mode, self.mask_value)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate random parameters for the rotation transformation to introduce variation in image augmentation.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing a randomly sampled 'angle' within the predefined rotation limits.\n    \nWhy:\n    Generating a random angle for rotation increases the diversity of augmented images, which helps improve model robustness during training.\n\"\"\"",
                    "source_code": "return {\"angle\": random.uniform(self.limit[0], self.limit[1])}"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nRotate a bounding box by a specified angle, ensuring the box remains correctly aligned with the transformed image and its spatial context.\n\nArgs:\n    bbox (sequence): Bounding box coordinates to be rotated.\n    angle (float): Rotation angle in degrees.\n    **params: Additional parameters for the rotation operation.\n\nReturns:\n    sequence: Rotated bounding box coordinates.\n\"\"\"",
                    "source_code": "return F.bbox_rotate(bbox, angle, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nRotates given keypoints by a specified angle and additional parameters, ensuring their positions accurately reflect any rotation applied to the associated image.\n\nArgs:\n    keypoint (tuple or list): The coordinates of the keypoint to be rotated.\n    angle (float): The angle by which to rotate the keypoint, in degrees.\n    **params: Additional keyword arguments required for the rotation transformation (such as the image center or scale).\n\nReturns:\n    tuple: The new coordinates of the rotated keypoint.\n\"\"\"",
                    "source_code": "return F.keypoint_rotate(keypoint, angle, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to recreate the transform's configuration. \n\nThis allows for reliable serialization, deserialization, and reproduction of augmentation pipelines, ensuring consistent behavior across different environments.\n\nReturns:\n    tuple: A tuple containing the argument names used to initialize the transform.\n\"\"\"",
                    "source_code": "return (\"limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\")"
                }
            ],
            "name": "Rotate",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the RandomScale transformation with the specified scaling limits and interpolation method, preparing the augmentation to randomly scale input images within the defined range during processing.\n\nArgs:\n    always_apply (bool): If True, the transformation will be applied to every image.\n    p (float): Probability of applying the transformation.\n    scale_limit (float or tuple of float): Range specifying the lower and upper bounds for random scaling.\n    interpolation (int): Interpolation method used when resizing the image.\n\nReturns:\n    None\n\nWhy:\n    Randomly scaling input images introduces important variations in spatial dimensions, helping models learn scale-invariant features and improving their robustness to real-world data where object sizes may vary.\n\"\"\"",
                    "source_code": "super(RandomScale, self).__init__(always_apply, p)\n        self.scale_limit = to_tuple(scale_limit, bias=1.0)\n        self.interpolation = interpolation"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerates a random scaling factor within the defined scale limits to be applied during image augmentation.  \nThis randomness helps increase the diversity of training samples, encouraging models to handle different object sizes and improving robustness.\n\nReturns:\n    dict: A dictionary containing a randomly selected 'scale' value that specifies how much to scale the input image.\n\"\"\"",
                    "source_code": "return {\"scale\": random.uniform(self.scale_limit[0], self.scale_limit[1])}"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies scaling to the input image using the specified scale factor and interpolation method.  \nScaling is performed to introduce variability in the input data, which helps improve the robustness of computer vision models during training.\n\nArgs:\n    img (numpy.ndarray): The input image to be scaled.\n    scale (float): The scaling factor to resize the image.\n    interpolation (int): Interpolation method used for resizing.\n\nReturns:\n    numpy.ndarray: The scaled image.\n\"\"\"",
                    "source_code": "return F.scale(img, scale, interpolation)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nReturns the bounding box unchanged.\n\nThis method exists to maintain consistency within transformation pipelines, ensuring that bounding boxes are processed in harmony with other elements even if a particular transformation does not alter them.\n\nArgs:\n    bbox (Any): The bounding box to process.\n\nReturns:\n    Any: The original, unmodified bounding box.\n\"\"\"",
                    "source_code": "return bbox"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nScales the input keypoint by the specified scale factors along both axes.\n\nThis method modifies keypoint coordinates to remain consistent with the scaling applied to the image, which ensures that augmented data used for training maintains proper spatial relationships.\n\nArgs:\n    keypoint (tuple): The (x, y, angle, scale, ...)-formatted keypoint to be scaled.\n    scale (float): The scale factor to apply to the keypoint coordinates.\n\nReturns:\n    tuple: The scaled keypoint with updated coordinates.\n\"\"\"",
                    "source_code": "return F.keypoint_scale(keypoint, scale, scale)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args",
                    "second_doc": "\"\"\"\nRetrieve the initial arguments used to configure the scaling transformation, ensuring that relevant parameters can be preserved for reproducibility, analysis, or serialization.\n\nArgs:\n    self: Instance of the RandomScale class containing transformation parameters.\n\nReturns:\n    dict: A dictionary with the values for \"interpolation\" and \"scale_limit\" that define the behavior of the scaling operation.\n\"\"\"",
                    "source_code": "return {\"interpolation\": self.interpolation, \"scale_limit\": to_tuple(self.scale_limit, bias=-1.0)}"
                }
            ],
            "name": "RandomScale",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the parameters required for applying shift, scale, and rotate transformations to images.\n\nArgs:\n    shift_limit (float or tuple): Range for random horizontal and vertical shifts.\n    scale_limit (float or tuple): Range for random scaling.\n    rotate_limit (float or tuple): Range for random rotations in degrees.\n    interpolation (int): Interpolation method for resizing images after transformation.\n    border_mode (int): Pixel extrapolation method for image borders.\n    value (int, float, list, or tuple): Value used for padding if border_mode is set accordingly.\n    mask_value (int, float, list, or tuple): Value used for mask padding if border_mode is set accordingly.\n    always_apply (bool): Whether to always apply the transformation.\n    p (float): Probability of applying the transformation.\n\nReturns:\n    None\n\nWhy:\n    Setting these parameters allows precise control over how geometric augmentations are applied to images and masks, ensuring consistent transformations that help create varied, realistic training samples without manual setup for each transformation type.\n\"\"\"",
                    "source_code": "super(ShiftScaleRotate, self).__init__(always_apply, p)\n        self.shift_limit = to_tuple(shift_limit)\n        self.scale_limit = to_tuple(scale_limit, bias=1.0)\n        self.rotate_limit = to_tuple(rotate_limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply a geometric transformation that shifts, scales, and rotates the input image.\n\nThis method modifies the spatial properties of the image by applying shift, scale, and rotation parameters. These transformations improve the robustness and generalization of models by exposing them to a wider variety of image perspectives and alignments during training.\n\nArgs:\n    img (numpy.ndarray): The input image to be transformed.\n    angle (float): The rotation angle in degrees.\n    scale (float): The scaling factor.\n    dx (float): Horizontal shift.\n    dy (float): Vertical shift.\n    interpolation (int): Interpolation method used for transformation.\n\nReturns:\n    numpy.ndarray: The transformed image.\n\"\"\"",
                    "source_code": "return F.shift_scale_rotate(img, angle, scale, dx, dy, interpolation, self.border_mode, self.value)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nApplies a geometric shift, scaling, and rotation transformation to the input mask using nearest-neighbor interpolation. This operation ensures that the spatial structure of the mask remains aligned with similarly transformed images, preserving annotation accuracy during data augmentation.\n\nArgs:\n    img (numpy.ndarray): The input mask image to be transformed.\n    angle (float): The rotation angle in degrees.\n    scale (float): The scaling factor.\n    dx (float): Horizontal shift.\n    dy (float): Vertical shift.\n\nReturns:\n    numpy.ndarray: The transformed mask image.\n\"\"\"",
                    "source_code": "return F.shift_scale_rotate(img, angle, scale, dx, dy, cv2.INTER_NEAREST, self.border_mode, self.mask_value)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nApply shift, scale, and rotation transformations to a keypoint coordinate to ensure spatial consistency between augmented images and their associated keypoints.\n\nArgs:\n    keypoint (tuple): The (x, y, angle, scale) representation of the keypoint to be transformed.\n    angle (float): The rotation angle in degrees to apply.\n    scale (float): The scaling factor to apply.\n    dx (float): The horizontal shift.\n    dy (float): The vertical shift.\n    rows (int): The number of rows in the image (image height).\n    cols (int): The number of columns in the image (image width).\n\nReturns:\n    tuple: The transformed keypoint coordinates, adjusted with the specified affine parameters.\n\"\"\"",
                    "source_code": "return F.keypoint_shift_scale_rotate(keypoint, angle, scale, dx, dy, rows, cols)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerates random parameters for angle, scale, and translation shifts to be used in affine transformation operations. \n\nThis method samples values within the specified limits for rotation, scaling, and shifting on both axes. By introducing controlled randomness to these transformation parameters, it helps in creating varied and realistic modifications of images, which promotes model robustness during training.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing randomly selected values for \"angle\", \"scale\", \"dx\", and \"dy\" to parameterize the transformation.\n\"\"\"",
                    "source_code": "return {\n            \"angle\": random.uniform(self.rotate_limit[0], self.rotate_limit[1]),\n            \"scale\": random.uniform(self.scale_limit[0], self.scale_limit[1]),\n            \"dx\": random.uniform(self.shift_limit[0], self.shift_limit[1]),\n            \"dy\": random.uniform(self.shift_limit[0], self.shift_limit[1]),\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nApplies a combined shift, scale, and rotation transformation to a bounding box, modifying its position and shape to match a corresponding geometric transformation applied to the associated image. This ensures that the bounding box annotations remain consistent after data augmentation, which is crucial for training object detection models.\n\nArgs:\n    bbox (tuple): The coordinates of the bounding box to be transformed.\n    angle (float): Rotation angle in degrees.\n    scale (float): Scaling factor.\n    dx (float): Horizontal shift factor.\n    dy (float): Vertical shift factor.\n    interpolation (int, optional): Interpolation method used for transformation. Defaults to cv2.INTER_LINEAR.\n    **params: Additional parameters for the transformation.\n\nReturns:\n    tuple: The transformed bounding box coordinates.\n\"\"\"",
                    "source_code": "return F.bbox_shift_scale_rotate(bbox, angle, scale, dx, dy, interpolation=cv2.INTER_LINEAR, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args",
                    "second_doc": "\"\"\"\nCollects and returns the initialization parameters required for configuring the shift, scale, and rotation transformations. \n\nThese parameters are needed to ensure that each transformation accurately applies controlled modifications to images, maintaining reproducibility and consistency during augmentation.\n\nReturns:\n    dict: A dictionary containing the shift limit, scale limit (as a tuple), rotation limit, interpolation method, border mode, fill values for empty areas, and mask values used during transformation.\n\"\"\"",
                    "source_code": "return {\n            \"shift_limit\": self.shift_limit,\n            \"scale_limit\": to_tuple(self.scale_limit, bias=-1.0),\n            \"rotate_limit\": self.rotate_limit,\n            \"interpolation\": self.interpolation,\n            \"border_mode\": self.border_mode,\n            \"value\": self.value,\n            \"mask_value\": self.mask_value,\n        }"
                }
            ],
            "name": "ShiftScaleRotate",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitialize the CenterCrop transformation with the specified height and width, configuring how the cropping operation will behave on input images.\n\nArgs:\n    height (int): The height of the crop box.\n    width (int): The width of the crop box.\n    always_apply (bool, optional): If True, the transformation will be applied to every image. Defaults to False.\n    p (float, optional): Probability of applying the transformation. Defaults to 1.0.\n\nReturns:\n    None\n\nWhy:\n    This method sets up the CenterCrop operation to ensure that subsequent image transformations consistently extract a centrally located region of the specified size. This focuses training and evaluation on a region of interest, improves input consistency, and helps maintain relevant spatial features across different images.\n\"\"\"",
                    "source_code": "super(CenterCrop, self).__init__(always_apply, p)\n        self.height = height\n        self.width = width"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nCrops the input image at the center to the specified height and width.\n\nThis method performs a central crop to focus on the most relevant area of the image, which helps standardize input dimensions for further processing.\n\nArgs:\n    img (numpy.ndarray or PIL.Image): Input image to be cropped.\n\nReturns:\n    numpy.ndarray or PIL.Image: Centrally cropped image with shape (height, width).\n\"\"\"",
                    "source_code": "return F.center_crop(img, self.height, self.width)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nApply a center crop transformation to a bounding box, adjusting its coordinates to fit within a region of the specified height and width.\n\nThis method ensures the bounding box remains consistent with the corresponding image crop, maintaining geometric alignment after cropping.\n\nArgs:\n    bbox (tuple): A bounding box specified as a tuple in the required format (e.g., (x_min, y_min, x_max, y_max)).\n    params (dict): Additional keyword arguments for customizing the cropping behavior.\n\nReturns:\n    tuple: The resulting bounding box, transformed to fit within the centered crop region.\n\"\"\"",
                    "source_code": "return F.bbox_center_crop(bbox, self.height, self.width, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nApplies a center crop transformation to the input keypoints.\n\nThis method ensures that keypoints are properly adjusted when the corresponding image patch is center-cropped, maintaining their spatial consistency within the new cropped region. This is crucial for tasks that rely on accurate keypoint locations, such as pose estimation.\n\nArgs:\n    keypoint (Any): The keypoint or set of keypoints to be transformed.\n    **params: Additional parameters controlling the cropping behavior.\n\nReturns:\n    Any: The keypoint(s) transformed to be consistent with the center-cropped region.\n\"\"\"",
                    "source_code": "return F.keypoint_center_crop(keypoint, self.height, self.width, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to reconstruct the transformation.\n\nReturns:\n    tuple: A tuple containing the names of the required arguments (\"height\", \"width\").\n\"\"\"",
                    "source_code": "return (\"height\", \"width\")"
                }
            ],
            "name": "CenterCrop",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a RandomCrop augmentation with specified crop size and application parameters.\n\nThis method sets up the necessary attributes to perform random cropping on images, which introduces variability and helps the model become robust to the spatial positioning of features.\n\nArgs:\n    height (int): Target height of the cropped image.\n    width (int): Target width of the cropped image.\n    always_apply (bool): Whether to always apply this augmentation.\n    p (float): Probability of applying this transformation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(RandomCrop, self).__init__(always_apply, p)\n        self.height = height\n        self.width = width"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nRandomly crops the input image to the specified height and width starting from the provided coordinates.\n\nArgs:\n    img (np.ndarray): The input image to be cropped.\n    h_start (int): The starting height coordinate for the crop.\n    w_start (int): The starting width coordinate for the crop.\n\nReturns:\n    np.ndarray: The cropped portion of the image.\n\nWhy:\n    Cropping images at random locations introduces variability in the visual content exposed to machine learning models during training, which helps to reduce overfitting and encourages the models to generalize better to unseen data.\n\"\"\"",
                    "source_code": "return F.random_crop(img, self.height, self.width, h_start, w_start)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate random starting coordinates for height and width to define the top-left corner of a crop.\n\nThis method introduces variability in crop locations to ensure diverse and unbiased sampling of image regions during augmentation.\n\nReturns:\n    dict: A dictionary containing randomly selected starting points:\n        - \"h_start\": float, random value in [0, 1) for vertical offset.\n        - \"w_start\": float, random value in [0, 1) for horizontal offset.\n\"\"\"",
                    "source_code": "return {\"h_start\": random.random(), \"w_start\": random.random()}"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nRandomly crops the given set of bounding boxes to a new size defined by the specified height and width, using the provided parameters for randomness and placement.\n\nThis method adjusts bounding boxes to align with a randomly selected crop region, ensuring that bounding boxes correctly reflect the new spatial context after cropping. This increases variability in the training data and helps models become less sensitive to object positions.\n\nArgs:\n    bbox (Any): The input bounding boxes to be cropped.\n    params (dict): Additional randomization parameters controlling the placement and shape of the crop.\n\nReturns:\n    Any: Bounding boxes that have been transformed to fit the new cropped image region.\n\"\"\"",
                    "source_code": "return F.bbox_random_crop(bbox, self.height, self.width, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nApply a random crop transformation to a keypoint by cropping its coordinates according to specified height and width, ensuring alignment with randomly-cropped image regions.\n\nArgs:\n    keypoint: The input keypoint data to be cropped.\n    params: Additional parameters dictating how the crop is applied (e.g., cropping offsets).\n\nReturns:\n    The transformed keypoint with coordinates adjusted to match a randomly-cropped region of the image.\n\nWhy:\n    This method ensures that keypoints remain accurately positioned and meaningful after random crops are applied to images, preserving the integrity of spatial annotations during augmentation workflows.\n\"\"\"",
                    "source_code": "return F.keypoint_random_crop(keypoint, self.height, self.width, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments necessary for applying the random cropping transformation, which helps ensure that augmentation pipelines can correctly record and reproduce the cropping parameters.\n\nReturns:\n    tuple: A tuple containing the names of the required arguments ('height', 'width') for initializing the transformation.\n\"\"\"",
                    "source_code": "return (\"height\", \"width\")"
                }
            ],
            "name": "RandomCrop",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the RandomCropNearBBox transformation with specified parameters, setting up its probability of application and the maximum allowed shift for bounding box parts during cropping.\n\nArgs:\n    always_apply (bool): If True, the transformation is always applied.\n    p (float): Probability of applying the transformation.\n    max_part_shift (float): Maximum shift allowed for bounding box parts during cropping.\n\nReturns:\n    None\n\nWhy:\n    This setup prepares the transformation to perform targeted random cropping near bounding boxes, which helps create challenging training examples by varying object localization, thereby enhancing model robustness to bounding box shifts.\n\"\"\"",
                    "source_code": "super(RandomCropNearBBox, self).__init__(always_apply, p)\n        self.max_part_shift = max_part_shift"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nCrop the input image to the specified bounding box coordinates, ensuring coordinates remain within image boundaries.\n\nThis method is used to efficiently obtain a subregion of the image based on given coordinates while guaranteeing that the crop does not exceed image dimensions. This helps maintain data integrity during preprocessing steps.\n\nArgs:\n    img: Input image to be cropped.\n    x_min (int): Minimum x-coordinate of the crop box.\n    y_min (int): Minimum y-coordinate of the crop box.\n    x_max (int): Maximum x-coordinate of the crop box.\n    y_max (int): Maximum y-coordinate of the crop box.\n\nReturns:\n    Cropped image corresponding to the specified region, clamped to stay within the original image's bounds.\n\"\"\"",
                    "source_code": "return F.clamping_crop(img, x_min, y_min, x_max, y_max)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerate randomized cropping parameters based on the position and size of a given bounding box, introducing variability while retaining relevance to the original object of interest.\n\nThis method calculates possible shifts for the bounding box coordinates, then randomly perturbs the cropping area boundaries. This helps to create more diverse training samples by increasing the unpredictability of the crop positions relative to target objects.\n\nArgs:\n    params (dict): Dictionary containing a 'cropping_bbox' key with the bounding box coordinates [x_min, y_min, x_max, y_max].\n\nReturns:\n    dict: Contains updated cropping coordinates with keys 'x_min', 'x_max', 'y_min', 'y_max' after applying random shifts.\n\"\"\"",
                    "source_code": "bbox = params[\"cropping_bbox\"]\n        h_max_shift = int((bbox[3] - bbox[1]) * self.max_part_shift)\n        w_max_shift = int((bbox[2] - bbox[0]) * self.max_part_shift)\n\n        x_min = bbox[0] - random.randint(-w_max_shift, w_max_shift)\n        x_max = bbox[2] + random.randint(-w_max_shift, w_max_shift)\n\n        y_min = bbox[1] - random.randint(-h_max_shift, h_max_shift)\n        y_max = bbox[3] + random.randint(-h_max_shift, h_max_shift)\n\n        return {\"x_min\": x_min, \"x_max\": x_max, \"y_min\": y_min, \"y_max\": y_max}"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nCrop a bounding box to a specified region defined by start coordinates and size relative to the initial bounding box.\n\nThis method adjusts bounding boxes to stay within a desired cropped area, ensuring that annotations remain accurate after image transformations.\n\nArgs:\n    bbox (Any): The original bounding box to be cropped.\n    y_min (int): The starting vertical coordinate of the crop.\n    y_max (int): The ending vertical coordinate of the crop.\n    x_min (int): The starting horizontal coordinate of the crop.\n    x_max (int): The ending horizontal coordinate of the crop.\n    params (dict): Additional parameters for the cropping function.\n\nReturns:\n    Any: The cropped bounding box adjusted to the specified region.\n\"\"\"",
                    "source_code": "h_start = y_min\n        w_start = x_min\n        return F.bbox_crop(bbox, y_max - y_min, x_max - x_min, h_start, w_start, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nAdjusts the coordinates of a keypoint to reflect its new location after a random cropping operation near a bounding box.\n\nThis method recalculates the position of a keypoint when a region of the image is cropped, ensuring the keypoint's location remains accurate within the transformed image. It is essential for maintaining the correctness of keypoint annotations after spatial transformations.\n\nArgs:\n    keypoint (tuple): The (x, y, ...) coordinates representing the keypoint to be transformed.\n    x_min (int): The minimum x-coordinate of the cropping rectangle.\n    y_min (int): The minimum y-coordinate of the cropping rectangle.\n    x_max (int): The maximum x-coordinate of the cropping rectangle.\n    y_max (int): The maximum y-coordinate of the cropping rectangle.\n    params (dict): Dictionary containing image dimensions, expects keys \"rows\" and \"cols\".\n\nReturns:\n    tuple: The adjusted coordinates of the keypoint after cropping.\n\"\"\"",
                    "source_code": "return F.crop_keypoint_by_coords(\n            keypoint,\n            crop_coords=(x_min, y_min, x_max, y_max),\n            crop_height=y_max - y_min,\n            crop_width=x_max - x_min,\n            rows=params[\"rows\"],\n            cols=params[\"cols\"],\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies the list of target parameters that are relevant for processing by this transformation.\n\nReturns:\n    list: A list containing the names of parameters expected by the transformation to define the cropping region.\n\nWhy:\n    This enables the transformation to properly identify and utilize the bounding box information necessary for performing cropping operations near specific regions of interest within the image.\n\"\"\"",
                    "source_code": "return [\"cropping_bbox\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to serialize and reproduce this transformation. \n\nThis enables consistent reconstruction of augmentation parameters, ensuring augmentations can be reproduced or re-applied identically during processes such as model training or evaluation.\n\nReturns:\n    tuple: A tuple containing the names of the arguments used to initialize the transformation (e.g., (\"max_part_shift\",)).\n\"\"\"",
                    "source_code": "return (\"max_part_shift\",)"
                }
            ],
            "name": "RandomCropNearBBox",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the base configuration for randomly cropping an image to a specified size using a particular interpolation method. This setup enables image data to be augmented by extracting random subregions, which helps introduce variety and improve model robustness.\n\nArgs:\n    always_apply (bool): If True, the transformation is always applied.\n    p (float): Probability of applying the transformation.\n    height (int): Height of the crop.\n    width (int): Width of the crop.\n    interpolation (int): Interpolation method to use for resizing.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(_BaseRandomSizedCrop, self).__init__(always_apply, p)\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nRandomly crops an image to the specified size and then resizes it back to a predetermined output dimension using the chosen interpolation method. This process generates varied views of the original image, which is useful for introducing diversity and preventing overfitting during model training.\n\nArgs:\n    img (numpy.ndarray): Input image to be cropped and resized.\n    crop_height (int): Height of the crop.\n    crop_width (int): Width of the crop.\n    h_start (int): Vertical starting coordinate for the crop.\n    w_start (int): Horizontal starting coordinate for the crop.\n    interpolation (int): Interpolation method for resizing the cropped image.\n\nReturns:\n    numpy.ndarray: The cropped and resized image.\n\"\"\"",
                    "source_code": "crop = F.random_crop(img, crop_height, crop_width, h_start, w_start)\n        return F.resize(crop, self.height, self.width, interpolation)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nRandomly crops the input bounding boxes according to the specified crop dimensions and starting coordinates, ensuring that the transformed bounding boxes correspond to the cropped region of the image.\n\nArgs:\n    bbox: The bounding box or list of bounding boxes to be cropped.\n    crop_height: The height of the crop.\n    crop_width: The width of the crop.\n    h_start: The vertical start coordinate for the crop (top-left corner).\n    w_start: The horizontal start coordinate for the crop (top-left corner).\n    rows: Total number of rows (image height).\n    cols: Total number of columns (image width).\n\nReturns:\n    The updated bounding boxes that fit within the cropped area.\n\nWhy:\n    This method adjusts bounding boxes to remain correctly aligned with randomly cropped image regions, which is necessary for maintaining accurate object detection annotations after data augmentation transformations.\n\"\"\"",
                    "source_code": "return F.bbox_random_crop(bbox, crop_height, crop_width, h_start, w_start, rows, cols)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nApplies a random crop to the given keypoint and rescales it to match the target output dimensions.\n\nThis method ensures that keypoints remain correctly positioned and proportioned after a random crop is performed, maintaining the spatial relationship between augmented images and their associated keypoints.\n\nArgs:\n    keypoint: The keypoint to be transformed.\n    crop_height (int): Height of the crop region.\n    crop_width (int): Width of the crop region.\n    h_start (int): Vertical starting position for the crop.\n    w_start (int): Horizontal starting position for the crop.\n    rows (int): Number of rows in the image.\n    cols (int): Number of columns in the image.\n\nReturns:\n    Transformed keypoint with updated coordinates reflecting the crop and rescaling.\n\"\"\"",
                    "source_code": "keypoint = F.keypoint_random_crop(keypoint, crop_height, crop_width, h_start, w_start, rows, cols)\n        scale_x = self.width / crop_width\n        scale_y = self.height / crop_height\n        keypoint = F.keypoint_scale(keypoint, scale_x, scale_y)\n        return keypoint"
                }
            ],
            "name": "_BaseRandomSizedCrop",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the RandomSizedCrop transformation that randomly crops a region from an input image based on specified height, width, and aspect ratio constraints, ensuring randomness and variability in the augmented data.\n\nArgs:\n    height (int): Desired height of the output image after cropping.\n    width (int): Desired width of the output image after cropping.\n    min_max_height (Tuple[int, int]): Minimum and maximum heights to sample from when generating the crop.\n    w2h_ratio (float): Width-to-height aspect ratio range for the crop.\n    interpolation (int, optional): Interpolation method used for resizing the crop. Defaults to a pre-set value.\n    always_apply (bool, optional): If True, applies the transformation to every image. Defaults to pre-set value.\n    p (float, optional): Probability of applying the transformation. Defaults to pre-set value.\n\nReturns:\n    None\n\nWhy:\n    Random cropping with size and aspect ratio constraints increases data diversity during augmentation, helping prevent overfitting and simulating a variety of object scales and spatial arrangements.\n\"\"\"",
                    "source_code": "super(RandomSizedCrop, self).__init__(\n            height=height, width=width, interpolation=interpolation, always_apply=always_apply, p=p\n        )\n        self.min_max_height = min_max_height\n        self.w2h_ratio = w2h_ratio"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate randomized parameters for cropping an image with variable dimensions, ensuring variability and adaptability in image preprocessing routines.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing the randomly selected starting coordinates for height and width ('h_start', 'w_start'), and the randomly determined crop size ('crop_height', 'crop_width').\n    \nWhy:\n    By randomizing crop position and size, this method introduces variability in training data, which helps reduce model overfitting and improves robustness to spatial transformations.\n\"\"\"",
                    "source_code": "crop_height = random.randint(self.min_max_height[0], self.min_max_height[1])\n        return {\n            \"h_start\": random.random(),\n            \"w_start\": random.random(),\n            \"crop_height\": crop_height,\n            \"crop_width\": int(crop_height * self.w2h_ratio),\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to configure this cropping transformation. This enables consistent serialization and reproducibility of the transformation setup.\n\nReturns:\n    tuple: A tuple containing the names of the arguments used to initialize the random sized crop operation: (\"min_max_height\", \"height\", \"width\", \"w2h_ratio\", \"interpolation\").\n\"\"\"",
                    "source_code": "return \"min_max_height\", \"height\", \"width\", \"w2h_ratio\", \"interpolation\""
                }
            ],
            "name": "RandomSizedCrop",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the RandomResizedCrop transformation with specified output dimensions and configuration for random cropping.\n\nThis method sets up the parameters controlling the range of scaling and aspect ratios for the random crop, ensuring input images are augmented through varied cropping patterns to improve model robustness.\n\nArgs:\n    height (int): The desired height of the output image after cropping and resizing.\n    width (int): The desired width of the output image after cropping and resizing.\n    interpolation (optional): Interpolation method to be used for resizing. Defaults to a preset value if not specified.\n    always_apply (bool, optional): If set to True, the transformation will always be applied. Defaults to False.\n    p (float, optional): Probability of applying the transformation. Defaults to 1.0.\n    scale (tuple of float): Lower and upper bounds for the random area of the crop, relative to the original image area.\n    ratio (tuple of float): Lower and upper bounds for the aspect ratio of the crop.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(RandomResizedCrop, self).__init__(\n            height=height, width=width, interpolation=interpolation, always_apply=always_apply, p=p\n        )\n        self.scale = scale\n        self.ratio = ratio"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerate parameters for performing a random crop on an image, while respecting target area coverage and aspect ratio constraints. This method attempts to select a crop region that is valid and satisfies scale and ratio requirements; if unsuccessful, it defaults to a central crop as a fallback.\n\nArgs:\n    params (dict): Dictionary containing the image under the \"image\" key as a numpy array.\n\nReturns:\n    dict: Parameters specifying the crop region, including:\n        - 'crop_height' (int): Height of the crop.\n        - 'crop_width' (int): Width of the crop.\n        - 'h_start' (float): Normalized vertical start position of the crop.\n        - 'w_start' (float): Normalized horizontal start position of the crop.\n\nWhy:\n    Randomly sampling crop sizes and locations in this way introduces meaningful variations in the training data, improving model robustness to changes in scale and spatial arrangement within images.\n\"\"\"",
                    "source_code": "img = params[\"image\"]\n        area = img.shape[0] * img.shape[1]\n\n        for _attempt in range(10):\n            target_area = random.uniform(*self.scale) * area\n            log_ratio = (math.log(self.ratio[0]), math.log(self.ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if 0 < w <= img.shape[1] and 0 < h <= img.shape[0]:\n                i = random.randint(0, img.shape[0] - h)\n                j = random.randint(0, img.shape[1] - w)\n                return {\n                    \"crop_height\": h,\n                    \"crop_width\": w,\n                    \"h_start\": i * 1.0 / (img.shape[0] - h + 1e-10),\n                    \"w_start\": j * 1.0 / (img.shape[1] - w + 1e-10),\n                }\n\n        # Fallback to central crop\n        in_ratio = img.shape[1] / img.shape[0]\n        if in_ratio < min(self.ratio):\n            w = img.shape[1]\n            h = int(round(w / min(self.ratio)))\n        elif in_ratio > max(self.ratio):\n            h = img.shape[0]\n            w = int(round(h * max(self.ratio)))\n        else:  # whole image\n            w = img.shape[1]\n            h = img.shape[0]\n        i = (img.shape[0] - h) // 2\n        j = (img.shape[1] - w) // 2\n        return {\n            \"crop_height\": h,\n            \"crop_width\": w,\n            \"h_start\": i * 1.0 / (img.shape[0] - h + 1e-10),\n            \"w_start\": j * 1.0 / (img.shape[1] - w + 1e-10),\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nRetrieve the parameters used by this transformation.\n\nReturns an empty dictionary, indicating that this specific transform currently does not require or use any additional parameters. This approach facilitates consistency in the transform interface and enables easier integration and extension within augmentation pipelines.\n\nReturns:\n    dict: An empty dictionary, as no parameters are needed for this transform.\n\"\"\"",
                    "source_code": "return {}"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which targets are treated as parameters for the transformation.\n\nReturns:\n    list: A list containing the identifiers of targets that the transformation operates on.\n    \nWhy: This facilitates the application of transformations specifically to images within augmentation pipelines, enabling consistent handling and customization of affected data items during processing.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of initialization arguments required to configure the random resized crop transformation.\n\nThis method provides the argument names needed to correctly reconstruct or serialize the transformation instance. Ensuring all necessary parameters are identified allows for consistent application and reproducibility of the transformation in different augmentation pipelines.\n\nReturns:\n    Tuple[str, ...]: A tuple containing the names of the required initialization arguments: \"height\", \"width\", \"scale\", \"ratio\", \"interpolation\".\n\"\"\"",
                    "source_code": "return \"height\", \"width\", \"scale\", \"ratio\", \"interpolation\""
                }
            ],
            "name": "RandomResizedCrop",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an instance to perform random cropping of an image to a specified size while ensuring that the bounding boxes remain valid. This helps guarantee that important annotated objects are still usable after augmentations that involve cropping.\n\nArgs:\n    height (int): Desired crop height.\n    width (int): Desired crop width.\n    interpolation (int or str, optional): Interpolation method to use during resizing after cropping.\n    erosion_rate (float, optional): How much to erode bounding boxes to minimize the risk of partially cropped objects.\n    always_apply (bool, optional): Whether to always apply this transform.\n    p (float, optional): Probability of applying the transform.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(RandomSizedBBoxSafeCrop, self).__init__(always_apply, p)\n        self.height = height\n        self.width = width\n        self.interpolation = interpolation\n        self.erosion_rate = erosion_rate"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies a random crop of specified size to the input image at given coordinates, then resizes the cropped region to target dimensions using the provided interpolation method.\n\nThis method increases the diversity of image samples by extracting variable crops and standardizing their dimensions, which helps models generalize better to different spatial configurations.\n\nArgs:\n    img (numpy.ndarray): The input image to be cropped and resized.\n    crop_height (int): Height of the crop to be extracted.\n    crop_width (int): Width of the crop to be extracted.\n    h_start (int): Vertical coordinate to start the crop.\n    w_start (int): Horizontal coordinate to start the crop.\n    interpolation (int): Interpolation method to use for resizing.\n\nReturns:\n    numpy.ndarray: The resulting image after cropping and resizing.\n\"\"\"",
                    "source_code": "crop = F.random_crop(img, crop_height, crop_width, h_start, w_start)\n        return F.resize(crop, self.height, self.width, interpolation)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nCalculate crop parameters that adapt to the spatial distribution of objects in the image, promoting crops that either fully cover relevant bounding boxes or, if none are present, vary crop size to increase data diversity.\n\nArgs:\n    params (dict): Dictionary containing \"image\" as a NumPy array and \"bboxes\" as a list of bounding boxes.\n\nReturns:\n    dict: Dictionary with keys \"h_start\", \"w_start\", \"crop_height\", and \"crop_width\" specifying the crop position and size.\n\"\"\"",
                    "source_code": "img_h, img_w = params[\"image\"].shape[:2]\n        if len(params[\"bboxes\"]) == 0:  # less likely, this class is for use with bboxes.\n            erosive_h = int(img_h * (1.0 - self.erosion_rate))\n            crop_height = img_h if erosive_h >= img_h else random.randint(erosive_h, img_h)\n            return {\n                \"h_start\": random.random(),\n                \"w_start\": random.random(),\n                \"crop_height\": crop_height,\n                \"crop_width\": int(crop_height * img_w / img_h),\n            }\n        # get union of all bboxes\n        x, y, x2, y2 = union_of_bboxes(\n            width=img_w, height=img_h, bboxes=params[\"bboxes\"], erosion_rate=self.erosion_rate\n        )\n        # find bigger region\n        bx, by = x * random.random(), y * random.random()\n        bx2, by2 = x2 + (1 - x2) * random.random(), y2 + (1 - y2) * random.random()\n        bw, bh = bx2 - bx, by2 - by\n        crop_height, crop_width = int(img_h * bh), int(img_w * bw)\n        h_start = np.clip(0.0 if bh >= 1.0 else by / (1.0 - bh), 0.0, 1.0)\n        w_start = np.clip(0.0 if bw >= 1.0 else bx / (1.0 - bw), 0.0, 1.0)\n        return {\"h_start\": h_start, \"w_start\": w_start, \"crop_height\": crop_height, \"crop_width\": crop_width}"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nRandomly crops the given bounding box within specified dimensions and image boundaries. This method ensures that the resulting cropped area remains valid and safe for further image processing tasks, such as data augmentation, by maintaining the integrity of object localization.\n\nArgs:\n    bbox (tuple): The bounding box to crop, typically defined by coordinates (x_min, y_min, x_max, y_max).\n    crop_height (int): The height of the crop area.\n    crop_width (int): The width of the crop area.\n    h_start (float): Relative vertical starting position for the crop (0.0 - 1.0).\n    w_start (float): Relative horizontal starting position for the crop (0.0 - 1.0).\n    rows (int): The total number of rows (image height).\n    cols (int): The total number of columns (image width).\n\nReturns:\n    tuple: The coordinates of the cropped bounding box, adjusted to fit within the specified crop region.\n\"\"\"",
                    "source_code": "return F.bbox_random_crop(bbox, crop_height, crop_width, h_start, w_start, rows, cols)"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which input parameters the transformation expects to process.\n\nReturns:\n    list: A list containing the names of the parameters ('image' and 'bboxes') that the crop transformation will operate on. This ensures the crop is correctly applied to both the input image and its associated bounding boxes.\n\"\"\"",
                    "source_code": "return [\"image\", \"bboxes\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the arguments required to initialize the transformation, ensuring that all essential parameters are tracked for reproducibility and serialization.\n\nReturns:\n    tuple: Names of the initialization arguments ('height', 'width', 'erosion_rate', 'interpolation').\n\"\"\"",
                    "source_code": "return (\"height\", \"width\", \"erosion_rate\", \"interpolation\")"
                }
            ],
            "name": "RandomSizedBBoxSafeCrop",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the transformation with specified output dimensions and optional mask value or channel exclusions.\n\nArgs:\n    height (int): Desired crop height.\n    width (int): Desired crop width.\n    ignore_values (list, optional): Mask values to ignore during cropping. Defaults to None.\n    ignore_channels (list, optional): Mask channels to ignore during cropping. Defaults to None.\n    always_apply (bool, optional): Whether to always apply the transform. Defaults to False.\n    p (float, optional): Probability of applying the transform. Defaults to 1.0.\n\nRaises:\n    ValueError: If ignore_values or ignore_channels are not lists.\n\nReturns:\n    None\n\nWhy:\n    Ensures the cropping transformation is configured correctly so that relevant content is preserved while allowing for flexibility in excluding specified mask values or channels from consideration. This helps users manage the handling of masks according to their specific dataset needs.\n\"\"\"",
                    "source_code": "super(CropNonEmptyMaskIfExists, self).__init__(always_apply, p)\n\n        if ignore_values is not None and not isinstance(ignore_values, list):\n            raise ValueError(\"Expected `ignore_values` of type `list`, got `{}`\".format(type(ignore_values)))\n        if ignore_channels is not None and not isinstance(ignore_channels, list):\n            raise ValueError(\"Expected `ignore_channels` of type `list`, got `{}`\".format(type(ignore_channels)))\n\n        self.height = height\n        self.width = width\n        self.ignore_values = ignore_values\n        self.ignore_channels = ignore_channels"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nCrop the input image to the specified bounding box coordinates.\n\nThis method isolates a specific region of interest within the image, which is useful for focusing processing or model attention on areas containing relevant content.\n\nArgs:\n    img: The input image to be cropped.\n    x_min: The left boundary of the cropping box.\n    y_min: The top boundary of the cropping box.\n    x_max: The right boundary of the cropping box.\n    y_max: The bottom boundary of the cropping box.\n\nReturns:\n    The cropped image corresponding to the specified coordinates.\n\"\"\"",
                    "source_code": "return F.crop(img, x_min, y_min, x_max, y_max)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nCrop a bounding box to fit within specified coordinates while preserving its position relative to the cropped image.\n\nArgs:\n    bbox (tuple): The bounding box to be cropped, typically in the format (x_min, y_min, x_max, y_max).\n    x_min (int): Minimum x-coordinate for the crop.\n    x_max (int): Maximum x-coordinate for the crop.\n    y_min (int): Minimum y-coordinate for the crop.\n    y_max (int): Maximum y-coordinate for the crop.\n    params (dict): Additional parameters containing 'rows' and 'cols', representing the dimensions of the cropped image.\n\nReturns:\n    tuple: The adjusted bounding box after cropping.\n\nWhy:\n    This method ensures that the spatial alignment between bounding boxes and their corresponding cropped images is maintained, which is essential for generating consistent annotations after image transformations and augmentations.\n\"\"\"",
                    "source_code": "return F.bbox_crop(\n            bbox, x_min=x_min, x_max=x_max, y_min=y_min, y_max=y_max, rows=params[\"rows\"], cols=params[\"cols\"]\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nAdjusts keypoint coordinates to correspond with the new region defined by cropping, ensuring keypoints accurately reflect transformations applied to the image.\n\nArgs:\n    keypoint (tuple or list): The original keypoint coordinates to be transformed.\n\nReturns:\n    tuple: The keypoint coordinates adjusted to the cropped image area.\n\"\"\"",
                    "source_code": "return F.crop_keypoint_by_coords(\n            keypoint,\n            crop_coords=[x_min, y_min, x_max, y_max],\n            crop_height=y_max - y_min,\n            crop_width=x_max - x_min,\n            rows=params[\"rows\"],\n            cols=params[\"cols\"],\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nLists the target entities that are treated as parameters by this transformation.\n\nReturns:\n    List[str]: A list containing the names of targets that the transformation operates on, ensuring correct handling and application of the cropping operation.\n\"\"\"",
                    "source_code": "return [\"mask\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nCalculate cropping coordinates such that the crop region covers at least part of a target object in the mask, if present, while respecting ignore values and channels.\n\nThis method determines the coordinates for cropping an image based on the corresponding mask, ensuring that the crop region either includes non-empty mask areas or, if there are none, is chosen randomly. Adjustments are made to mask values and channels based on user-defined ignore settings. This approach helps to focus training on relevant areas within images during preprocessing.\n\nArgs:\n    params (dict): Dictionary containing the mask to be used for determining crop placement. The mask should be a NumPy array under the key \"mask\".\n\nReturns:\n    dict: A dictionary with crop coordinates: {\"x_min\", \"x_max\", \"y_min\", \"y_max\"}.\n\"\"\"",
                    "source_code": "mask = params[\"mask\"]\n        mask_height, mask_width = mask.shape[:2]\n\n        if self.ignore_values is not None:\n            ignore_values_np = np.array(self.ignore_values)\n            mask = np.where(np.isin(mask, ignore_values_np), 0, mask)\n\n        if mask.ndim == 3 and self.ignore_channels is not None:\n            target_channels = np.array([ch for ch in range(mask.shape[-1]) if ch not in self.ignore_channels])\n            mask = np.take(mask, target_channels, axis=-1)\n\n        if self.height > mask_height or self.width > mask_width:\n            raise ValueError(\n                \"Crop size ({},{}) is larger than image ({},{})\".format(\n                    self.height, self.width, mask_height, mask_width\n                )\n            )\n\n        if mask.sum() == 0:\n            x_min = random.randint(0, mask_width - self.width)\n            y_min = random.randint(0, mask_height - self.height)\n        else:\n            mask = mask.sum(axis=-1) if mask.ndim == 3 else mask\n            non_zero_yx = np.argwhere(mask)\n            y, x = random.choice(non_zero_yx)\n            x_min = x - random.randint(0, self.width - 1)\n            y_min = y - random.randint(0, self.height - 1)\n            x_min = np.clip(x_min, 0, mask_width - self.width)\n            y_min = np.clip(y_min, 0, mask_height - self.height)\n\n        x_max = x_min + self.width\n        y_max = y_min + self.height\n\n        return {\"x_min\": x_min, \"x_max\": x_max, \"y_min\": y_min, \"y_max\": y_max}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization parameters required to configure the transformation.\n\nReturns:\n    tuple: A tuple containing the names of the parameters ('height', 'width', 'ignore_values', 'ignore_channels') used to set up the transformation instance.\n\nWhy:\n    Listing these parameter names allows for consistent serialization, reproducibility, and integration with tools or frameworks that need to inspect or initialize transformation objects with specific arguments.\n\"\"\"",
                    "source_code": "return (\"height\", \"width\", \"ignore_values\", \"ignore_channels\")"
                }
            ],
            "name": "CropNonEmptyMaskIfExists",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the OpticalDistortion transformation with specified distortion and shift limits, as well as interpolation, border, and fill value settings.\n\nArgs:\n    shift_limit (float or tuple of float): Range for random image shift during distortion.\n    distort_limit (float or tuple of float): Range for magnitude of optical distortion.\n    interpolation (int): Interpolation method used for image transformation.\n    border_mode (int): Pixel extrapolation method for border pixels.\n    value (int, float, list of int/float): Value used for filling pixels outside the input image.\n    mask_value (int, float, list of int/float): Value used for filling mask pixels outside the input image.\n    always_apply (bool): Whether to always apply the transformation.\n    p (float): Probability of applying the transformation.\n\nReturns:\n    None\n\nWhy:\n    This method sets up the necessary parameters for simulating optical distortion in images, enabling the generation of more varied and realistic training samples for image processing pipelines.\n\"\"\"",
                    "source_code": "super(OpticalDistortion, self).__init__(always_apply, p)\n        self.shift_limit = to_tuple(shift_limit)\n        self.distort_limit = to_tuple(distort_limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply optical distortion to an input image by warping it based on distortion coefficients and displacement parameters.\n\nThis method introduces synthetic lens distortions to images, which augments the diversity of dataset samples and helps models become more resilient to variations in real-world data.\n\nArgs:\n    img (numpy.ndarray): Input image to be distorted.\n    k (float): Distortion coefficient controlling the curvature of the distortion.\n    dx (float): Shift along the x-axis.\n    dy (float): Shift along the y-axis.\n    interpolation (int): Interpolation method to use when resampling pixels.\n\nReturns:\n    numpy.ndarray: Distorted image with applied optical effects.\n\"\"\"",
                    "source_code": "return F.optical_distortion(img, k, dx, dy, interpolation, self.border_mode, self.value)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nApplies optical distortion to a given image mask using nearest neighbor interpolation and specified distortion parameters.\n\nThis method simulates realistic optical distortion effects on mask images, which helps ensure that mask transformations remain consistent with corresponding image augmentations.\n\nArgs:\n    img (numpy.ndarray): The input mask image to be transformed.\n    k (float): Distortion coefficient that controls the intensity of the distortion.\n    dx (float): Distortion displacement along the x-axis.\n    dy (float): Distortion displacement along the y-axis.\n\nReturns:\n    numpy.ndarray: The mask image after applying optical distortion.\n\"\"\"",
                    "source_code": "return F.optical_distortion(img, k, dx, dy, cv2.INTER_NEAREST, self.border_mode, self.mask_value)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerates random optical distortion parameters within defined limits for subsequent image augmentation. This sampling introduces variability in geometric transformations, promoting diversity in augmented data.\n\nArgs:\n    None\n\nReturns:\n    dict: Dictionary containing the randomly sampled distortion coefficient 'k' and displacement values 'dx' and 'dy'.\n\"\"\"",
                    "source_code": "return {\n            \"k\": random.uniform(self.distort_limit[0], self.distort_limit[1]),\n            \"dx\": round(random.uniform(self.shift_limit[0], self.shift_limit[1])),\n            \"dy\": round(random.uniform(self.shift_limit[0], self.shift_limit[1])),\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of initialization arguments required for reconstructing this transformation's configuration.  \nThis allows for consistent serialization, deserialization, and reproducibility of transformation behaviors.\n\nReturns:\n    tuple: A tuple of strings representing the attribute names needed to initialize the transform.\n\"\"\"",
                    "source_code": "return (\"distort_limit\", \"shift_limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\")"
                }
            ],
            "name": "OpticalDistortion",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the GridDistortion transformation with parameters controlling the granularity, intensity, and application of grid-based geometric distortions to images. This enables the controlled simulation of spatial deformations, which can help models become more robust to geometric variations in input data.\n\nArgs:\n    always_apply (bool): If True, the transformation is always applied.\n    p (float): Probability of applying the transformation.\n    num_steps (int): Number of grid steps for distortion mapping.\n    distort_limit (tuple or float): Range specifying the minimum and maximum amount of distortion for grid points.\n    interpolation (int): Interpolation method used for image transformation.\n    border_mode (int): Pixel extrapolation method used when the transformation extends beyond image boundaries.\n    value (int, float, or tuple): Value for padded pixels if border_mode is set to a constant.\n    mask_value (int, float, or tuple): Value for padded pixels in the mask if border_mode is constant.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(GridDistortion, self).__init__(always_apply, p)\n        self.num_steps = num_steps\n        self.distort_limit = to_tuple(distort_limit)\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply a non-linear distortion to the input image by moving points within a regular grid, creating wavy or warped visual effects. This transformation increases the diversity of training samples by simulating geometric variations that can occur in real-world scenarios.\n\nArgs:\n    img (numpy.ndarray): The input image to be distorted.\n    stepsx (List[float]): Grid distortion factors along the x-axis.\n    stepsy (List[float]): Grid distortion factors along the y-axis.\n    interpolation (int): Interpolation method to use for resampling the image.\n\nReturns:\n    numpy.ndarray: The distorted image with the same shape as the input.\n\"\"\"",
                    "source_code": "return F.grid_distortion(img, self.num_steps, stepsx, stepsy, interpolation, self.border_mode, self.value)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nApply a grid distortion transformation to a mask for data augmentation purposes, ensuring the mask undergoes the same geometric distortion as the corresponding image.\n\nArgs:\n    img (numpy.ndarray): The input mask to be transformed.\n\nReturns:\n    numpy.ndarray: The mask after applying grid distortion with specified transformation parameters.\n\"\"\"",
                    "source_code": "return F.grid_distortion(\n            img, self.num_steps, stepsx, stepsy, cv2.INTER_NEAREST, self.border_mode, self.mask_value\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate randomized distortion parameters for grid-based image transformation.\n\nThis method samples perturbation values for both horizontal and vertical grid steps, which will later be used to distort the image grid and produce various warping effects. Randomizing these parameters introduces spatial variability to augment images during data processing, helping prevent overfitting to rigid patterns.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing 'stepsx' and 'stepsy', which are lists of random distortion values for the grid in the x and y directions, respectively.\n\"\"\"",
                    "source_code": "stepsx = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for i in range(self.num_steps + 1)]\n        stepsy = [1 + random.uniform(self.distort_limit[0], self.distort_limit[1]) for i in range(self.num_steps + 1)]\n        return {\"stepsx\": stepsx, \"stepsy\": stepsy}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to reconstruct the state of the GridDistortion transformation.  \nThis enables consistent serialization and deserialization of transformation instances, allowing reproducibility in data augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the names of the initialization arguments ('num_steps', 'distort_limit', 'interpolation', 'border_mode', 'value', 'mask_value').\n\"\"\"",
                    "source_code": "return (\"num_steps\", \"distort_limit\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\")"
                }
            ],
            "name": "GridDistortion",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ElasticTransform with parameters to control the strength, smoothness, and type of elastic deformation applied to images, as well as how border regions and masks are handled. This setup allows for realistic simulation of local distortions, helping to increase data variability and reduce overfitting in image processing tasks.\n\nArgs:\n    alpha (float): Scaling factor defining the intensity of the deformation.\n    sigma (float): Smoothing parameter that controls the spread of the deformation.\n    alpha_affine (float): Range for affine transformations within the elastic effect.\n    interpolation (int): Interpolation method used for image resampling.\n    border_mode (int): Pixel extrapolation method at the image boundaries.\n    value (int, float, list of ints, list of floats, or None): Value for out-of-bound pixels if border_mode is constant.\n    mask_value (int, float, list of ints, list of floats, or None): Fill value for mask areas outside transformed image, if applicable.\n    approximate (bool): Whether to use an approximate, faster implementation of the transform.\n    always_apply (bool): If True, the transform is always applied; otherwise, it is controlled by probability p.\n    p (float): Probability of applying the transform.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ElasticTransform, self).__init__(always_apply, p)\n        self.alpha = alpha\n        self.alpha_affine = alpha_affine\n        self.sigma = sigma\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.value = value\n        self.mask_value = mask_value\n        self.approximate = approximate"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies elastic deformation to the input image to simulate realistic geometric distortions often seen in real-world scenarios. This transformation helps diversify the dataset, making models more robust to spatial variations that could appear in unseen data.\n\nArgs:\n    img (numpy.ndarray): Input image to be transformed.\n    interpolation (int): Interpolation method used for image transformation.\n    random_state (int or None): Seed or state for random number generation, ensuring reproducibility.\n\nReturns:\n    numpy.ndarray: The elastically transformed image.\n\"\"\"",
                    "source_code": "return F.elastic_transform(\n            img,\n            self.alpha,\n            self.sigma,\n            self.alpha_affine,\n            interpolation,\n            self.border_mode,\n            self.value,\n            np.random.RandomState(random_state),\n            self.approximate,\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nApplies an elastic deformation to a mask image to simulate realistic, non-rigid spatial distortions while preserving discrete label values. This transformation is useful for expanding the variability of segmentation masks during training.\n\nArgs:\n    img (numpy.ndarray): The mask image to be transformed.\n    random_state (int): Seed for the random number generator to ensure deterministic augmentation.\n\nReturns:\n    numpy.ndarray: The transformed mask image after applying elastic deformation.\n\"\"\"",
                    "source_code": "return F.elastic_transform(\n            img,\n            self.alpha,\n            self.sigma,\n            self.alpha_affine,\n            cv2.INTER_NEAREST,\n            self.border_mode,\n            self.mask_value,\n            np.random.RandomState(random_state),\n            self.approximate,\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nReturns a dictionary containing a randomly generated seed value to ensure reproducibility and variability in the transformation's behavior.\n\nReturns:\n    dict: A dictionary with a single key 'random_state' mapping to a random integer between 0 and 10,000.\n\"\"\"",
                    "source_code": "return {\"random_state\": random.randint(0, 10000)}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments needed to reproduce the transformation state when serializing or deserializing the transform object.\n\nReturns:\n    tuple: A tuple containing the names of the parameters used during initialization.\n\"\"\"",
                    "source_code": "return (\"alpha\", \"sigma\", \"alpha_affine\", \"interpolation\", \"border_mode\", \"value\", \"mask_value\", \"approximate\")"
                }
            ],
            "name": "ElasticTransform",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an instance of the RandomGridShuffle transformation, configuring the probability and grid size for shuffling image regions.\n\nThis setup prepares the transformation to rearrange portions of the image according to a specified grid, thereby introducing controlled randomness to support regularization and better generalization during model training.\n\nArgs:\n    always_apply (bool): If True, the transformation will be applied to every image.\n    p (float): Probability of applying the transformation.\n    grid (tuple): Size of the grid (rows, columns) to divide the image for shuffling.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(RandomGridShuffle, self).__init__(always_apply, p)\n        self.grid = grid"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nShuffle selected tiles within the input image according to the specified tile arrangement.\n\nThis method rearranges different regions (tiles) of an image to create a randomized visual structure. Such transformations are helpful for making models robust to changes in spatial arrangements during training.\n\nArgs:\n    img (numpy.ndarray): Input image on which tiles will be shuffled.\n    tiles (list, optional): List of tile locations or indices to be swapped. Defaults to an empty list if None is provided.\n\nReturns:\n    numpy.ndarray: The resulting image after tiles have been swapped.\n\"\"\"",
                    "source_code": "if tiles is None:\n            tiles = []\n\n        return F.swap_tiles_on_image(img, tiles)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nShuffle selected rectangular regions within a given mask by swapping tiles, which introduces spatial variation to the mask and increases the diversity of training data.\n\nArgs:\n    img (numpy.ndarray): The mask image on which tiles will be shuffled.\n    tiles (list, optional): List of tile coordinates to be swapped. If None, an empty list is used.\n\nReturns:\n    numpy.ndarray: The mask image with specified tiles swapped.\n\"\"\"",
                    "source_code": "if tiles is None:\n            tiles = []\n\n        return F.swap_tiles_on_image(img, tiles)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nCalculate the mapping between original and shuffled grid tiles for an input image based on its dimensions and the specified grid configuration. Ensures valid grid and cell sizes, then randomly permutes equally sized tiles to generate tile shuffling indices used in subsequent transformations.\n\nArgs:\n    params (dict): Dictionary containing input image data, where 'image' is a NumPy array.\n\nReturns:\n    dict: A dictionary with a single key \"tiles\". The value is a NumPy array of shape (num_tiles, 6) describing the mapping between tile positions and sizes in the grid, used for shuffling the image content.\n\"\"\"",
                    "source_code": "height, width = params[\"image\"].shape[:2]\n        n, m = self.grid\n\n        if n <= 0 or m <= 0:\n            raise ValueError(\"Grid's values must be positive. Current grid [%s, %s]\" % (n, m))\n\n        if n > height // 2 or m > width // 2:\n            raise ValueError(\"Incorrect size cell of grid. Just shuffle pixels of image\")\n\n        random_state = np.random.RandomState(random.randint(0, 10000))\n\n        height_split = np.linspace(0, height, n + 1, dtype=np.int)\n        width_split = np.linspace(0, width, m + 1, dtype=np.int)\n\n        height_matrix, width_matrix = np.meshgrid(height_split, width_split, indexing=\"ij\")\n\n        index_height_matrix = height_matrix[:-1, :-1]\n        index_width_matrix = width_matrix[:-1, :-1]\n\n        shifted_index_height_matrix = height_matrix[1:, 1:]\n        shifted_index_width_matrix = width_matrix[1:, 1:]\n\n        height_tile_sizes = shifted_index_height_matrix - index_height_matrix\n        width_tile_sizes = shifted_index_width_matrix - index_width_matrix\n\n        tiles_sizes = np.stack((height_tile_sizes, width_tile_sizes), axis=2)\n\n        index_matrix = np.indices((n, m))\n        new_index_matrix = np.stack(index_matrix, axis=2)\n\n        for bbox_size in np.unique(tiles_sizes.reshape(-1, 2), axis=0):\n            eq_mat = np.all(tiles_sizes == bbox_size, axis=2)\n            new_index_matrix[eq_mat] = random_state.permutation(new_index_matrix[eq_mat])\n\n        new_index_matrix = np.split(new_index_matrix, 2, axis=2)\n\n        old_x = index_height_matrix[new_index_matrix[0], new_index_matrix[1]].reshape(-1)\n        old_y = index_width_matrix[new_index_matrix[0], new_index_matrix[1]].reshape(-1)\n\n        shift_x = height_tile_sizes.reshape(-1)\n        shift_y = width_tile_sizes.reshape(-1)\n\n        curr_x = index_height_matrix.reshape(-1)\n        curr_y = index_width_matrix.reshape(-1)\n\n        tiles = np.stack([curr_x, curr_y, old_x, old_y, shift_x, shift_y], axis=1)\n\n        return {\"tiles\": tiles}"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies the target types that this transformation should be applied to.\n\nReturns:\n    List[str]: A list containing the name(s) of the data targets to which the transformation applies, ensuring correct application within augmentation pipelines.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the arguments required to initialize the transform. This method is used to ensure that the correct parameters are tracked and can be serialized or deserialized consistently.\n\nReturns:\n    tuple: A tuple containing the names of the initialization arguments.\n\"\"\"",
                    "source_code": "return (\"grid\",)"
                }
            ],
            "name": "RandomGridShuffle",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the normalization transform with specified mean and standard deviation values, and sets the maximum possible pixel value. This allows for consistent preprocessing of input images, which helps standardize image data for further processing or model training.\n\nArgs:\n    always_apply (bool): If True, the transform will be applied to every image.\n    p (float): Probability of applying the transform.\n    mean (float, tuple, or list): The mean value(s) for each channel used for normalization.\n    std (float, tuple, or list): The standard deviation value(s) for each channel used for normalization.\n    max_pixel_value (float): The maximum possible value for a pixel in the input image (e.g., 255 for 8-bit images).\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Normalize, self).__init__(always_apply, p)\n        self.mean = mean\n        self.std = std\n        self.max_pixel_value = max_pixel_value"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nNormalize the input image using the specified mean, standard deviation, and max pixel value, adjusting its pixel intensity distribution.\n\nThis ensures that images processed have comparable pixel value ranges and distributions, improving the stability and effectiveness of downstream processing and model training.\n\nArgs:\n    image (numpy.ndarray): Input image to be normalized.\n\nReturns:\n    numpy.ndarray: Normalized image with adjusted pixel values.\n\"\"\"",
                    "source_code": "return F.normalize(image, self.mean, self.std, self.max_pixel_value)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to reproduce the normalization transformation. This enables consistent serialization and deserialization of the transformation's configuration, ensuring that the same normalization parameters can be easily reapplied or transferred.\n\nReturns:\n    tuple: A tuple containing the names of the parameters (\"mean\", \"std\", \"max_pixel_value\") used during initialization of the normalization transform.\n\"\"\"",
                    "source_code": "return (\"mean\", \"std\", \"max_pixel_value\")"
                }
            ],
            "name": "Normalize",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Cutout image transformation with specified parameters for masking out random regions of input images. This process helps introduce occlusions during data augmentation, thereby encouraging models to rely on less conspicuous features in the data. Also issues a deprecation warning recommending the use of an alternative class.\n\nArgs:\n    always_apply (bool): Whether to always apply the transformation, regardless of probability.\n    p (float): Probability of applying the transformation.\n    num_holes (int): Number of rectangular regions to mask out from the image.\n    max_h_size (int): Maximum height of each mask region.\n    max_w_size (int): Maximum width of each mask region.\n    fill_value (int or float or sequence): Pixel value with which to fill the masked regions.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Cutout, self).__init__(always_apply, p)\n        self.num_holes = num_holes\n        self.max_h_size = max_h_size\n        self.max_w_size = max_w_size\n        self.fill_value = fill_value\n        warnings.warn(\"This class has been deprecated. Please use CoarseDropout\", DeprecationWarning)"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply cutout augmentation to an input image by masking specified rectangular regions (holes) with a given fill value. This operation increases the robustness of models by simulating occlusions and encouraging them to focus on less discriminative features.\n\nArgs:\n    image (numpy.ndarray): The input image to which the cutout augmentation will be applied.\n    holes (list of tuples): List of tuples, each specifying the coordinates of a rectangular region to be masked (e.g., (x1, y1, x2, y2)).\n    fill_value (int or tuple): The value or color with which to fill the masked regions in the image.\n\nReturns:\n    numpy.ndarray: The augmented image with specified regions masked out.\n\"\"\"",
                    "source_code": "return F.cutout(image, holes, fill_value)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerate random rectangular regions (holes) within an image where content can be removed or masked, improving robustness to occlusions and aiding model generalization.\n\nArgs:\n    params (dict): Dictionary containing transformation parameters, must include an \"image\" key with the image array.\n\nReturns:\n    dict: A dictionary with a single key \"holes\" mapping to a list of tuples, each representing the coordinates (x1, y1, x2, y2) of a randomly-generated rectangular region within the image.\n\"\"\"",
                    "source_code": "img = params[\"image\"]\n        height, width = img.shape[:2]\n\n        holes = []\n        for _n in range(self.num_holes):\n            y = random.randint(0, height)\n            x = random.randint(0, width)\n\n            y1 = np.clip(y - self.max_h_size // 2, 0, height)\n            y2 = np.clip(y + self.max_h_size // 2, 0, height)\n            x1 = np.clip(x - self.max_w_size // 2, 0, width)\n            x2 = np.clip(x + self.max_w_size // 2, 0, width)\n            holes.append((x1, y1, x2, y2))\n\n        return {\"holes\": holes}"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which data targets the Cutout transformation should treat as parameters, enabling the transformation to correctly process and modify relevant components of the input.\n\nReturns:\n    list: A list containing the names of targets (\"image\") treated as parameters by the transformation.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments needed to reconstruct the Cutout transformation instance. \n\nThis enables consistent tracking and reproducibility of transformation parameters, which is important for serialization and experiment management.\n\nReturns:\n    tuple: A tuple containing the names of the arguments used to initialize the Cutout transform ('num_holes', 'max_h_size', 'max_w_size').\n\"\"\"",
                    "source_code": "return (\"num_holes\", \"max_h_size\", \"max_w_size\")"
                }
            ],
            "name": "Cutout",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the parameters required to randomly drop rectangular holes in an input image, simulating occlusions or missing regions during augmentation in order to increase the robustness of computer vision models.\n\nArgs:\n    always_apply (bool): If True, the transformation will be applied to every image.\n    p (float): Probability of applying the transformation.\n    max_holes (int): Maximum number of holes to cut out of the image.\n    max_height (int): Maximum height of each hole.\n    max_width (int): Maximum width of each hole.\n    min_holes (int, optional): Minimum number of holes to cut out of the image. Defaults to max_holes if not provided.\n    min_height (int, optional): Minimum height of each hole. Defaults to max_height if not provided.\n    min_width (int, optional): Minimum width of each hole. Defaults to max_width if not provided.\n    fill_value (int, float, or sequence): Value to fill the holes with.\n\nReturns:\n    None\n\nWhy:\n    This setup introduces localized occlusions in training images, promoting the learning of more generalized and robust features by preventing over-reliance on specific regions.\n\"\"\"",
                    "source_code": "super(CoarseDropout, self).__init__(always_apply, p)\n        self.max_holes = max_holes\n        self.max_height = max_height\n        self.max_width = max_width\n        self.min_holes = min_holes if min_holes is not None else max_holes\n        self.min_height = min_height if min_height is not None else max_height\n        self.min_width = min_width if min_width is not None else max_width\n        self.fill_value = fill_value\n        assert 0 < self.min_holes <= self.max_holes\n        assert 0 < self.min_height <= self.max_height\n        assert 0 < self.min_width <= self.max_width"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies coarse dropout augmentation to the input image by masking out rectangular regions (holes) and filling them with a specified value. This technique increases the robustness of models by simulating occlusions and missing regions in images during training.\n\nArgs:\n    image (numpy.ndarray): Input image to be augmented.\n    holes (list of tuples): List of coordinates defining rectangular regions to be masked out.\n    fill_value (int, float, or tuple): Value used to fill the masked regions.\n\nReturns:\n    numpy.ndarray: Augmented image with applied coarse dropout.\n\"\"\"",
                    "source_code": "return F.cutout(image, holes, fill_value)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerate randomly positioned rectangular regions (\"holes\") to be masked out within the input image based on specified size and count constraints.\n\nThis simulates partial occlusions in the image data, which helps models become more robust to missing or corrupted visual information during training.\n\nArgs:\n    params (dict): Dictionary containing the input image under the \"image\" key.\n\nReturns:\n    dict: Dictionary with a \"holes\" key, where the value is a list of tuples. Each tuple defines a rectangular region with coordinates (x1, y1, x2, y2) representing the top-left and bottom-right corners of each hole to be masked.\n\"\"\"",
                    "source_code": "img = params[\"image\"]\n        height, width = img.shape[:2]\n\n        holes = []\n        for _n in range(random.randint(self.min_holes, self.max_holes)):\n            hole_height = random.randint(self.min_height, self.max_height)\n            hole_width = random.randint(self.min_width, self.max_width)\n\n            y1 = random.randint(0, height - hole_height)\n            x1 = random.randint(0, width - hole_width)\n            y2 = y1 + hole_height\n            x2 = x1 + hole_width\n            holes.append((x1, y1, x2, y2))\n\n        return {\"holes\": holes}"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which data target keys should be interpreted as parameters for augmentation transformations.\n\nReturns:\n    List[str]: A list containing the keys of the targets that are affected by this transformation, in this case, only \"image\".\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of initialization arguments required to configure the CoarseDropout transformation. This enables serialization and reproducibility of the augmentation setup.\n\nReturns:\n    tuple: A tuple containing the names of initialization parameters (\"max_holes\", \"max_height\", \"max_width\", \"min_holes\", \"min_height\", \"min_width\") necessary for reconstructing the transformation.\n\"\"\"",
                    "source_code": "return (\"max_holes\", \"max_height\", \"max_width\", \"min_holes\", \"min_height\", \"min_width\")"
                }
            ],
            "name": "CoarseDropout",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the image compression transform by specifying the type of compression and quality range. Performs validation to ensure quality parameters are appropriate for the selected compression type. This setup allows the application of realistic compression artifacts to images, supporting the creation of more diverse augmented datasets.\n\nArgs:\n    always_apply (bool): Whether to always apply this transformation.\n    p (float): Probability of applying the transformation.\n    compression_type (ImageCompressionType): Compression algorithm to use (e.g., JPEG, WEBP).\n    quality_lower (int): Lower bound of quality value for compression.\n    quality_upper (int): Upper bound of quality value for compression.\n\nRaises:\n    AssertionError: If the quality parameters are outside acceptable bounds for the chosen compression type.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ImageCompression, self).__init__(always_apply, p)\n\n        self.compression_type = compression_type\n        low_thresh_quality_assert = 0\n\n        if self.compression_type == ImageCompression.ImageCompressionType.WEBP:\n            low_thresh_quality_assert = 1\n\n        assert low_thresh_quality_assert <= quality_lower <= 100\n        assert low_thresh_quality_assert <= quality_upper <= 100\n\n        self.quality_lower = quality_lower\n        self.quality_upper = quality_upper"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nCompresses the input image using the specified quality and format settings.\n\nThis method is used to reduce the size or alter the fidelity of an image, which can be beneficial for simulating real-world distortions, optimizing storage, or creating diverse training samples for computer vision tasks.\n\nArgs:\n    image (np.ndarray): Input image to be compressed.\n    quality (int): Compression quality level.\n    image_type (str): Format for image compression (e.g., \"jpeg\", \"webp\").\n\nReturns:\n    np.ndarray: The compressed image.\n\"\"\"",
                    "source_code": "return F.image_compression(image, quality, image_type)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerates compression parameters for saving an image, including the compression quality and output file type, based on the instance's configuration. This allows for flexible adjustment of image storage options during augmentation, supporting experimentation with different compression strategies.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing:\n        - 'quality' (int): Randomly selected quality value within the specified range (`quality_lower`, `quality_upper`).\n        - 'image_type' (str): The file extension (\".jpg\" or \".webp\") determined by the chosen compression type.\n\"\"\"",
                    "source_code": "image_type = \".jpg\"\n\n        if self.compression_type == ImageCompression.ImageCompressionType.WEBP:\n            image_type = \".webp\"\n\n        return {\"quality\": random.randint(self.quality_lower, self.quality_upper), \"image_type\": image_type}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to configure the image compression transformation.\n\nThis method enables consistent serialization and deserialization of the transformation object, which is essential for reproducibility and ensuring that augmentation pipelines can be easily saved and reloaded.\n\nReturns:\n    tuple: A tuple containing the names of the initialization arguments (\"quality_lower\", \"quality_upper\", \"compression_type\").\n\"\"\"",
                    "source_code": "return (\"quality_lower\", \"quality_upper\", \"compression_type\")"
                }
            ],
            "name": "ImageCompression",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the JpegCompression transformation, configuring the quality range, application probability, and marking the transform as deprecated in favor of a newer alternative.\n\nArgs:\n    quality_lower (int): Lower bound for JPEG compression quality.\n    quality_upper (int): Upper bound for JPEG compression quality.\n    always_apply (bool): Whether to always apply this transformation.\n    p (float): Probability of applying the transformation.\n\nReturns:\n    None\n\nWhy:\n    This method sets up the internal parameters required to simulate JPEG artifacts during image augmentation and issues a warning to update usage to a newer approach, ensuring users are notified of deprecated practices while maintaining image transformation functionality.\n\"\"\"",
                    "source_code": "super(JpegCompression, self).__init__(\n            quality_lower=quality_lower,\n            quality_upper=quality_upper,\n            compression_type=ImageCompression.ImageCompressionType.JPEG,\n            always_apply=always_apply,\n            p=p,\n        )\n        warnings.warn(\"This class has been deprecated. Please use ImageCompression\", DeprecationWarning)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args",
                    "second_doc": "\"\"\"\nRetrieve the initialization arguments for the image compression transform, capturing the allowed JPEG quality range.  \nThis information is essential for serialization purposes and ensures reproducibility and consistency when saving or reconstructing the transformation pipeline.\n\nReturns:\n    dict: A dictionary containing the lower and upper bounds for JPEG quality, with keys \"quality_lower\" and \"quality_upper\".\n\"\"\"",
                    "source_code": "return {\"quality_lower\": self.quality_lower, \"quality_upper\": self.quality_upper}"
                }
            ],
            "name": "JpegCompression",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the parameters required to simulate a randomized snow effect in image augmentation. Ensures that the snow effect and brightness adjustments are applied within valid bounds, allowing consistent and controlled augmentation behavior.\n\nArgs:\n    always_apply (bool): If True, always apply the transform. Default: False.\n    p (float): Probability of applying the transform. Default: 0.5.\n    snow_point_lower (float): Lower bound for snow intensity, must be between 0 and 1.\n    snow_point_upper (float): Upper bound for snow intensity, must be between 0 and 1, and no less than snow_point_lower.\n    brightness_coeff (float): Coefficient to adjust image brightness, must be non-negative.\n\nReturns:\n    None\n\nWhy:\n    The method validates and stores configuration options for generating realistic snow and brightness variations during augmentation, supporting reproducible and flexible transformations necessary for improving data diversity in computer vision tasks.\n\"\"\"",
                    "source_code": "super(RandomSnow, self).__init__(always_apply, p)\n\n        assert 0 <= snow_point_lower <= snow_point_upper <= 1\n        assert 0 <= brightness_coeff\n\n        self.snow_point_lower = snow_point_lower\n        self.snow_point_upper = snow_point_upper\n        self.brightness_coeff = brightness_coeff"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nSimulates snowy weather conditions by overlaying snow effects on the input image to introduce visual variability and make models more robust to environmental changes.\n\nArgs:\n    image (numpy.ndarray): Input image to which the snow effect will be applied.\n    snow_point (float): The intensity or amount of snow to add to the image.\n\nReturns:\n    numpy.ndarray: The transformed image with simulated snow effects.\n\"\"\"",
                    "source_code": "return F.add_snow(image, snow_point, self.brightness_coeff)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate a random parameter value for the snow effect to introduce variability in image augmentations.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing a randomly selected 'snow_point' value within the specified range, which controls the intensity or appearance of the snow effect during image transformation.\n\"\"\"",
                    "source_code": "return {\"snow_point\": random.uniform(self.snow_point_lower, self.snow_point_upper)}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to configure the random snow effect transformation. This enables consistent serialization and deserialization of transformation parameters when defining augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the names of the required initialization parameters: (\"snow_point_lower\", \"snow_point_upper\", \"brightness_coeff\").\n\"\"\"",
                    "source_code": "return (\"snow_point_lower\", \"snow_point_upper\", \"brightness_coeff\")"
                }
            ],
            "name": "RandomSnow",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an instance of the RandomRain transformation with specified parameters that control the visual and statistical properties of simulated rain streaks applied to images.\n\nArgs:\n    always_apply (bool): Whether to always apply the transformation.\n    p (float): Probability of applying the transformation.\n    slant_lower (int): Lower bound for the slant angle of rain streaks in degrees (must be between -20 and 20).\n    slant_upper (int): Upper bound for the slant angle of rain streaks in degrees (must be between -20 and 20 and greater than or equal to slant_lower).\n    drop_length (int): Length of each rain drop streak (between 0 and 100).\n    drop_width (int): Width of each rain drop streak (between 1 and 5).\n    drop_color (tuple or int): Color value for rain drops.\n    blur_value (int): Amount of blur applied to rain streaks.\n    brightness_coefficient (float): Coefficient controlling the brightness reduction of the image due to rain (between 0 and 1).\n    rain_type (str or None): Type or intensity of simulated rain, such as \"drizzle\", \"heavy\", \"torrential\", or None.\n\nReturns:\n    None\n\nWhy:\n    This method sets up the RandomRain transformation to produce diverse and realistic rain effects for augmenting image datasets, which helps models better handle real-world weather conditions in computer vision tasks.\n\"\"\"",
                    "source_code": "super(RandomRain, self).__init__(always_apply, p)\n\n        assert rain_type in [\"drizzle\", \"heavy\", \"torrential\", None]\n\n        assert -20 <= slant_lower <= slant_upper <= 20\n        assert 1 <= drop_width <= 5\n        assert 0 <= drop_length <= 100\n        assert 0 <= brightness_coefficient <= 1\n\n        self.slant_lower = slant_lower\n        self.slant_upper = slant_upper\n\n        self.drop_length = drop_length\n        self.drop_width = drop_width\n        self.drop_color = drop_color\n        self.blur_value = blur_value\n        self.brightness_coefficient = brightness_coefficient\n        self.rain_type = rain_type"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply a synthetic rain effect to the input image by overlaying rain streaks with configurable visual parameters such as slant, drop length, drop width, color, blur, and brightness. This transformation is used to increase variability in training data, helping models become more robust to challenging weather conditions.\n\nArgs:\n    image (numpy.ndarray): The input image to which the rain effect will be applied.\n    slant (int or float): The angle at which the rain streaks are slanted.\n    drop_length (int): The length of each rain streak.\n    rain_drops (list): The list of coordinates or specifications for each rain drop to be rendered.\n\nReturns:\n    numpy.ndarray: The augmented image with rain effect applied.\n\"\"\"",
                    "source_code": "return F.add_rain(\n            image,\n            slant,\n            drop_length,\n            self.drop_width,\n            self.drop_color,\n            self.blur_value,\n            self.brightness_coefficient,\n            rain_drops,\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which data targets will be affected by the transformation, allowing augmentation to be applied correctly to images.\n\nReturns:\n    list: A list containing the single string 'image', indicating that only image data is targeted by this transformation.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nCompute rain effect parameters based on the input image dimensions and specified rain intensity. By adapting drop length and placement to image size and rain type, the method ensures realistic and scalable augmentation for diverse training samples.\n\nArgs:\n    params (dict): Dictionary containing the 'image' key with the input image as a NumPy array.\n\nReturns:\n    dict: A dictionary with keys:\n        - 'drop_length' (int): Length of each rain streak.\n        - 'rain_drops' (list of tuple): Starting coordinates (x, y) for each rain streak.\n\"\"\"",
                    "source_code": "img = params[\"image\"]\n        slant = int(random.uniform(self.slant_lower, self.slant_upper))\n\n        height, width = img.shape[:2]\n        area = height * width\n\n        if self.rain_type == \"drizzle\":\n            num_drops = area // 770\n            drop_length = 10\n        elif self.rain_type == \"heavy\":\n            num_drops = width * height // 600\n            drop_length = 30\n        elif self.rain_type == \"torrential\":\n            num_drops = area // 500\n            drop_length = 60\n        else:\n            drop_length = self.drop_length\n            num_drops = area // 600\n\n        rain_drops = []\n\n        for _i in range(num_drops):  # If You want heavy rain, try increasing this\n            if slant < 0:\n                x = random.randint(slant, width)\n            else:\n                x = random.randint(0, width - slant)\n\n            y = random.randint(0, height - drop_length)\n\n            rain_drops.append((x, y))\n\n        return {\"drop_length\": drop_length, \"rain_drops\": rain_drops}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns a tuple containing the names of parameters used to configure the random rain effect transformation. This allows the transformation to be serialized or reconstructed with the same settings, enabling consistent and reproducible visual augmentations for images.\n\nReturns:\n    tuple: Parameter names required to initialize the rain effect transformation, including slant, drop geometry, color, blur, brightness, and type.\n\"\"\"",
                    "source_code": "return (\n            \"slant_lower\",\n            \"slant_upper\",\n            \"drop_length\",\n            \"drop_width\",\n            \"drop_color\",\n            \"blur_value\",\n            \"brightness_coefficient\",\n            \"rain_type\",\n        )"
                }
            ],
            "name": "RandomRain",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the parameters controlling the application and intensity of the fog effect to images. Ensures that the specified ranges for fog strength and transparency are valid, storing them as instance attributes.\n\nArgs:\n    always_apply (bool): If True, the transformation will be applied to every image.\n    p (float): Probability of applying the transformation.\n    fog_coef_lower (float): Lower bound for the fog intensity coefficient (must be between 0 and 1).\n    fog_coef_upper (float): Upper bound for the fog intensity coefficient (must be between 0 and 1).\n    alpha_coef (float): Alpha blending factor for the fog effect (must be between 0 and 1).\n\nReturns:\n    None\n\nWhy:\n    The method validates and stores the range and blending parameters so that realistic and random fog effects can later be added to images, increasing visual variety and helping models learn to recognize scenes in different weather conditions.\n\"\"\"",
                    "source_code": "super(RandomFog, self).__init__(always_apply, p)\n\n        assert 0 <= fog_coef_lower <= fog_coef_upper <= 1\n        assert 0 <= alpha_coef <= 1\n\n        self.fog_coef_lower = fog_coef_lower\n        self.fog_coef_upper = fog_coef_upper\n        self.alpha_coef = alpha_coef"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies a simulated fog effect to the input image to alter its visual appearance and introduce variability for data augmentation purposes.\n\nArgs:\n    image (numpy.ndarray): The input image to which the fog effect will be applied.\n    fog_coef (float): The coefficient determining the strength or density of the fog effect.\n    haze_list (list): Parameters influencing the distribution and look of the fog in the image.\n\nReturns:\n    numpy.ndarray: The augmented image with the fog effect applied.\n    \nThe method introduces fog to synthetically increase data diversity, making models trained on augmented data more robust to challenging weather-like conditions.\n\"\"\"",
                    "source_code": "return F.add_fog(image, fog_coef, self.alpha_coef, haze_list)"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which input targets the RandomFog transformation should treat as parameterizable, enabling the augmentation to be applied specifically to those targets.\n\nReturns:\n    List[str]: A list containing the names of input targets (currently only \"image\") that the transformation expects and will operate on.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerate randomized parameters for simulating a fog effect over an image, producing both the fog intensity and the spatial distribution of haze centers. These parameters drive the procedural application of fog, allowing for varied visual appearances to enhance data diversity.\n\nArgs:\n    params (dict): Dictionary containing the 'image' key with a NumPy array representing the input image.\n\nReturns:\n    dict: Contains 'haze_list', a list of (x, y) coordinates for haze centers, and 'fog_coef', a float indicating fog intensity.\n\"\"\"",
                    "source_code": "img = params[\"image\"]\n        fog_coef = random.uniform(self.fog_coef_lower, self.fog_coef_upper)\n\n        height, width = imshape = img.shape[:2]\n\n        hw = max(1, int(width // 3 * fog_coef))\n\n        haze_list = []\n        midx = width // 2 - 2 * hw\n        midy = height // 2 - hw\n        index = 1\n\n        while midx > -hw or midy > -hw:\n            for _i in range(hw // 10 * index):\n                x = random.randint(midx, width - midx - hw)\n                y = random.randint(midy, height - midy - hw)\n                haze_list.append((x, y))\n\n            midx -= 3 * hw * width // sum(imshape)\n            midy -= 3 * hw * height // sum(imshape)\n            index += 1\n\n        return {\"haze_list\": haze_list, \"fog_coef\": fog_coef}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the parameters required to initialize the random fog transformation. \n\nThese parameter names are utilized to ensure transform configurations can be properly instantiated, saved, or reproduced.\n\nReturns:\n    tuple: Names of the initialization arguments ('fog_coef_lower', 'fog_coef_upper', 'alpha_coef').\n\"\"\"",
                    "source_code": "return (\"fog_coef_lower\", \"fog_coef_upper\", \"alpha_coef\")"
                }
            ],
            "name": "RandomFog",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the RandomSunFlare image transformation with specified region, angle, and visual characteristics for simulating sun flare effects.\n\nArgs:\n    always_apply (bool): If True, the transformation will always be applied.\n    p (float): Probability of applying the transformation.\n    flare_roi (tuple): A tuple (flare_center_lower_x, flare_center_lower_y, flare_center_upper_x, flare_center_upper_y) specifying the bounds within the normalized image region where the sun flare center can be placed.\n    angle_lower (float): Lower bound (normalized 0 to 1) for the angle of the flare effect.\n    angle_upper (float): Upper bound (normalized 0 to 1) for the angle of the flare effect.\n    num_flare_circles_lower (int): Minimum number of circles to be used in the sun flare.\n    num_flare_circles_upper (int): Maximum number of circles to be used in the sun flare.\n    src_radius (int): Radius of the flare source in pixels.\n    src_color (tuple): RGB color tuple for the flare source.\n\nReturns:\n    None\n\nWhy:\n    This method sets up constraints and parameters necessary to produce realistic and varied sun flare augmentations in images, thereby introducing diversity into training datasets and making trained models more robust to lens flare artifacts.\n\"\"\"",
                    "source_code": "super(RandomSunFlare, self).__init__(always_apply, p)\n\n        (flare_center_lower_x, flare_center_lower_y, flare_center_upper_x, flare_center_upper_y) = flare_roi\n\n        assert 0 <= flare_center_lower_x < flare_center_upper_x <= 1\n        assert 0 <= flare_center_lower_y < flare_center_upper_y <= 1\n        assert 0 <= angle_lower < angle_upper <= 1\n        assert 0 <= num_flare_circles_lower < num_flare_circles_upper\n\n        self.flare_center_lower_x = flare_center_lower_x\n        self.flare_center_upper_x = flare_center_upper_x\n\n        self.flare_center_lower_y = flare_center_lower_y\n        self.flare_center_upper_y = flare_center_upper_y\n\n        self.angle_lower = angle_lower\n        self.angle_upper = angle_upper\n        self.num_flare_circles_lower = num_flare_circles_lower\n        self.num_flare_circles_upper = num_flare_circles_upper\n\n        self.src_radius = src_radius\n        self.src_color = src_color"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nSimulates the visual effect of sun flares in the input image by adding light artifacts at a specified location. This operation enriches the diversity of the dataset by introducing realistic weather-induced variations, which can help models generalize better to natural lighting conditions encountered in real-world scenarios.\n\nArgs:\n    image (numpy.ndarray): The input image to which the sun flare effect will be applied.\n    flare_center_x (int): The x-coordinate of the flare's central point.\n    flare_center_y (int): The y-coordinate of the flare's central point.\n    circles (List[Tuple[int, int, int]]): A list of tuples, each representing a circle's center coordinates and radius to compose the flare arcs/glow.\n\nReturns:\n    numpy.ndarray: Image with the sun flare artifact applied.\n\"\"\"",
                    "source_code": "return F.add_sun_flare(image, flare_center_x, flare_center_y, self.src_radius, self.src_color, circles)"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies the target components that the transformation operates on, ensuring that only relevant inputs are considered during augmentation.\n\nReturns:\n    list: A list containing the string \"image\", indicating that this transformation should be applied to image data.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerate parameters for random sun flare effects tailored to the dimensions of the input image, such as the location of the flare center and the properties of multiple flare circles. This approach introduces realistic variability and placement of sun flares, supporting the creation of diverse image augmentations.\n\nArgs:\n    params (dict): Dictionary containing the key \"image\" with a NumPy array representing the input image.\n\nReturns:\n    dict: Dictionary with keys:\n        \"circles\" (list): A list of tuples, each defining a flare circle with attributes for transparency, position, radius, and color.\n        \"flare_center_x\" (int): X-coordinate of the sun flare center.\n        \"flare_center_y\" (int): Y-coordinate of the sun flare center.\n\"\"\"",
                    "source_code": "img = params[\"image\"]\n        height, width = img.shape[:2]\n\n        angle = 2 * math.pi * random.uniform(self.angle_lower, self.angle_upper)\n\n        flare_center_x = random.uniform(self.flare_center_lower_x, self.flare_center_upper_x)\n        flare_center_y = random.uniform(self.flare_center_lower_y, self.flare_center_upper_y)\n\n        flare_center_x = int(width * flare_center_x)\n        flare_center_y = int(height * flare_center_y)\n\n        num_circles = random.randint(self.num_flare_circles_lower, self.num_flare_circles_upper)\n\n        circles = []\n\n        x = []\n        y = []\n\n        for rand_x in range(0, width, 10):\n            rand_y = math.tan(angle) * (rand_x - flare_center_x) + flare_center_y\n            x.append(rand_x)\n            y.append(2 * flare_center_y - rand_y)\n\n        for _i in range(num_circles):\n            alpha = random.uniform(0.05, 0.2)\n            r = random.randint(0, len(x) - 1)\n            rad = random.randint(1, max(height // 100 - 2, 2))\n\n            r_color = random.randint(max(self.src_color[0] - 50, 0), self.src_color[0])\n            g_color = random.randint(max(self.src_color[0] - 50, 0), self.src_color[0])\n            b_color = random.randint(max(self.src_color[0] - 50, 0), self.src_color[0])\n\n            circles += [(alpha, (int(x[r]), int(y[r])), pow(rad, 3), (r_color, g_color, b_color))]\n\n        return {\"circles\": circles, \"flare_center_x\": flare_center_x, \"flare_center_y\": flare_center_y}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args",
                    "second_doc": "\"\"\"\nRetrieve the initialization arguments required to configure the random sun flare transformation.\n\nThis method collects the parameters that define the positioning, angle, number of circles, and appearance of the sun flare effect, enabling consistent transformation behavior during serialization or pipeline replication.\n\nReturns:\n    dict: A dictionary containing all initialization arguments for reproducing the transformation, including flare region, angle limits, circle count limits, radius, and color settings.\n\"\"\"",
                    "source_code": "return {\n            \"flare_roi\": (\n                self.flare_center_lower_x,\n                self.flare_center_lower_y,\n                self.flare_center_upper_x,\n                self.flare_center_upper_y,\n            ),\n            \"angle_lower\": self.angle_lower,\n            \"angle_upper\": self.angle_upper,\n            \"num_flare_circles_lower\": self.num_flare_circles_lower,\n            \"num_flare_circles_upper\": self.num_flare_circles_upper,\n            \"src_radius\": self.src_radius,\n            \"src_color\": self.src_color,\n        }"
                }
            ],
            "name": "RandomSunFlare",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the RandomShadow transformation, configuring parameters that control the region, quantity, and dimensions of potential shadow effects to be applied within an image. This setup enables synthetic variation in lighting conditions across datasets, leading to more resilient visual recognition systems.\n\nArgs:\n    always_apply (bool): If True, the transform will be applied to every image.\n    p (float): Probability of applying the transformation.\n    shadow_roi (tuple): Rectangle region (lower_x, lower_y, upper_x, upper_y) within an image where shadows can be drawn, with coordinates normalized between 0 and 1.\n    num_shadows_lower (int): Minimum number of shadows to generate.\n    num_shadows_upper (int): Maximum number of shadows to generate.\n    shadow_dimension (int): Maximum vertices count for shadow polygons.\n\nRaises:\n    AssertionError: If the provided shadow_roi coordinates are not within [0, 1] or if shadow counts are invalid.\n\"\"\"",
                    "source_code": "super(RandomShadow, self).__init__(always_apply, p)\n\n        (shadow_lower_x, shadow_lower_y, shadow_upper_x, shadow_upper_y) = shadow_roi\n\n        assert 0 <= shadow_lower_x <= shadow_upper_x <= 1\n        assert 0 <= shadow_lower_y <= shadow_upper_y <= 1\n        assert 0 <= num_shadows_lower <= num_shadows_upper\n\n        self.shadow_roi = shadow_roi\n\n        self.num_shadows_lower = num_shadows_lower\n        self.num_shadows_upper = num_shadows_upper\n\n        self.shadow_dimension = shadow_dimension"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies a shadow effect to the input image using the specified shadow regions.\n\nThis method is used to simulate varying lighting conditions and occlusions in images by casting artificial shadows, thereby increasing the diversity and robustness of the training data for computer vision models.\n\nArgs:\n    image (numpy.ndarray): The input image to which the shadow will be applied.\n    vertices_list (list): A list of vertex coordinates defining the regions where shadows will appear.\n\nReturns:\n    numpy.ndarray: The augmented image with shadows applied.\n\"\"\"",
                    "source_code": "return F.add_shadow(image, vertices_list)"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which input arguments should be treated as targets for transformation operations.\n\nReturns:\n    List[str]: A list containing the names of target parameters that will be subjected to augmentation.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerates a list of random polygon vertices defining shadow regions based on image size and transformation parameters.\n\nThis method calculates coordinates for simulated shadow areas to be added to an image, with the number and shape of shadows determined by the object's configuration and the input image dimensions. This enables dynamic, randomized placement and shape for each shadow effect, contributing to diverse augmentation outcomes.\n\nArgs:\n    params (dict): Dictionary containing the input image under the key \"image\".\n\nReturns:\n    dict: A dictionary with a single key \"vertices_list\", containing a list of NumPy arrays. Each array defines the vertices of a polygon representing a shadow region in the image.\n\"\"\"",
                    "source_code": "img = params[\"image\"]\n        height, width = img.shape[:2]\n\n        num_shadows = random.randint(self.num_shadows_lower, self.num_shadows_upper)\n\n        x_min, y_min, x_max, y_max = self.shadow_roi\n\n        x_min = int(x_min * width)\n        x_max = int(x_max * width)\n        y_min = int(y_min * height)\n        y_max = int(y_max * height)\n\n        vertices_list = []\n\n        for _index in range(num_shadows):\n            vertex = []\n            for _dimension in range(self.shadow_dimension):\n                vertex.append((random.randint(x_min, x_max), random.randint(y_min, y_max)))\n\n            vertices = np.array([vertex], dtype=np.int32)\n            vertices_list.append(vertices)\n\n        return {\"vertices_list\": vertices_list}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the list of initialization argument names for serialization and reproducibility of the RandomShadow transformation.\n\nReturns:\n    tuple: A tuple containing the names of the constructor arguments (\"shadow_roi\", \"num_shadows_lower\", \"num_shadows_upper\", \"shadow_dimension\") required to re-instantiate the transform.\n\"\"\"",
                    "source_code": "return (\"shadow_roi\", \"num_shadows_lower\", \"num_shadows_upper\", \"shadow_dimension\")"
                }
            ],
            "name": "RandomShadow",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the HueSaturationValue transformation with specified limits for shifting hue, saturation, and value channels. This enables flexible color manipulation within defined boundaries, allowing for controlled augmentation of image appearance.\n\nArgs:\n    hue_shift_limit (int, float, or tuple): Range for random hue shifting.\n    sat_shift_limit (int, float, or tuple): Range for random saturation shifting.\n    val_shift_limit (int, float, or tuple): Range for random value (brightness) shifting.\n    always_apply (bool, optional): Whether to always apply the transformation.\n    p (float, optional): Probability of applying the transformation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(HueSaturationValue, self).__init__(always_apply, p)\n        self.hue_shift_limit = to_tuple(hue_shift_limit)\n        self.sat_shift_limit = to_tuple(sat_shift_limit)\n        self.val_shift_limit = to_tuple(val_shift_limit)"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply shifts in the hue, saturation, and value channels to the input image.\n\nThis method is used to alter the color properties of an image, introducing controlled diversity in appearance. Such modifications help machine learning models become more robust to color variations during training.\n\nArgs:\n    image (numpy.ndarray): Input image on which to apply the HSV shift.\n    hue_shift (int or float): Value to shift the hue channel.\n    sat_shift (int or float): Value to shift the saturation channel.\n    val_shift (int or float): Value to shift the value (brightness) channel.\n\nReturns:\n    numpy.ndarray: Image with adjusted hue, saturation, and value.\n\"\"\"",
                    "source_code": "return F.shift_hsv(image, hue_shift, sat_shift, val_shift)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate random values for hue, saturation, and value shifts within predefined limits to enable diverse color-based image transformations.\n\nThis method creates slight, randomized alterations to image color properties, which helps introduce variability and reduces overfitting during model training.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing randomly sampled 'hue_shift', 'sat_shift', and 'val_shift' values within their respective configured ranges.\n\"\"\"",
                    "source_code": "return {\n            \"hue_shift\": random.uniform(self.hue_shift_limit[0], self.hue_shift_limit[1]),\n            \"sat_shift\": random.uniform(self.sat_shift_limit[0], self.sat_shift_limit[1]),\n            \"val_shift\": random.uniform(self.val_shift_limit[0], self.val_shift_limit[1]),\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required for configuring the hue, saturation, and value shift limits in this color transformation.\n\nThis method is used to ensure that all essential parameters for adjusting image hue, saturation, and brightness are tracked and correctly recorded, which is important for consistent transformation instantiation, serialization, and reproducibility.\n\nReturns:\n    tuple: A tuple containing the argument names (\"hue_shift_limit\", \"sat_shift_limit\", \"val_shift_limit\").\n\"\"\"",
                    "source_code": "return (\"hue_shift_limit\", \"sat_shift_limit\", \"val_shift_limit\")"
                }
            ],
            "name": "HueSaturationValue",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitialize the Solarize transform with specified parameters, processing the threshold value to ensure it is represented as a tuple for internal consistency. This allows for flexible and robust handling of threshold inputs, accommodating both single values and ranges for image augmentation.\n\nArgs:\n    always_apply (bool): If True, always apply the transform.\n    p (float): Probability of applying the transform.\n    threshold (int, float, or tuple): Value(s) determining the pixel intensity threshold for the solarization effect.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Solarize, self).__init__(always_apply, p)\n\n        if isinstance(threshold, (int, float)):\n            self.threshold = to_tuple(threshold, low=threshold)\n        else:\n            self.threshold = to_tuple(threshold, low=0)"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply a solarization effect to the input image, inverting all pixel values above a specified threshold.\n\nArgs:\n    image (numpy.ndarray or PIL.Image): The input image to be solarized.\n    threshold (int): All pixel values above this threshold will be inverted.\n\nReturns:\n    numpy.ndarray or PIL.Image: The transformed image after solarization.\n\nWhy:\n    This method introduces a nonlinear photometric transformation, which helps create diverse visual examples during image augmentation. Such diversity can aid machine learning models in learning more robust, generalizable features from the training data.\n\"\"\"",
                    "source_code": "return F.solarize(image, threshold)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate a random threshold value within the specified range for use in the solarize transformation.\n\nThis method introduces controlled randomness into the transformation parameters to ensure that each application of the transformation can produce varied effects, thereby increasing data diversity.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing a randomly selected 'threshold' value within the specified range.\n\"\"\"",
                    "source_code": "return {\"threshold\": random.uniform(self.threshold[0], self.threshold[1])}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of initialization arguments required to reproduce the transform's configuration state.\n\nThis method enables consistent serialization and deserialization of the transform, facilitating its inclusion and parameter tracking within augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the string names of initialization arguments used by the transform (\"threshold\",).\n\"\"\"",
                    "source_code": "return (\"threshold\",)"
                }
            ],
            "name": "Solarize",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Posterize transformation by setting the number of bits used to represent each color channel in the output image. The flexible handling of input allows for consistent and controlled reduction of image color depth, which can enhance the augmentation process by creating visually simplified variations.\n\nArgs:\n    always_apply (bool): Whether to always apply the transformation.\n    p (float): Probability of applying the transformation.\n    num_bits (int, list, or tuple): Number of bits to use for each image channel. Can be a single integer, or a list/tuple for channel-specific configuration.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Posterize, self).__init__(always_apply, p)\n\n        if isinstance(num_bits, (list, tuple)):\n            if len(num_bits) == 3:\n                self.num_bits = [to_tuple(i, 0) for i in num_bits]\n            else:\n                self.num_bits = to_tuple(num_bits, 0)\n        else:\n            self.num_bits = to_tuple(num_bits, num_bits)"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nReduces the number of bits used to represent each color channel in the input image, effectively decreasing the image's color depth.\n\nThis operation simulates lower-fidelity imaging conditions, which can help models become more robust to variations in image quality.\n\nArgs:\n    image (numpy.ndarray or PIL.Image): The input image to be posterized.\n    num_bits (int): The number of high bits to keep for each color channel.\n\nReturns:\n    numpy.ndarray or PIL.Image: The posterized image with reduced color depth.\n\"\"\"",
                    "source_code": "return F.posterize(image, num_bits)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate randomized parameters for the posterization operation by selecting appropriate bit depth values for image channels.\n\nThis method introduces controlled variability in the posterization process, supporting enhanced data diversity during augmentation.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing randomly selected 'num_bits' values, either as a list for multi-channel images or a single integer for single-channel images.\n\"\"\"",
                    "source_code": "if len(self.num_bits) == 3:\n            return {\"num_bits\": [random.randint(i[0], i[1]) for i in self.num_bits]}\n        return {\"num_bits\": random.randint(self.num_bits[0], self.num_bits[1])}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to configure the Posterize transformation. \n\nThis allows consistent serialization and deserialization of the transform's parameters within augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the string names of the init parameters.\n\"\"\"",
                    "source_code": "return (\"num_bits\",)"
                }
            ],
            "name": "Posterize",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Equalize transformation with specified operation mode and other configuration options. Ensures the selected mode for equalization is supported before storing parameters needed to control how the transformation is applied.\n\nArgs:\n    mode (str): The equalization mode to use; must be either \"cv\" or \"pil\".\n    by_channels (bool): Whether to apply equalization independently to each channel.\n    mask (callable, optional): Optional mask to specify regions where equalization should be applied.\n    mask_params (tuple, optional): Parameters for mask generation.\n    always_apply (bool, optional): If True, always apply the transformation.\n    p (float, optional): Probability of applying the transformation.\n\nRaises:\n    ValueError: If the provided mode is not supported.\n\nReturns:\n    None\n\nWhy:\n    This setup ensures the transformation is initialized with valid parameters and is configurable, promoting consistency and flexibility when preprocessing or augmenting images for downstream tasks.\n\"\"\"",
                    "source_code": "modes = [\"cv\", \"pil\"]\n        if mode not in modes:\n            raise ValueError(\"Unsupported equalization mode. Supports: {}. \" \"Got: {}\".format(modes, mode))\n\n        super(Equalize, self).__init__(always_apply, p)\n        self.mode = mode\n        self.by_channels = by_channels\n        self.mask = mask\n        self.mask_params = mask_params"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply histogram equalization to the input image, enhancing image contrast by redistributing pixel intensity values. This transformation helps the model learn more robust features by standardizing lighting conditions across the dataset.\n\nArgs:\n    image (np.ndarray): Input image to be equalized.\n    mask (np.ndarray, optional): Optional mask to selectively equalize regions of the image.\n\nReturns:\n    np.ndarray: The equalized image with adjusted contrast.\n\"\"\"",
                    "source_code": "return F.equalize(image, mode=self.mode, by_channels=self.by_channels, mask=mask)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerate a dictionary of parameters for the 'mask' attribute, adapting to whether 'mask' is a callable or a static value. This enables flexible handling of different 'mask' types during transformation pipelines by providing either the static value or computed mask based on the input parameters.\n\nArgs:\n    params (dict): Input parameters that may be used when 'mask' is a callable to generate the appropriate mask.\n\nReturns:\n    dict: A dictionary containing the 'mask', either as a static value or as the result of calling the 'mask' function with the given parameters.\n\"\"\"",
                    "source_code": "if not callable(self.mask):\n            return {\"mask\": self.mask}\n\n        return {\"mask\": self.mask(**params)}"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nReturns a list of parameter names required for the equalization transformation, including 'image' and any additional parameters related to mask handling. \n\nThis allows the transformation to dynamically specify which input data elements are necessary for its operation, ensuring correct application within augmentation pipelines.\n\nReturns:\n    List[str]: Names of parameters expected by this transformation ('image' plus relevant mask parameters).\n\"\"\"",
                    "source_code": "return [\"image\"] + list(self.mask_params)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the parameters required to initialize the transform, which helps ensure correct reconstruction and reproducibility of the transformation when used in augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the names of the parameters (\"mode\", \"by_channels\") used for initializing the transform.\n\"\"\"",
                    "source_code": "return (\"mode\", \"by_channels\")"
                }
            ],
            "name": "Equalize",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitialize the RGBShift transformation by setting up shift limits for each color channel, allowing controlled and randomized adjustments to the red, green, and blue values of input images. This helps simulate variations in lighting or color conditions during training.\n\nArgs:\n    always_apply (bool): Whether to always apply this transformation.\n    p (float): Probability of applying the transform.\n    r_shift_limit (int, float, or tuple of two numbers): Range for random red channel shifts.\n    g_shift_limit (int, float, or tuple of two numbers): Range for random green channel shifts.\n    b_shift_limit (int, float, or tuple of two numbers): Range for random blue channel shifts.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(RGBShift, self).__init__(always_apply, p)\n        self.r_shift_limit = to_tuple(r_shift_limit)\n        self.g_shift_limit = to_tuple(g_shift_limit)\n        self.b_shift_limit = to_tuple(b_shift_limit)"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nShift the red, green, and blue channels of the image by specified amounts to generate color variations.\n\nArgs:\n    image (numpy.ndarray): Input image to be transformed.\n    r_shift (int): Value to shift the red channel.\n    g_shift (int): Value to shift the green channel.\n    b_shift (int): Value to shift the blue channel.\n\nReturns:\n    numpy.ndarray: The image with shifted color channels.\n\nThis method modifies the color composition of images to increase the diversity of visual patterns, allowing models to learn robust features despite color changes in the input data.\n\"\"\"",
                    "source_code": "return F.shift_rgb(image, r_shift, g_shift, b_shift)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate randomized shift values for the red, green, and blue color channels within specified limits.\n\nThis method introduces controlled randomness to the color balance of images, supporting diverse augmentation for more robust model training.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing 'r_shift', 'g_shift', and 'b_shift' keys mapped to float values representing the random shifts for each color channel.\n\"\"\"",
                    "source_code": "return {\n            \"r_shift\": random.uniform(self.r_shift_limit[0], self.r_shift_limit[1]),\n            \"g_shift\": random.uniform(self.g_shift_limit[0], self.g_shift_limit[1]),\n            \"b_shift\": random.uniform(self.b_shift_limit[0], self.b_shift_limit[1]),\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to configure the RGB channel shifting transformation. \n\nThis method makes it possible to serialize and reproduce the transformation by consistently identifying its key parameter names.\n\nReturns:\n    tuple: A tuple containing the names of the parameters (\"r_shift_limit\", \"g_shift_limit\", \"b_shift_limit\") that control the random shift limits for each color channel.\n\"\"\"",
                    "source_code": "return (\"r_shift_limit\", \"g_shift_limit\", \"b_shift_limit\")"
                }
            ],
            "name": "RGBShift",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the parameters for random brightness and contrast image transformations, allowing controlled variability within specified limits.\n\nArgs:\n    always_apply (bool): Whether to always apply the transformation.\n    p (float): Probability of applying the transformation.\n    brightness_limit (tuple or float): Minimum and maximum factor range for adjusting brightness.\n    contrast_limit (tuple or float): Minimum and maximum factor range for adjusting contrast.\n    brightness_by_max (bool): Whether brightness adjustment is computed by maximum pixel value.\n\nReturns:\n    None\n\nWhy:\n    This method sets up the configuration for applying random brightness and contrast changes to images, which helps introduce variability in image data and can improve the robustness of computer vision models during training.\n\"\"\"",
                    "source_code": "super(RandomBrightnessContrast, self).__init__(always_apply, p)\n        self.brightness_limit = to_tuple(brightness_limit)\n        self.contrast_limit = to_tuple(contrast_limit)\n        self.brightness_by_max = brightness_by_max"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply random brightness and contrast adjustments to an input image.\n\nThis method modifies the given image by scaling its brightness and contrast using provided alpha and beta values. This transformation helps introduce variability into the dataset, allowing downstream models to become more robust to color and illumination changes.\n\nArgs:\n    img (numpy.ndarray): Input image to be transformed.\n    alpha (float): Factor to control the contrast of the image.\n    beta (float): Factor to control the brightness of the image.\n\nReturns:\n    numpy.ndarray: Transformed image with adjusted brightness and contrast.\n\"\"\"",
                    "source_code": "return F.brightness_contrast_adjust(img, alpha, beta, self.brightness_by_max)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate randomized parameters for image brightness and contrast augmentation within specified limits.\n\nThis method computes and returns random values for the alpha (contrast scaling) and beta (brightness shifting) parameters. By varying these parameters for each augmentation, the method introduces variability into the visual properties of images, which helps prevent overfitting and improves model robustness.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing:\n        - \"alpha\" (float): Randomized contrast adjustment factor.\n        - \"beta\" (float): Randomized brightness adjustment value.\n\"\"\"",
                    "source_code": "return {\n            \"alpha\": 1.0 + random.uniform(self.contrast_limit[0], self.contrast_limit[1]),\n            \"beta\": 0.0 + random.uniform(self.brightness_limit[0], self.brightness_limit[1]),\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the parameters required to initialize the transformation, enabling consistent tracking and reproducibility of transform configurations.\n\nReturns:\n    tuple: A tuple containing the names of initialization arguments ('brightness_limit', 'contrast_limit', 'brightness_by_max').\n\"\"\"",
                    "source_code": "return (\"brightness_limit\", \"contrast_limit\", \"brightness_by_max\")"
                }
            ],
            "name": "RandomBrightnessContrast",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the RandomBrightness transformation with specified parameters, restricting augmentation to brightness adjustment only, and issues a deprecation warning to guide users toward the recommended alternative.\n\nArgs:\n    limit (float, tuple of float): Maximum range for random brightness adjustment.\n    always_apply (bool): If True, apply the transform to every image.\n    p (float): Probability of applying the transform.\n\nReturns:\n    None\n\nRaises:\n    DeprecationWarning: Informs users that this class is deprecated and suggests using RandomBrightnessContrast instead.\n\nWhy:\n    The method sets up the transformation to allow only controlled brightness changes, ensuring backward compatibility and gently transitioning users to preferred augmentation patterns by notifying them about the deprecation.\n\"\"\"",
                    "source_code": "super(RandomBrightness, self).__init__(\n            brightness_limit=limit, contrast_limit=0, always_apply=always_apply, p=p\n        )\n        warnings.warn(\"This class has been deprecated. Please use RandomBrightnessContrast\", DeprecationWarning)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args",
                    "second_doc": "\"\"\"\nCollects and returns the initialization arguments needed to serialize or reproduce the state of the RandomBrightness transformation.\n\nReturns:\n    dict: A dictionary containing the brightness change limit used to configure the transformation.\n\"\"\"",
                    "source_code": "return {\"limit\": self.brightness_limit}"
                }
            ],
            "name": "RandomBrightness",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the RandomContrast transformation with specified parameters and emits a deprecation warning, guiding users toward newer alternatives.  \nArgs:  \n    limit (float or tuple of float): Range for contrast adjustment.  \n    always_apply (bool, optional): If True, apply the transformation to every image.  \n    p (float, optional): Probability of applying the transformation.  \nReturns:  \n    None\n\nWhy:  \nThis method is used to configure contrast augmentation for images, helping to introduce variability during training and encouraging models to be more robust to changes in image contrast. The deprecation warning ensures users are informed about preferred and updated practices.\n\"\"\"",
                    "source_code": "super(RandomContrast, self).__init__(brightness_limit=0, contrast_limit=limit, always_apply=always_apply, p=p)\n        warnings.warn(\"This class has been deprecated. Please use RandomBrightnessContrast\", DeprecationWarning)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args",
                    "second_doc": "\"\"\"\nRetrieve the initialization parameters required to recreate the random contrast transformation.\n\nThis method is used to serialize the key arguments that govern how contrast adjustments are applied, enabling reproducibility and consistency when saving or reconstructing augmentation pipelines.\n\nReturns:\n    dict: A dictionary containing the contrast limit parameter used for this transformation.\n\"\"\"",
                    "source_code": "return {\"limit\": self.contrast_limit}"
                }
            ],
            "name": "RandomContrast",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Blur transformation with specified probability and blur intensity range.\n\nThis method sets up the parameters needed to control the probability of applying the blur effect and the range of blur intensities, making the transformation configurable for various use cases.\n\nArgs:\n    always_apply (bool): If set to True, the transformation will be applied to every image.\n    p (float): Probability of applying the transformation.\n    blur_limit (int or tuple of int): Maximum kernel size for the blur operation. Can be a single integer or a tuple specifying the lower and upper bounds.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Blur, self).__init__(always_apply, p)\n        self.blur_limit = to_tuple(blur_limit, 3)"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies a blurring effect to the input image using a specified kernel size.\n\nThis method is used to reduce noise and details in an image, which can help introduce variability and robustness to machine learning model training by simulating real-world imperfections.\n\nArgs:\n    image (numpy.ndarray): The input image to be blurred.\n    ksize (int or tuple of int): Size of the kernel used for blurring.\n\nReturns:\n    numpy.ndarray: The blurred image.\n\"\"\"",
                    "source_code": "return F.blur(image, ksize)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nRandomly selects a kernel size for the blur operation within the specified blur limits.\n\nThis method introduces randomness in the blur transformation, which helps create varied augmented images for more robust model training.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing a randomly chosen odd-valued 'ksize' (kernel size) for blurring.\n\"\"\"",
                    "source_code": "return {\"ksize\": random.choice(np.arange(self.blur_limit[0], self.blur_limit[1] + 1, 2))}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nProvides the names of the arguments required to initialize the blur transformation. This allows for consistent saving and reconstruction of transformation configurations when composing augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the string names of the initialization arguments for the blur operation (e.g., (\"blur_limit\",)).\n\"\"\"",
                    "source_code": "return (\"blur_limit\",)"
                }
            ],
            "name": "Blur",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies a motion blur effect to the input image using the specified kernel.  \nThis transformation simulates realistic artifacts caused by camera or object movement, thereby increasing the diversity and robustness of the training dataset for computer vision models.\n\nArgs:\n    img (numpy.ndarray): Input image on which to apply the motion blur effect.\n    kernel (numpy.ndarray): Convolution kernel that defines the direction and intensity of the blur.\n\nReturns:\n    numpy.ndarray: Image with motion blur applied.\n\"\"\"",
                    "source_code": "return F.motion_blur(img, kernel=kernel)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerates parameters required for simulating realistic motion blur effects by creating a random motion blur kernel. This helps in introducing variability and complexity into image data, which can aid in developing more robust computer vision models.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing a randomly generated motion blur kernel under the key \"kernel\".\n\"\"\"",
                    "source_code": "ksize = random.choice(np.arange(self.blur_limit[0], self.blur_limit[1] + 1, 2))\n        assert ksize > 2\n        kernel = np.zeros((ksize, ksize), dtype=np.uint8)\n        xs, xe = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n        if xs == xe:\n            ys, ye = random.sample(range(ksize), 2)\n        else:\n            ys, ye = random.randint(0, ksize - 1), random.randint(0, ksize - 1)\n        cv2.line(kernel, (xs, ys), (xe, ye), 1, thickness=1)\n        return {\"kernel\": kernel}"
                }
            ],
            "name": "MotionBlur",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the MedianBlur transformation with specified parameters, preparing it for use within image preprocessing pipelines to reduce noise and preserve edges.\n\nArgs:\n    blur_limit (int, tuple of int): Maximum size of the kernel used for blurring. Can be a single integer or a tuple specifying a range.\n    always_apply (bool): If True, the transformation is always applied. Default is False.\n    p (float): Probability of applying the transformation. Default is 0.5.\n\nReturns:\n    None\n\nWhy:\n    Configuring initialization parameters at this stage ensures the transformation is properly set up to contribute to data diversity during model training, which is essential for improving robustness and generalization in computer vision tasks.\n\"\"\"",
                    "source_code": "super(MedianBlur, self).__init__(blur_limit, always_apply, p)"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply a median blur filter to the input image to reduce noise while preserving edges.\n\nThis transformation is used to enhance image quality and maintain important structural information, supporting downstream visual recognition tasks.\n\nArgs:\n    image (numpy.ndarray): Input image to be processed.\n    ksize (int): Size of the kernel to be used for the median blur operation.\n\nReturns:\n    numpy.ndarray: Image after applying the median blur filter.\n\"\"\"",
                    "source_code": "return F.median_blur(image, ksize)"
                }
            ],
            "name": "MedianBlur",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the GaussianBlur transformation by configuring blur parameters and application probability. This setup ensures the transformation can be consistently applied within augmentation pipelines to control the sharpness of images.\n\nArgs:\n    blur_limit: The range for the Gaussian kernel size used to determine the level of blurring.\n    always_apply: If True, the transformation is always applied; otherwise, it is applied randomly according to the probability p.\n    p: The probability of applying the transformation.\n\nReturns:\n    None\n\nThe method establishes the necessary configuration so that Gaussian blurring can be selectively and reproducibly applied to input images, helping to generate training data with varied sharpness to improve model robustness.\n\"\"\"",
                    "source_code": "super(GaussianBlur, self).__init__(blur_limit, always_apply, p)"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies a Gaussian blur to the input image using the specified kernel size. This smoothing operation helps reduce image noise and detail, which can improve robustness and generalization in subsequent computer vision tasks.\n\nArgs:\n    image (numpy.ndarray): Input image to be processed.\n    ksize (tuple[int, int]): Size of the Gaussian kernel.\n\nReturns:\n    numpy.ndarray: The blurred image.\n\"\"\"",
                    "source_code": "return F.gaussian_blur(image, ksize)"
                }
            ],
            "name": "GaussianBlur",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the GaussNoise transformation by setting the mean and variance limits for the Gaussian noise to be applied. Ensures the specified variance limits are non-negative, either as a tuple defining the range or as a single value for the upper limit, promoting consistent application of noise perturbations to images during augmentation.\n\nArgs:\n    var_limit (tuple or float or int): Range for the variance of the Gaussian noise. Should be non-negative; can be specified as a tuple (min, max) or a non-negative number as the upper limit with 0 as the lower limit.\n    mean (float): Mean value of the Gaussian noise.\n    always_apply (bool, optional): If True, always apply the transformation. Default is False.\n    p (float, optional): Probability of applying the transformation. Default is 0.5.\n\nRaises:\n    ValueError: If any value in var_limit is negative.\n\"\"\"",
                    "source_code": "super(GaussNoise, self).__init__(always_apply, p)\n        if isinstance(var_limit, tuple):\n            if var_limit[0] < 0:\n                raise ValueError(\"Lower var_limit should be non negative.\")\n            if var_limit[1] < 0:\n                raise ValueError(\"Upper var_limit should be non negative.\")\n            self.var_limit = var_limit\n        elif isinstance(var_limit, (int, float)):\n            if var_limit < 0:\n                raise ValueError(\" var_limit should be non negative.\")\n\n            self.var_limit = (0, var_limit)\n\n        self.mean = mean"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies Gaussian noise to the input image to simulate sensor or environmental variations, helping models become more robust to noisy data during training.\n\nArgs:\n    img (numpy.ndarray): Input image to which Gaussian noise will be added.\n    gauss (float or array-like): Gaussian noise parameter(s) that define the noise to be applied.\n\nReturns:\n    numpy.ndarray: Image with added Gaussian noise.\n\"\"\"",
                    "source_code": "return F.gauss_noise(img, gauss=gauss)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerate a dictionary containing Gaussian noise parameters based on the input image.\n\nThis method creates a Gaussian noise array shaped to match the input image, with randomly sampled variance and mean values. The randomness ensures variability in the added noise for each call, helping to simulate diverse imaging conditions during augmentation.\n\nArgs:\n    params (dict): Dictionary containing the \"image\" key with a NumPy array value to augment.\n\nReturns:\n    dict: Dictionary with a \"gauss\" key corresponding to the generated Gaussian noise array.\n\"\"\"",
                    "source_code": "image = params[\"image\"]\n        var = random.uniform(self.var_limit[0], self.var_limit[1])\n        sigma = var ** 0.5\n        random_state = np.random.RandomState(random.randint(0, 2 ** 32 - 1))\n\n        gauss = random_state.normal(self.mean, sigma, image.shape)\n        return {\"gauss\": gauss}"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which input data fields are relevant for the application of Gaussian noise.\n\nReturns:\n    list: A list containing \"image\", indicating that the Gaussian noise operation is applied to image data. This ensures only target fields suitable for this transformation are affected within augmentation pipelines.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of arguments required to initialize this transformation, allowing for consistent serialization and reproducibility of the augmentation configuration.\n\nReturns:\n    tuple: A tuple containing the names of the required initialization arguments for this transformation.\n\"\"\"",
                    "source_code": "return (\"var_limit\",)"
                }
            ],
            "name": "GaussNoise",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an ISONoise image augmentation transformation, setting its intensity and color shift parameters.\n\nThis method sets up the ISONoise transformation so that it can later inject realistic camera sensor noise into images, making model training more robust to real-world visual artifacts.\n\nArgs:\n    always_apply (bool): Whether to always apply this transformation.\n    p (float): Probability of applying the transformation.\n    intensity (float): Level of noise intensity to apply.\n    color_shift (float): Degree of color shift present in the simulated noise.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ISONoise, self).__init__(always_apply, p)\n        self.intensity = intensity\n        self.color_shift = color_shift"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply ISO noise augmentation to an input image to simulate the effect of camera sensor noise. This is used to increase the robustness of computer vision models by exposing them to a broader variety of image artifacts during training.\n\nArgs:\n    img (numpy.ndarray): Input image to which the noise will be applied.\n    color_shift (float): Intensity of color shift in the noise.\n    intensity (float): Overall strength of the ISO noise to be added.\n    random_state (int or None): Random seed for reproducibility.\n\nReturns:\n    numpy.ndarray: Augmented image with ISO noise applied.\n\"\"\"",
                    "source_code": "return F.iso_noise(img, color_shift, intensity, np.random.RandomState(random_state))"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerates randomized parameters used for applying ISO noise adjustments to an image. This method ensures that each augmentation instance introduces different characteristics in terms of color shift, intensity, and stochasticity, increasing the diversity and unpredictability of augmented data.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing:\n        - color_shift (float): Random value within the specified color shift range.\n        - intensity (float): Random value within the specified intensity range.\n        - random_state (int): Random integer seed for reproducibility of random noise.\n\"\"\"",
                    "source_code": "return {\n            \"color_shift\": random.uniform(self.color_shift[0], self.color_shift[1]),\n            \"intensity\": random.uniform(self.intensity[0], self.intensity[1]),\n            \"random_state\": random.randint(0, 65536),\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nSpecifies the names of the keyword arguments required to initialize this transformation.\n\nReturns:\n    tuple: A tuple containing the names of arguments (\"intensity\", \"color_shift\") that are necessary for correctly configuring and reconstructing this transformation, which enables consistent augmentation behavior and serialization.\n\"\"\"",
                    "source_code": "return (\"intensity\", \"color_shift\")"
                }
            ],
            "name": "ISONoise",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the CLAHE (Contrast Limited Adaptive Histogram Equalization) transformation by setting parameters that control the contrast enhancement process and the tiling of the input image. This helps promote more uniform brightness and detail throughout the image, which can benefit subsequent image processing steps.\n\nArgs:\n    always_apply (bool): Whether to always apply this transformation.\n    p (float): Probability of applying the transformation.\n    clip_limit (float or tuple of float): Threshold for contrast limiting.\n    tile_grid_size (int or tuple of int): Defines the size of the grid for the equalization.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(CLAHE, self).__init__(always_apply, p)\n        self.clip_limit = to_tuple(clip_limit, 1)\n        self.tile_grid_size = tuple(tile_grid_size)"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply Contrast Limited Adaptive Histogram Equalization (CLAHE) to the input image to enhance local contrast and bring out fine details, which can aid downstream processing and analysis.\n\nArgs:\n    img (numpy.ndarray): Input image on which CLAHE will be applied.\n    clip_limit (float): Threshold for contrast limiting.\n    \nReturns:\n    numpy.ndarray: Image after CLAHE has been applied.\n\"\"\"",
                    "source_code": "return F.clahe(img, clip_limit, self.tile_grid_size)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate random parameters for the CLAHE transformation to introduce variability in the augmentation process.\n\nThis method samples a value for the clip limit parameter within the specified range, helping to create diverse training examples and reduce model overfitting.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing a randomly selected 'clip_limit' value within the given range.\n\"\"\"",
                    "source_code": "return {\"clip_limit\": random.uniform(self.clip_limit[0], self.clip_limit[1])}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments for the CLAHE transformation.\n\nThis method enables serialization and reproducibility of image augmentation pipelines by specifying which parameters are essential for reconstructing the transformation.\n\nReturns:\n    tuple: A tuple containing the names of the initialization arguments ('clip_limit', 'tile_grid_size').\n\"\"\"",
                    "source_code": "return (\"clip_limit\", \"tile_grid_size\")"
                }
            ],
            "name": "CLAHE",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a ChannelDropout transformation that randomly drops a range of channels from input images.\n\nThis method sets the range of channels that may be zeroed out and the value to be used for filling the dropped channels, enabling the simulation of missing or corrupted data channels during augmentation. This encourages the model to become more robust to incomplete or noisy channel information during training.\n\nArgs:\n    always_apply (bool): Whether to always apply this transformation.\n    p (float): Probability of applying the transform.\n    channel_drop_range (tuple): A tuple (min_channels, max_channels) specifying the minimum and maximum number of channels to be dropped.\n    fill_value (float or int): The value with which to fill the dropped channels.\n\nRaises:\n    AssertionError: If channel_drop_range does not specify at least one channel and min is not less than or equal to max.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ChannelDropout, self).__init__(always_apply, p)\n\n        self.channel_drop_range = channel_drop_range\n\n        self.min_channels = channel_drop_range[0]\n        self.max_channels = channel_drop_range[1]\n\n        assert 1 <= self.min_channels <= self.max_channels\n\n        self.fill_value = fill_value"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nRandomly masks out selected channels from the input image tensor by setting their values to a predefined fill value.\n\nThis method simulates the omission of information in certain color or feature channels, which encourages models to learn more robust and generalized representations by preventing reliance on any single channel.\n\nArgs:\n    img (Tensor): The input image tensor.\n    channels_to_drop (list[int]): Indices of channels to be masked (dropped out).\n\nReturns:\n    Tensor: The image tensor with specified channels zeroed out or set to the configured fill value.\n\"\"\"",
                    "source_code": "return F.channel_dropout(img, channels_to_drop, self.fill_value)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nDetermine which channels from the input image should be randomly dropped to simulate channel-level data loss during augmentation, increasing robustness of models to missing or corrupted channel information.\n\nArgs:\n    params (dict): Dictionary containing input data, expected to include the key \"image\" with a NumPy array representing the image.\n\nReturns:\n    dict: A dictionary with the key \"channels_to_drop\" containing a list of randomly selected channel indices to be dropped.\n\nRaises:\n    NotImplementedError: If the image is single-channel, as channel dropout is undefined for such cases.\n    ValueError: If the max_channels parameter permits dropping all available channels, which is not allowed.\n\"\"\"",
                    "source_code": "img = params[\"image\"]\n\n        num_channels = img.shape[-1]\n\n        if len(img.shape) == 2 or num_channels == 1:\n            raise NotImplementedError(\"Images has one channel. ChannelDropout is not defined.\")\n\n        if self.max_channels >= num_channels:\n            raise ValueError(\"Can not drop all channels in ChannelDropout.\")\n\n        num_drop_channels = random.randint(self.min_channels, self.max_channels)\n\n        channels_to_drop = random.sample(range(num_channels), k=num_drop_channels)\n\n        return {\"channels_to_drop\": channels_to_drop}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the arguments required to initialize the ChannelDropout transformation. This enables consistent serialization and deserialization of transformation parameters, supporting reproducibility and easy configuration adjustments in augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the names of the initialization arguments (\"channel_drop_range\", \"fill_value\").\n\"\"\"",
                    "source_code": "return (\"channel_drop_range\", \"fill_value\")"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which data fields should be treated as parameters for the transformation, enabling consistent application of augmentations to images. \n\nReturns:\n    list: A list containing the string \"image\", indicating that the transformation operates on image data.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                }
            ],
            "name": "ChannelDropout",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nSpecifies which data fields should be treated as parameters for the transformation.\n\nReturns:\n    list: A list containing the names of the targets this transformation operates on, in this case ['image'].\n    \nWhy:\n    This ensures that the transformation is correctly applied only to the intended data fields during augmentation, maintaining the integrity and consistency of the data processing pipeline.\n\"\"\"",
                    "source_code": "return [\"image\"]"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nShuffle the channels of the input image according to the specified order.\n\nThis method enables the application of channel shuffling as a form of transformation, which can increase data diversity and prevent models from relying too heavily on specific channel correlations, ultimately fostering more robust feature learning.\n\nArgs:\n    img (numpy.ndarray): Input image array.\n    channels_shuffled (tuple or list): Sequence representing the desired channel order after shuffling.\n\nReturns:\n    numpy.ndarray: Image with channels rearranged in the specified order.\n\"\"\"",
                    "source_code": "return F.channel_shuffle(img, channels_shuffled)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nGenerate a randomized channel order for an input image to facilitate the shuffling of color channels as part of data augmentation.\n\nArgs:\n    params (dict): Dictionary containing the input data, where 'image' is expected to be a NumPy array of shape (H, W, C).\n    \nReturns:\n    dict: A dictionary with the key 'channels_shuffled' containing a list representing the new, randomly shuffled order of image channels.\n\nThis method enhances the variability of image data by altering the arrangement of image channels, which can help models become invariant to channel order during training.\n\"\"\"",
                    "source_code": "img = params[\"image\"]\n        ch_arr = list(range(img.shape[2]))\n        random.shuffle(ch_arr)\n        return {\"channels_shuffled\": ch_arr}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the parameters required to initialize the ChannelShuffle transformation.\n\nThis method is used to facilitate consistency and reproducibility when saving, loading, or reconstructing an augmentation pipeline configuration. In this case, since ChannelShuffle does not require initialization parameters, it returns an empty tuple.\n\nReturns:\n    tuple: An empty tuple, indicating no initialization arguments are needed for this transform.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "ChannelShuffle",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply a pixel-wise inversion to the input image, reversing the intensity values.\n\nThis method alters image appearance by flipping the pixel values, which introduces valuable diversity in the training data. Such transformation helps models become more robust to changes in image characteristics and reduces overfitting.\n\nArgs:\n    img (numpy.ndarray or PIL.Image): Input image to be inverted.\n\nReturns:\n    numpy.ndarray or PIL.Image: The resulting image with inverted pixel values.\n\"\"\"",
                    "source_code": "return F.invert(img)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns a tuple containing the names of initialization arguments for the transformation. \nThis is used for reconstructing or serializing the transformation instance, ensuring consistent augmentation behavior.\n\nReturns:\n    tuple: An empty tuple, as this transformation does not require any initialization arguments.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "InvertImg",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the RandomGamma augmentation with specified parameters to control the intensity of gamma correction applied to images, helping introduce photometric variability to the dataset.\n\nArgs:\n    always_apply (bool): If True, the transform will always be applied.\n    p (float): Probability of applying the transform.\n    gamma_limit (float or tuple of float): Range for selecting the gamma correction factor.\n    eps (float): Small constant to avoid division by zero.\n\nReturns:\n    None\n\nWhy:\n    Adjusting the gamma values of images simulates varying lighting conditions and contrast, which increases the diversity of training samples and promotes model robustness to changes in image illumination.\n\"\"\"",
                    "source_code": "super(RandomGamma, self).__init__(always_apply, p)\n        self.gamma_limit = to_tuple(gamma_limit)\n        self.eps = eps"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies gamma correction to the input image using the specified gamma value and an epsilon parameter for numerical stability.\n\nThis transformation adjusts the brightness of the image in a non-linear way, which helps models learn to be invariant to lighting conditions encountered in real-world visual environments.\n\nArgs:\n    img (numpy.ndarray): Input image to be transformed.\n    gamma (float): Gamma value to be applied for correction.\n\nReturns:\n    numpy.ndarray: Gamma-corrected version of the input image.\n\"\"\"",
                    "source_code": "return F.gamma_transform(img, gamma=gamma, eps=self.eps)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate a random gamma value within the specified limits for use in gamma transformation.\n\nThis method selects a gamma value by generating a random integer in the configured gamma range, then normalizes it for use in image augmentation. The randomness helps introduce diversity in the augmentation process, ensuring that the model is exposed to varied illumination conditions.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing a single key 'gamma' with a randomly chosen float value within the specified limits.\n\"\"\"",
                    "source_code": "return {\"gamma\": random.randint(self.gamma_limit[0], self.gamma_limit[1]) / 100.0}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments for the RandomGamma transformation, which are essential for tracking and serializing the parameters that control gamma adjustment.\n\nReturns:\n    tuple: A tuple containing the names of the initialization arguments ('gamma_limit', 'eps').\n\"\"\"",
                    "source_code": "return (\"gamma_limit\", \"eps\")"
                }
            ],
            "name": "RandomGamma",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nConverts the input image to grayscale using a predefined function.\n\nThis method facilitates consistent preprocessing steps as part of an image transformation pipeline, ensuring standardized input for downstream computer vision tasks.\n\nArgs:\n    img (numpy.ndarray or PIL.Image): The input image to be converted to grayscale.\n\nReturns:\n    numpy.ndarray or PIL.Image: The grayscale version of the input image.\n\"\"\"",
                    "source_code": "return F.to_gray(img)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments for this transformation. As this transformation does not require any user-specified initialization arguments, an empty tuple is returned.\n\nThis method facilitates serialization, composition, and consistent application of transformations within augmentation pipelines.\n\nReturns:\n    tuple: An empty tuple, indicating that no initialization arguments are needed for this transformation.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "ToGray",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitialize the ToFloat transformation by setting whether the transformation should always be applied, its probability, and the maximum allowed value for casting to float.\n\nArgs:\n    always_apply (bool): If True, the transformation will be applied to every input.\n    p (float): Probability of applying the transformation.\n    max_value (float or None): Maximum value for scaling during the conversion to float.\n\nReturns:\n    None\n\nWhy:\n    By allowing images to be converted and scaled to floating-point values during preprocessing, this method helps adapt image data to formats commonly required by deep learning models, facilitating consistent and numerically stable model training.\n\"\"\"",
                    "source_code": "super(ToFloat, self).__init__(always_apply, p)\n        self.max_value = max_value"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nConvert the input image to floating point representation for further processing.\n\nArgs:\n    img: Input image to be converted. Can be a NumPy array or similar image representation.\n\nReturns:\n    Image converted to a floating-point format with values scaled according to `self.max_value`.\n\nWhy:\n    Ensures consistent numeric format and value range needed for applying subsequent image transformations and augmentations effectively.\n\"\"\"",
                    "source_code": "return F.to_float(img, self.max_value)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to configure the transformation.\n\nThis allows for consistent serialization and deserialization of transformation parameters across pipelines, ensuring reproducibility and flexibility in defining augmentation workflows.\n\nReturns:\n    tuple: A tuple containing the names of the initialization arguments for the transform, in this case ('max_value',).\n\"\"\"",
                    "source_code": "return (\"max_value\",)"
                }
            ],
            "name": "ToFloat",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the FromFloat transformation, setting the target data type and maximum value for subsequent image processing steps.\n\nThis constructor is designed to prepare the transformation by specifying how floating-point image data will be converted, ensuring consistency and control over the data type and value scaling throughout the augmentation pipeline.\n\nArgs:\n    always_apply (bool): If True, the transformation will always be applied. Defaults to False.\n    p (float): Probability of applying the transformation. Defaults to 0.5.\n    dtype (str or np.dtype): The target NumPy data type to which the image will be converted.\n    max_value (float or int): The maximum value used for scaling the image data during conversion.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(FromFloat, self).__init__(always_apply, p)\n        self.dtype = np.dtype(dtype)\n        self.max_value = max_value"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nConverts a floating-point image array to a specified data type and maximum value, facilitating compatibility with downstream image processing or model requirements.\n\nArgs:\n    img (np.ndarray): Input image array in floating-point format.\n\nReturns:\n    np.ndarray: Image array converted to the target data type and scaled to the given maximum value.\n\"\"\"",
                    "source_code": "return F.from_float(img, self.dtype, self.max_value)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args",
                    "second_doc": "\"\"\"\nCollects and returns the arguments needed to re-initialize the transformation, enabling reproducibility and consistent application when serializing or re-creating pipelines.\n\nReturns:\n    dict: Dictionary containing the data type ('dtype') and maximum value ('max_value') used by the transformation.\n\"\"\"",
                    "source_code": "return {\"dtype\": self.dtype.name, \"max_value\": self.max_value}"
                }
            ],
            "name": "FromFloat",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Downscale transformation with specified parameters, ensuring valid scaling range and interpolation method. This allows for controlled downscaling of images as part of a transformation pipeline, which helps simulate images of lower resolution to increase dataset variability and promote model robustness.\n\nArgs:\n    always_apply (bool): If True, the transformation will always be applied.\n    p (float): Probability of applying the transformation.\n    scale_min (float): The minimum scaling factor; must be \u2264 scale_max and < 1.\n    scale_max (float): The maximum scaling factor; must be \u2265 scale_min and < 1.\n    interpolation (int): Interpolation method used for resizing.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Downscale, self).__init__(always_apply, p)\n        assert scale_min <= scale_max, \"Expected scale_min be less or equal scale_max, got {} {}\".format(\n            scale_min, scale_max\n        )\n        assert scale_max < 1, \"Expected scale_max to be less than 1, got {}\".format(scale_max)\n        self.scale_min = scale_min\n        self.scale_max = scale_max\n        self.interpolation = interpolation"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply a downscaling operation to the input image, reducing its dimensions according to the specified scaling factor and interpolation method.\n\nThis transformation is used to simulate lower-resolution images, which helps models become robust to variations in image quality and resolution during training.\n\nArgs:\n    image (numpy.ndarray): The input image to be downscaled.\n    scale (float): The scaling factor to reduce the image size.\n    interpolation (int): Interpolation method used for resizing.\n\nReturns:\n    numpy.ndarray: The downscaled image.\n\"\"\"",
                    "source_code": "return F.downscale(image, scale=scale, interpolation=interpolation)"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nGenerate parameters for the downscaling transformation by randomly sampling a scale factor within a specified range and specifying the interpolation method to use.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing a randomly selected 'scale' value within [self.scale_min, self.scale_max] and the 'interpolation' method. This ensures each augmentation instance introduces controlled variability, supporting more diverse training data.\n\"\"\"",
                    "source_code": "return {\"scale\": np.random.uniform(self.scale_min, self.scale_max), \"interpolation\": self.interpolation}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of initialization arguments required to configure the transformation.\n\nThis method facilitates the serialization, deserialization, and reproducibility of the transformation by specifying which parameters are essential for initialization.\n\nReturns:\n    tuple: A tuple containing the names of required initialization arguments ('scale_min', 'scale_max', 'interpolation').\n\"\"\"",
                    "source_code": "return \"scale_min\", \"scale_max\", \"interpolation\""
                }
            ],
            "name": "Downscale",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Lambda transformation with optional custom functions for different data targets (image, mask, keypoint, bbox). This allows users to specify custom augmentation logic per data type, making the transformation highly flexible for varied tasks and data formats.\n\nArgs:\n    image (callable, optional): Custom function to apply to image data.\n    mask (callable, optional): Custom function to apply to mask data.\n    keypoint (callable, optional): Custom function to apply to keypoint data.\n    bbox (callable, optional): Custom function to apply to bounding box data.\n    always_apply (bool, optional): If True, always apply this transformation. Default is False.\n    p (float, optional): Probability of applying the transformation. Default is 0.5.\n    name (str, optional): Name for this transformation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Lambda, self).__init__(always_apply, p)\n\n        self.name = name\n        self.custom_apply_fns = {target_name: F.noop for target_name in (\"image\", \"mask\", \"keypoint\", \"bbox\")}\n        for target_name, custom_apply_fn in {\"image\": image, \"mask\": mask, \"keypoint\": keypoint, \"bbox\": bbox}.items():\n            if custom_apply_fn is not None:\n                if isinstance(custom_apply_fn, LambdaType):\n                    warnings.warn(\n                        \"Using lambda is incompatible with multiprocessing. \"\n                        \"Consider using regular functions or partial().\"\n                    )\n\n                self.custom_apply_fns[target_name] = custom_apply_fn"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply a user-defined image transformation function with provided parameters.\n\nThis method allows flexible integration of custom operations into the image processing pipeline.\n\nArgs:\n    img (numpy.ndarray): Input image to be transformed.\n    **params: Additional parameters to pass to the custom transformation function.\n\nReturns:\n    numpy.ndarray: The transformed image after applying the custom function.\n\"\"\"",
                    "source_code": "fn = self.custom_apply_fns[\"image\"]\n        return fn(img, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nApply a custom transformation function to the provided mask using user-defined parameters.\n\nArgs:\n    mask (numpy.ndarray): The mask on which the transformation will be applied.\n    **params: Arbitrary keyword arguments required by the custom function.\n\nReturns:\n    numpy.ndarray: The transformed mask.\n\nThis method allows flexible adjustment or augmentation of mask data, enabling tailored pre-processing steps needed for various computer vision tasks.\n\"\"\"",
                    "source_code": "fn = self.custom_apply_fns[\"mask\"]\n        return fn(mask, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nApplies a user-defined transformation function to a bounding box using specified parameters.\n\nThis method enables flexible handling and augmentation of bounding box data, ensuring custom modifications can be incorporated into processing pipelines as required.\n\nArgs:\n    bbox (tuple or list): The bounding box to be transformed.\n    **params: Additional parameters to be passed to the custom transformation function.\n\nReturns:\n    The transformed bounding box, after applying the user-specified function.\n\"\"\"",
                    "source_code": "fn = self.custom_apply_fns[\"bbox\"]\n        return fn(bbox, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nApplies a user-defined transformation function to a keypoint using provided parameters.\n\nThis method enables the flexible modification of keypoints by delegating the transformation logic to a custom function, allowing users to adapt keypoint processing to specific requirements or tasks.\n\nArgs:\n    keypoint: The target keypoint to be transformed.\n    **params: Additional parameters required by the custom transformation function.\n\nReturns:\n    The transformed keypoint after applying the user-provided function.\n\"\"\"",
                    "source_code": "fn = self.custom_apply_fns[\"keypoint\"]\n        return fn(keypoint, **params)"
                },
                {
                    "docstring": null,
                    "method_name": "_to_dict",
                    "second_doc": "\"\"\"\nSerialize the Lambda transform instance to a dictionary representation, ensuring it is uniquely identifiable by its provided name. This supports reproducibility and consistent handling during pipeline saving and loading.\n\nRaises:\n    ValueError: If the transform's name is not set, as serialization requires a unique identifier.\n\nReturns:\n    dict: A dictionary containing the type and name of the Lambda transform instance.\n\"\"\"",
                    "source_code": "if self.name is None:\n            raise ValueError(\n                \"To make a Lambda transform serializable you should provide the `name` argument, \"\n                \"e.g. `Lambda(name='my_transform', image=<some func>, ...)`.\"\n            )\n        return {\"__type__\": \"Lambda\", \"__name__\": self.name}"
                },
                {
                    "docstring": null,
                    "method_name": "__repr__",
                    "second_doc": "\"\"\"\nReturns a string representation of the Lambda transformation with its key parameters and settings. This method assembles a human-readable summary of the current object's initialization state, useful for debugging and for understanding the configuration of a transformation component.\n\nArgs:\n    None\n\nReturns:\n    str: A string summarizing the class name and all relevant arguments used to initialize the object.\n\"\"\"",
                    "source_code": "state = {\"name\": self.name}\n        state.update(self.custom_apply_fns.items())\n        state.update(self.get_base_init_args())\n        return \"{name}({args})\".format(name=self.__class__.__name__, args=format_args(state))"
                }
            ],
            "name": "Lambda",
            "type": "class"
        }
    ],
    "albumentations/albumentations/core/__init__.py": [],
    "albumentations/albumentations/core/composition.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Transforms class by setting up a sequence of image transformations and determining the positions of specific dual transformations within the sequence.\n\nArgs:\n    transforms (list): A list or sequence of transformation objects to be applied to images.\n\nReturns:\n    None\n\nWhy:\n    This setup is necessary to efficiently organize and reference transformation steps\u2014especially paired or \"dual\" transforms\u2014enabling advanced augmentation workflows and easier access for pipeline processing or conditional logic.\n\"\"\"",
                    "source_code": "self.transforms = transforms\n        self.start_end = self._find_dual_start_end(transforms)"
                },
                {
                    "docstring": null,
                    "method_name": "_find_dual_start_end",
                    "second_doc": "\"\"\"\nIdentify the indices of the first and last occurrence of dual-image transforms within a sequence, including those within nested compositions.\n\nArgs:\n    transforms (list): A list of transform objects which may include DualTransform instances and/or nested BaseCompose objects.\n\nReturns:\n    list or None: A list containing the indices [start, end] of the first and last DualTransform found, or None if no such transforms are present.\n\nWhy:\n    Determining the span of dual-image transformations is important to ensure proper alignment and consistency of paired augmentations, especially when managing complex, nested transformation pipelines.\n\"\"\"",
                    "source_code": "dual_start_end = None\n        last_dual = None\n        for idx, transform in enumerate(transforms):\n            if isinstance(transform, DualTransform):\n                last_dual = idx\n                if dual_start_end is None:\n                    dual_start_end = [idx]\n            if isinstance(transform, BaseCompose):\n                inside = self._find_dual_start_end(transform)\n                if inside is not None:\n                    last_dual = idx\n                    if dual_start_end is None:\n                        dual_start_end = [idx]\n        if dual_start_end is not None:\n            dual_start_end.append(last_dual)\n        return dual_start_end"
                },
                {
                    "docstring": null,
                    "method_name": "get_always_apply",
                    "second_doc": "\"\"\"\nExtracts and returns a new Transforms object containing only those transforms marked to always be applied, including those nested within composite transformations.\n\nThis method enables users to isolate and consistently apply the essential transformations in a pipeline, ensuring certain critical operations persist across all augmentation scenarios.\n\nArgs:\n    transforms (list): A list of transform objects, which may include nested composite transforms.\n\nReturns:\n    Transforms: A new Transforms object comprised solely of transforms where always_apply is set to True.\n\"\"\"",
                    "source_code": "new_transforms = []\n        for transform in transforms:\n            if isinstance(transform, BaseCompose):\n                new_transforms.extend(self.get_always_apply(transform))\n            elif transform.always_apply:\n                new_transforms.append(transform)\n        return Transforms(new_transforms)"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieve a specific transformation from the collection of image transforms by its index or key.\n\nThis allows users to access and inspect individual transformations within an augmentation pipeline, facilitating customization and flexible pipeline composition.\n\nArgs:\n    item (int or str): Index or key identifying the transform to retrieve.\n\nReturns:\n    Callable: The transformation function or object corresponding to the specified key or index.\n\"\"\"",
                    "source_code": "return self.transforms[item]"
                }
            ],
            "name": "Transforms",
            "type": "class"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "set_always_apply",
                "second_doc": "\"\"\"\nEnsures that all provided transformation objects are applied unconditionally by setting their 'always_apply' attribute to True.\n\nThis is done to guarantee that every transform in the sequence is executed, which can be useful for achieving consistent preprocessing or augmentation behavior regardless of external randomness or conditions.\n\nArgs:\n    transforms (list): A list of transformation objects that possess an 'always_apply' attribute.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "for t in transforms:\n        t.always_apply = True"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the composition of multiple image transformation steps and controls their application probability and replay mode status.\n\nArgs:\n    transforms (list): List of image transformation operations to be composed together.\n    p (float): Probability with which the set of composed transforms will be applied.\n\nWhy:\n    By grouping and managing multiple transformations and their application logic, this method enables flexible, repeatable, and controllable image processing workflows, which is crucial for consistent experimentation and robust data augmentation strategies.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transforms = Transforms(transforms)\n        self.p = p\n\n        self.replay_mode = False\n        self.applied_in_replay = False"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieve a specific transformation from the composition by index, enabling flexible access and manipulation of individual augmentations within a transformation pipeline.\n\nArgs:\n    item (int): The index of the desired transformation within the pipeline.\n\nReturns:\n    Callable: The transformation function or object at the specified index.\n\"\"\"",
                    "source_code": "return self.transforms[item]"
                },
                {
                    "docstring": null,
                    "method_name": "__repr__",
                    "second_doc": "\"\"\"\nReturn a readable, indented string representation of the composition, which helps users quickly visualize and understand the sequence and structure of transformations within the pipeline.\n\nReturns:\n    str: An indented string outlining the nested structure of composed transformations.\n\"\"\"",
                    "source_code": "return self.indented_repr()"
                },
                {
                    "docstring": null,
                    "method_name": "indented_repr",
                    "second_doc": "\"\"\"\nGenerate a formatted string representation of the composition object and its nested transforms, showing the structure and important parameters in an indented and human-readable way.\n\nArgs:\n    indent (int): The number of spaces to use for each indentation level.\n\nReturns:\n    str: An indented string representation of the composition and its contents, useful for visualization and debugging.\n\nWhy:\n    This method helps users and developers quickly understand and inspect the hierarchy and configurations of transformation pipelines, aiding in debugging and verification of the composed transformations.\n\"\"\"",
                    "source_code": "args = {k: v for k, v in self._to_dict().items() if not (k.startswith(\"__\") or k == \"transforms\")}\n        repr_string = self.__class__.__name__ + \"([\"\n        for t in self.transforms:\n            repr_string += \"\\n\"\n            if hasattr(t, \"indented_repr\"):\n                t_repr = t.indented_repr(indent + REPR_INDENT_STEP)\n            else:\n                t_repr = repr(t)\n            repr_string += \" \" * indent + t_repr + \",\"\n        repr_string += \"\\n\" + \" \" * (indent - REPR_INDENT_STEP) + \"], {args})\".format(args=format_args(args))\n        return repr_string"
                },
                {
                    "docstring": null,
                    "method_name": "get_class_fullname",
                    "second_doc": "\"\"\"\nGet the fully qualified name of the class, including its module.\n\nThis is useful for uniquely identifying classes, facilitating debugging, logging, serialization, and dynamic instantiation of transformation pipelines.\n\nArgs:\n    cls (type): The class for which to obtain the fully qualified name.\n\nReturns:\n    str: The full module path and class name in the format 'module_name.ClassName'.\n\"\"\"",
                    "source_code": "return \"{cls.__module__}.{cls.__name__}\".format(cls=cls)"
                },
                {
                    "docstring": null,
                    "method_name": "_to_dict",
                    "second_doc": "\"\"\"\nSerialize the composition of image transformations into a dictionary format, including class identity, parameters, and a list of its contained transforms.\n\nThis method enables easy saving, sharing, and reconstruction of augmentation pipelines by representing them in a standardized data structure.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing the class name, probability parameter, and serialized representations of all child transforms.\n\"\"\"",
                    "source_code": "return {\n            \"__class_fullname__\": self.get_class_fullname(),\n            \"p\": self.p,\n            \"transforms\": [t._to_dict() for t in self.transforms],\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "get_dict_with_id",
                    "second_doc": "\"\"\"\nConstructs a dictionary containing serialization information about this transformation object, including its class name, a unique identifier, and the same information for all its sub-transformations.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary with the full class name, object ID, parameters (as None), and a list of dictionaries for each child transformation.\n    \nWhy:\n    This method creates a structured representation of a transformation pipeline, enabling inspection, reproducibility, and serialization of the chain of transformations and their identities.\n\"\"\"",
                    "source_code": "return {\n            \"__class_fullname__\": self.get_class_fullname(),\n            \"id\": id(self),\n            \"params\": None,\n            \"transforms\": [t.get_dict_with_id() for t in self.transforms],\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "add_targets",
                    "second_doc": "\"\"\"\nAdds specified additional targets to all transform objects within the composition pipeline. This ensures that each transformation is aware of and correctly processes the new data types or structures, maintaining consistency throughout complex augmentation workflows.\n\nArgs:\n    additional_targets (dict): A dictionary mapping new target names to base target types, indicating additional data structures requiring transformation (e.g., masks, keypoints).\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "if additional_targets:\n            for t in self.transforms:\n                t.add_targets(additional_targets)"
                },
                {
                    "docstring": null,
                    "method_name": "set_deterministic",
                    "second_doc": "\"\"\"\nSet the deterministic behavior for each transform in the sequence to ensure reproducible augmentation results.\n\nArgs:\n    flag (bool): If True, enables deterministic mode for all transforms; otherwise, disables it.\n    save_key (Optional[str]): Optional key to identify and store the deterministic state.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "for t in self.transforms:\n            t.set_deterministic(flag, save_key)"
                }
            ],
            "name": "BaseCompose",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a composition of image transformation operations, along with optional handling for bounding boxes and keypoints using provided parameters. This method sets up specialized processors to ensure transformations are consistently and correctly applied to all relevant data elements, including any user-defined additional targets.\n\nArgs:\n    transforms (list): A list of transformation operations to be applied in sequence. None values are ignored.\n    p (float, optional): Probability of applying the composition of transforms.\n    bbox_params (dict or BboxParams, optional): Parameters specifying how to handle bounding box data during transformations.\n    keypoint_params (dict or KeypointParams, optional): Parameters specifying how to handle keypoint data during transformations.\n    additional_targets (dict, optional): Additional data types (e.g., masks, other images) that should undergo the same transformations as the main input.\n\nRaises:\n    ValueError: If bbox_params or keypoint_params are provided in an unsupported format.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Compose, self).__init__([t for t in transforms if t is not None], p)\n\n        self.processors = {}\n        if bbox_params:\n            if isinstance(bbox_params, dict):\n                params = BboxParams(**bbox_params)\n            elif isinstance(bbox_params, BboxParams):\n                params = bbox_params\n            else:\n                raise ValueError(\"unknown format of bbox_params, please use `dict` or `BboxParams`\")\n            self.processors[\"bboxes\"] = BboxProcessor(params, additional_targets)\n\n        if keypoint_params:\n            if isinstance(keypoint_params, dict):\n                params = KeypointParams(**keypoint_params)\n            elif isinstance(keypoint_params, KeypointParams):\n                params = keypoint_params\n            else:\n                raise ValueError(\"unknown format of keypoint_params, please use `dict` or `KeypointParams`\")\n            self.processors[\"keypoints\"] = KeypointsProcessor(params, additional_targets)\n\n        if additional_targets is None:\n            additional_targets = {}\n\n        self.additional_targets = additional_targets\n\n        for proc in self.processors.values():\n            proc.ensure_transforms_valid(self.transforms)\n\n        self.add_targets(additional_targets)"
                },
                {
                    "docstring": null,
                    "method_name": "__call__",
                    "second_doc": "\"\"\"\nApplies a composed sequence of transformations and processing steps to the input data, either probabilistically or when explicitly forced, to ensure consistent handling and processing of data samples.\n\nThis approach allows integration of preprocessing and postprocessing steps around selected transformations while maintaining data validity, ensuring the desired manipulations are applied as intended in data pipelines.\n\nArgs:\n    force_apply (bool or int): If True or a nonzero integer, forces all transformations to be applied regardless of probability settings.\n    data (dict): The input data dictionary containing images and associated metadata to process.\n\nReturns:\n    dict: The transformed input data with all modifications, preprocessing, and postprocessing applied as specified in the composed operation.\n\"\"\"",
                    "source_code": "assert isinstance(force_apply, (bool, int)), \"force_apply must have bool or int type\"\n        need_to_run = force_apply or random.random() < self.p\n        for p in self.processors.values():\n            p.ensure_data_valid(data)\n        transforms = self.transforms if need_to_run else self.transforms.get_always_apply(self.transforms)\n        dual_start_end = transforms.start_end if self.processors else None\n\n        for idx, t in enumerate(transforms):\n            if dual_start_end is not None and idx == dual_start_end[0]:\n                for p in self.processors.values():\n                    p.preprocess(data)\n\n            data = t(force_apply=force_apply, **data)\n\n            if dual_start_end is not None and idx == dual_start_end[1]:\n                for p in self.processors.values():\n                    p.postprocess(data)\n\n        return data"
                },
                {
                    "docstring": null,
                    "method_name": "_to_dict",
                    "second_doc": "\"\"\"\nConvert the Compose object and its associated parameters into a serializable dictionary. This method collects transformation settings, including bounding box and keypoint processing parameters, along with additional target specifications, to facilitate configuration management and reproducibility.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing the serialized parameters of the Compose augmentation pipeline, including bounding box and keypoint configurations and any additional targets.\n\"\"\"",
                    "source_code": "dictionary = super(Compose, self)._to_dict()\n        bbox_processor = self.processors.get(\"bboxes\")\n        keypoints_processor = self.processors.get(\"keypoints\")\n        dictionary.update(\n            {\n                \"bbox_params\": bbox_processor.params._to_dict() if bbox_processor else None,\n                \"keypoint_params\": keypoints_processor.params._to_dict() if keypoints_processor else None,\n                \"additional_targets\": self.additional_targets,\n            }\n        )\n        return dictionary"
                }
            ],
            "name": "Compose",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the OneOf transformation by normalizing the probabilities of its constituent transforms, ensuring that selection among them is made according to the specified relative likelihoods.\n\nArgs:\n    transforms (list): A list of transformation objects, each with its own probability attribute 'p'.\n    p (float): Overall probability of applying one of the transformations.\n\nReturns:\n    None: This constructor modifies instance attributes in place for later use in transformation selection.\n\"\"\"",
                    "source_code": "super(OneOf, self).__init__(transforms, p)\n        transforms_ps = [t.p for t in transforms]\n        s = sum(transforms_ps)\n        self.transforms_ps = [t / s for t in transforms_ps]"
                },
                {
                    "docstring": null,
                    "method_name": "__call__",
                    "second_doc": "\"\"\"\nApply one or more transformations to the input data, either deterministically in replay mode or by randomly selecting and applying a single transformation based on their assigned probabilities.\n\nArgs:\n    data (dict): Input data to be augmented, typically containing images and related metadata.\n    force_apply (bool, optional): If True, transformation will be applied regardless of random probability. Default is False.\n\nReturns:\n    dict: Augmented data after applying the selected transformation(s).\n\nWhy:\n    This method is designed to introduce controlled randomness and diversity in the transformation process, enabling the automatic creation of a wide variety of augmented examples for model training and evaluation. This approach enhances the ability of models to generalize by exposing them to varied data representations.\n\"\"\"",
                    "source_code": "if self.replay_mode:\n            for t in self.transforms:\n                data = t(**data)\n            return data\n\n        if force_apply or random.random() < self.p:\n            random_state = np.random.RandomState(random.randint(0, 2 ** 32 - 1))\n            t = random_state.choice(self.transforms.transforms, p=self.transforms_ps)\n            data = t(force_apply=True, **data)\n        return data"
                }
            ],
            "name": "OneOf",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the OneOrOther object by selecting the set of transformations to apply.\n\nThis method ensures that either the provided transformations or a default pair of transformations are set for further processing, controlled by a specified probability.\n\nArgs:\n    transforms (list, optional): A list of image transformation functions to choose from. If None, defaults to [first, second].\n    p (float): Probability of applying one of the provided transformations.\n\nReturns:\n    None\n\nWhy:\n    This approach allows flexible augmentation scenarios by randomly selecting from the available transformations, increasing the variability of processed data.\n\"\"\"",
                    "source_code": "if transforms is None:\n            transforms = [first, second]\n        super(OneOrOther, self).__init__(transforms, p)"
                },
                {
                    "docstring": null,
                    "method_name": "__call__",
                    "second_doc": "\"\"\"\nApplies one of two possible image transformations to the provided data, either deterministically or randomly based on the set probability.\n\nThis method supports both deterministic replay of a transformation sequence and random application of a single transform. When in replay mode, all specified transforms are applied in order, ensuring consistency for reproducibility. Otherwise, one of two transforms is selected based on a predefined probability, which introduces controlled diversity into the output.\n\nArgs:\n    data (dict): Input data (such as image, mask, or related parameters) to be transformed.\n\nReturns:\n    dict: The input data after applying the selected transformation(s).\n\"\"\"",
                    "source_code": "if self.replay_mode:\n            for t in self.transforms:\n                data = t(**data)\n            return data\n\n        if random.random() < self.p:\n            return self.transforms[0](force_apply=True, **data)\n        else:\n            return self.transforms[-1](force_apply=True, **data)"
                }
            ],
            "name": "OneOrOther",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a PerChannel transformation object with specified transforms and probability, and assigns target channels for selective augmentation.\n\nThis method sets up the transformation pipeline to apply effects on specific image channels, enabling more granular and targeted augmentation operations.\n\nArgs:\n    transforms (list or callable): A sequence of transformation functions to be applied.\n    p (float): Probability of applying the transformations.\n    channels (list or tuple): Specifies which image channels the transformations should target.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(PerChannel, self).__init__(transforms, p)\n        self.channels = channels"
                },
                {
                    "docstring": null,
                    "method_name": "__call__",
                    "second_doc": "\"\"\"\nApply a sequence of transforms independently to specified image channels with a given probability.\n\nThis method ensures that channel-specific augmentations are applied in isolation, allowing for fine-grained modifications across different channels, which can help introduce greater variability and robustness to the dataset.\n\nArgs:\n    data (dict): A dictionary containing an 'image' key associated with a NumPy array representing the image to be transformed.\n    force_apply (bool, optional): If True, apply the transforms regardless of random probability. Defaults to False.\n\nReturns:\n    dict: The input dictionary with the transformed image.\n\"\"\"",
                    "source_code": "if force_apply or random.random() < self.p:\n\n            image = data[\"image\"]\n\n            # Expan mono images to have a single channel\n            if len(image.shape) == 2:\n                image = np.expand_dims(image, -1)\n\n            if self.channels is None:\n                self.channels = range(image.shape[2])\n\n            for c in self.channels:\n                for t in self.transforms:\n                    image[:, :, c] = t(image=image[:, :, c])[\"image\"]\n\n            data[\"image\"] = image\n\n        return data"
                }
            ],
            "name": "PerChannel",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ReplayCompose object by configuring a sequence of image augmentation transforms with support for deterministic execution and replay functionality.\n\nArgs:\n    transforms (list): List of transformation objects to be applied in sequence.\n    bbox_params (BBoxParams, optional): Parameters for handling bounding boxes.\n    keypoint_params (KeypointParams, optional): Parameters for handling keypoints.\n    additional_targets (dict, optional): Additional targets to transform along with the image.\n    p (float, optional): Probability of applying the composed transformations.\n    save_key (str, optional): Key name under which to store or retrieve the transformation replay data.\n\nReturns:\n    None\n\nWhy:\n    This method ensures that augmentation transformations can be applied in a consistent and reproducible manner across multiple runs, which is essential for debugging, validation, and repeatable experimentation in computer vision workflows.\n\"\"\"",
                    "source_code": "super(ReplayCompose, self).__init__(transforms, bbox_params, keypoint_params, additional_targets, p)\n        self.set_deterministic(True, save_key=save_key)\n        self.save_key = save_key"
                },
                {
                    "docstring": null,
                    "method_name": "__call__",
                    "second_doc": "\"\"\"\nApplies the composed series of augmentations to the input and records all applied transformations and their parameters for reproducibility or debugging purposes.\n\nArgs:\n    force_apply (bool, optional): If True, forces all augmentations to be applied regardless of their probability.\n    **kwargs: Additional keyword arguments for the transformations.\n\nReturns:\n    dict: A dictionary containing the augmented data and a detailed, serializable record of applied augmentations and parameters.\n\"\"\"",
                    "source_code": "kwargs[self.save_key] = defaultdict(dict)\n        result = super(ReplayCompose, self).__call__(force_apply=force_apply, **kwargs)\n        serialized = self.get_dict_with_id()\n        self.fill_with_params(serialized, result[self.save_key])\n        self.fill_applied(serialized)\n        result[self.save_key] = serialized\n        return result"
                },
                {
                    "docstring": null,
                    "method_name": "replay",
                    "second_doc": "\"\"\"\nReplay previously saved augmentation transformations on new input data.\n\nArgs:\n    saved_augmentations (dict): The dictionary containing the definition of the saved augmentation pipeline.\n    **kwargs: Arbitrary keyword arguments representing input data (e.g., image, mask, bounding boxes) to apply the augmentations on.\n\nReturns:\n    dict: A dictionary containing the results of applying the same sequence of augmentations to the new input data.\n\nWhy:\n    This method enables deterministic application of a previously used augmentation sequence, ensuring consistency when applying identical transformations to different data (such as related images and annotations) or for reproducibility in experiments.\n\"\"\"",
                    "source_code": "augs = ReplayCompose._restore_for_replay(saved_augmentations)\n        return augs(force_apply=True, **kwargs)"
                },
                {
                    "docstring": "\"\"\"\n        Args:\n            transform (dict): A dictionary with serialized transform pipeline.\n            lambda_transforms (dict): A dictionary that contains lambda transforms, that\n            is instances of the Lambda class.\n                This dictionary is required when you are restoring a pipeline that contains lambda transforms. Keys\n                in that dictionary should be named same as `name` arguments in respective lambda transforms from\n                a serialized pipeline.\n        \"\"\"",
                    "first_doc": "\"\"\"\nRestores a transform object from a serialized dictionary representation for replay purposes.\n\nThis method reconstructs a transform (potentially with nested sub-transforms) from its serialized form, rehydrates any Lambda transforms using the provided dictionary, and sets fields to indicate that the transform is being applied in replay mode.\n\nArgs:\n    transform_dict: A dictionary containing the serialized transform pipeline.\n        - Must include the fields \"applied\", \"params\", and \"__class_fullname__\". May include \"transforms\" for nested transforms.\n        - If the transform represents a Lambda transform, it must also contain \"__type__\" and \"__name__\".\n    lambda_transforms: A dictionary mapping Lambda transform names to their corresponding instances.\n        - Used to restore Lambda transforms found in the pipeline.\n        - Required when the serialized pipeline contains Lambda transforms; keys should match the 'name' arguments in those transforms.\n\nReturns:\n    An instantiated transform object restored from the serialized dictionary, set to replay mode with applied and parameters fields updated.\n\nClass fields initialized (for the returned object):\n    params: Stores the parameters for the transform as specified in the serialized data.\n    replay_mode: Boolean indicating the transform is operating in replay mode.\n    applied_in_replay: Indicates whether the transform was applied in the original operation or replay.\n\"\"\"",
                    "method_name": "_restore_for_replay",
                    "second_doc": "\"\"\"\nReconstructs a transform object and its parameters from a serialized dictionary, ensuring the restored object maintains the original configuration and application state for deterministic processing. This enables the reapplication or inspection of transformation pipelines exactly as they were previously executed, including custom Lambda transforms.\n\nArgs:\n    transform_dict (dict): A dictionary representing the serialized form of the transform pipeline. Must contain \"applied\", \"params\", and \"__class_fullname__\" keys, and may have a \"transforms\" key for nested structures. Lambda transform entries require \"__type__\" and \"__name__\".\n    lambda_transforms (dict): A mapping from Lambda transform names to their corresponding callable instances. Necessary for deserializing any Lambda transform referenced in the pipeline.\n\nReturns:\n    object: An instantiated transform object reconstructed from the serialized dictionary, configured for replay mode with parameters and application state restored.\n\"\"\"",
                    "source_code": "transform = transform_dict\n        applied = transform[\"applied\"]\n        params = transform[\"params\"]\n        lmbd = instantiate_lambda(transform, lambda_transforms)\n        if lmbd:\n            transform = lmbd\n        else:\n            name = transform[\"__class_fullname__\"]\n            args = {k: v for k, v in transform.items() if k not in [\"__class_fullname__\", \"applied\", \"params\"]}\n            cls = SERIALIZABLE_REGISTRY[name]\n            if \"transforms\" in args:\n                args[\"transforms\"] = [\n                    ReplayCompose._restore_for_replay(t, lambda_transforms=lambda_transforms)\n                    for t in args[\"transforms\"]\n                ]\n            transform = cls(**args)\n\n        transform.params = params\n        transform.replay_mode = True\n        transform.applied_in_replay = applied\n        return transform"
                },
                {
                    "docstring": null,
                    "method_name": "fill_with_params",
                    "second_doc": "\"\"\"\nRecursively fills a serialized transformation dictionary with parameters from a corresponding parameter dictionary, replacing identifier fields with actual parameters.\n\nArgs:\n    serialized (dict): The nested dictionary representing a transformation or pipeline of transformations, where \"id\" keys identify parameter sets.\n    all_params (dict): A mapping from transformation IDs to their associated parameters.\n\nReturns:\n    None: The function modifies the serialized dictionary in-place.\n\nWhy:\n    This method ensures that each transformation in a serialized pipeline contains the necessary parameters for correct and reproducible behavior during transformations, supporting consistency and flexibility in applying augmentation sequences.\n\"\"\"",
                    "source_code": "params = all_params.get(serialized.get(\"id\"))\n        serialized[\"params\"] = params\n        del serialized[\"id\"]\n        for transform in serialized.get(\"transforms\", []):\n            self.fill_with_params(transform, all_params)"
                },
                {
                    "docstring": null,
                    "method_name": "fill_applied",
                    "second_doc": "\"\"\"\nDetermine whether any transformation has been applied within a (potentially nested) set of serialized transformations, and update the serialized data accordingly.\n\nArgs:\n    serialized (dict): A dictionary representing a transformation or a sequence of transformations, possibly containing nested structures.\n\nReturns:\n    bool: True if at least one transformation in the provided (possibly nested) structure has been applied, otherwise False.\n\nWhy:\n    This method ensures that augmentation pipelines can accurately track which transformations have been executed, enabling deterministic behavior and reproducibility when augmenting data for automated preprocessing or experimentation.\n\"\"\"",
                    "source_code": "if \"transforms\" in serialized:\n            applied = [self.fill_applied(t) for t in serialized[\"transforms\"]]\n            serialized[\"applied\"] = any(applied)\n        else:\n            serialized[\"applied\"] = serialized.get(\"params\") is not None\n        return serialized[\"applied\"]"
                },
                {
                    "docstring": null,
                    "method_name": "_to_dict",
                    "second_doc": "\"\"\"\nThis method is intentionally not implemented to prevent serialization of ReplayCompose objects.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError: Always raised to indicate that serializing ReplayCompose is not supported.\n\nWhy:\n    Serialization is disabled for ReplayCompose to ensure the integrity and consistency of complex augmentation pipelines that track transform history and randomness, which cannot be reliably restored from a simple dictionary representation.\n\"\"\"",
                    "source_code": "raise NotImplementedError(\"You cannot serialize ReplayCompose\")"
                }
            ],
            "name": "ReplayCompose",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes bounding box parameters with constraints for area and visibility.\n\nThis method sets up the bounding box configuration, including the minimum required area and visibility fraction, to ensure that only meaningful and sufficiently visible bounding boxes are considered during image transformations.\n\nArgs:\n    format (str): The format in which bounding boxes are represented (e.g., 'pascal_voc', 'yolo').\n    label_fields (list): List of fields associated with bounding box labels.\n    min_area (float): Minimum area a bounding box must have to be kept.\n    min_visibility (float): Minimum fraction of bounding box visible after augmentation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(BboxParams, self).__init__(format, label_fields)\n        self.min_area = min_area\n        self.min_visibility = min_visibility"
                },
                {
                    "docstring": null,
                    "method_name": "_to_dict",
                    "second_doc": "\"\"\"\nConvert the bounding box parameters and their configuration into a dictionary format, including the minimum area and minimum visibility constraints.\n\nThis method enables serialization of the bounding box configuration, facilitating later reconstruction or usage within pipelines that require parameter persistence or inspection.\n\nReturns:\n    dict: A dictionary containing all bounding box parameters, including 'min_area' and 'min_visibility'.\n\"\"\"",
                    "source_code": "data = super(BboxParams, self)._to_dict()\n        data.update({\"min_area\": self.min_area, \"min_visibility\": self.min_visibility})\n        return data"
                }
            ],
            "name": "BboxParams",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes KeypointParams with parameters to control keypoint visibility handling and angle representation during augmentation. This setup ensures precise and consistent management of keypoints according to specific data pipeline requirements.\n\nArgs:\n    format (str): The format used to represent keypoints (e.g., 'xy', 'yx', etc.).\n    label_fields (list): List of fields corresponding to the labels associated with keypoints.\n    remove_invisible (bool): If True, keypoints that are not visible are excluded from the output.\n    angle_in_degrees (bool): If True, keypoint angles are interpreted in degrees; otherwise, in radians.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(KeypointParams, self).__init__(format, label_fields)\n        self.remove_invisible = remove_invisible\n        self.angle_in_degrees = angle_in_degrees"
                },
                {
                    "docstring": null,
                    "method_name": "_to_dict",
                    "second_doc": "\"\"\"\nConvert the keypoint parameters of the instance into a dictionary format, including additional attributes specific to keypoint processing. This facilitates consistent handling and serialization of transformation settings associated with keypoints during augmentation.\n\nReturns:\n    dict: A dictionary containing the keypoint parameter values, including 'remove_invisible' and 'angle_in_degrees', allowing for comprehensive tracking and storage of keypoint transformation options.\n\"\"\"",
                    "source_code": "data = super(KeypointParams, self)._to_dict()\n        data.update({\"remove_invisible\": self.remove_invisible, \"angle_in_degrees\": self.angle_in_degrees})\n        return data"
                }
            ],
            "name": "KeypointParams",
            "type": "class"
        }
    ],
    "albumentations/albumentations/core/serialization.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__new__",
                    "second_doc": "\"\"\"\nCreate a new class and automatically register it in the global serializable registry to enable efficient instantiation and consistent tracking of serializable transformations.\n\nArgs:\n    meta (type): The metaclass being invoked.\n    name (str): The name of the new class.\n    bases (tuple): Base classes for the new class.\n    class_dict (dict): Namespace containing definition of the class body.\n\nReturns:\n    type: The constructed class object, registered for serialization purposes.\n\"\"\"",
                    "source_code": "cls = type.__new__(meta, name, bases, class_dict)\n        SERIALIZABLE_REGISTRY[cls.get_class_fullname()] = cls\n        return cls"
                }
            ],
            "name": "SerializableMeta",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Take a transform pipeline and convert it to a serializable representation that uses only standard\n    python data types: dictionaries, lists, strings, integers, and floats.\n\n    Args:\n        transform (object): A transform that should be serialized. If the transform doesn't implement the `to_dict`\n            method and `on_not_implemented_error` equals to 'raise' then `NotImplementedError` is raised.\n            If `on_not_implemented_error` equals to 'warn' then `NotImplementedError` will be ignored\n            but no transform parameters will be serialized.\n    \"\"\"",
                "first_doc": "\"\"\"\nConverts the provided transform object into a dictionary representation suitable for serialization.\n\nArgs:\n    transform: The transform object to be serialized. It is expected to have a _to_dict method for serialization.\n\nReturns:\n    dict: A dictionary containing the serialization version and the serialized transform object. The dictionary has the keys:\n        - '__version__': The version string of the serialization format.\n        - 'transform': The serialized representation of the transform object as a dictionary. If serialization is not implemented, this will be an empty dictionary.\n\"\"\"",
                "method_name": "to_dict",
                "second_doc": "\"\"\"\nSerializes a transform object into a dictionary format to facilitate saving, sharing, or reconstructing the transformation pipeline.\n\nArgs:\n    transform: The object representing the image transformation, expected to implement a _to_dict method for serialization.\n    on_not_implemented_error (str): Determines behavior when serialization is not implemented. Accepted values are 'raise' (raises an error) and 'warn' (issues a warning and returns an empty dictionary).\n\nReturns:\n    dict: A dictionary with the following keys:\n        - '__version__': The current serialization format version.\n        - 'transform': The serialized contents of the transform as a dictionary. If serialization isn't implemented and 'warn' is set, this will be an empty dictionary.\n\nRaises:\n    ValueError: If 'on_not_implemented_error' is not 'raise' or 'warn'.\n    NotImplementedError: If '_to_dict' is not implemented and 'on_not_implemented_error' is set to 'raise'.\n\nWhy:\n    This method is necessary to enable consistent and reproducible storage and transfer of transformation configurations, ensuring that augmentation strategies can be reliably reapplied or shared across different environments and projects.\n\"\"\"",
                "source_code": "if on_not_implemented_error not in {\"raise\", \"warn\"}:\n        raise ValueError(\n            \"Unknown on_not_implemented_error value: {}. Supported values are: 'raise' and 'warn'\".format(\n                on_not_implemented_error\n            )\n        )\n    try:\n        transform_dict = transform._to_dict()\n    except NotImplementedError as e:\n        if on_not_implemented_error == \"raise\":\n            raise e\n        else:\n            transform_dict = {}\n            warnings.warn(\n                \"Got NotImplementedError while trying to serialize {obj}. Object arguments are not preserved. \"\n                \"Implement either '{cls_name}.get_transform_init_args_names' or '{cls_name}.get_transform_init_args' \"\n                \"method to make the transform serializable\".format(\n                    obj=transform, cls_name=transform.__class__.__name__\n                )\n            )\n    return {\"__version__\": __version__, \"transform\": transform_dict}"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "instantiate_lambda",
                "second_doc": "\"\"\"\nInstantiates a user-defined lambda transform by retrieving it from the provided collection based on its name. This mechanism is necessary to properly reconstruct custom transform logic during deserialization, ensuring reproducibility and modularity of transformation pipelines.\n\nArgs:\n    transform (dict): A dictionary representation of a lambda transform, containing the keys \"__type__\" and \"__name__\".\n    lambda_transforms (dict, optional): A mapping of transform names to their corresponding callable implementations. Required for reconstructing the lambda transform.\n\nReturns:\n    Callable: The lambda transform associated with the provided name.\n\nRaises:\n    ValueError: If no lambda_transforms dictionary is provided, or if the specified transform name cannot be found in lambda_transforms.\n\"\"\"",
                "source_code": "if transform.get(\"__type__\") == \"Lambda\":\n        name = transform[\"__name__\"]\n        if lambda_transforms is None:\n            raise ValueError(\n                \"To deserialize a Lambda transform with name {name} you need to pass a dict with this transform \"\n                \"as the `lambda_transforms` argument\".format(name=name)\n            )\n        transform = lambda_transforms.get(name)\n        if transform is None:\n            raise ValueError(\"Lambda transform with {name} was not found in `lambda_transforms`\".format(name=name))\n        return transform"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Args:\n        transform (dict): A dictionary with serialized transform pipeline.\n        lambda_transforms (dict): A dictionary that contains lambda transforms, that is instances of the Lambda class.\n            This dictionary is required when you are restoring a pipeline that contains lambda transforms. Keys\n            in that dictionary should be named same as `name` arguments in respective lambda transforms from\n            a serialized pipeline.\n    \"\"\"",
                "first_doc": "\"\"\"\nDeserializes a transform specification from a dictionary into a corresponding transform object.\n\nArgs:\n    transform_dict: A dictionary containing the configuration of the transform to instantiate.\n\nReturns:\n    An instantiated transform object based on the provided dictionary. If a lambda transform is specified, returns the corresponding lambda function; otherwise, constructs the object from the appropriate class.\n\"\"\"",
                "method_name": "from_dict",
                "second_doc": "\"\"\"\nReconstructs a transformation object from its dictionary representation, enabling reproducibility and consistency of data processing pipelines.\n\nArgs:\n    transform_dict (dict): Dictionary containing the serialized configuration of the transformation to be instantiated, including class information and parameters.\n    lambda_transforms (dict, optional): Mapping of custom lambda transform identifiers to functions. Used to instantiate lambda-based transformations.\n\nReturns:\n    object: An instance of the specified transform, fully initialized with the provided parameters. If a lambda transform is specified, returns the corresponding lambda function; otherwise, returns an object of the appropriate registered transformation class.\n\"\"\"",
                "source_code": "transform = transform_dict[\"transform\"]\n    lmbd = instantiate_lambda(transform, lambda_transforms)\n    if lmbd:\n        return lmbd\n    name = transform[\"__class_fullname__\"]\n    args = {k: v for k, v in transform.items() if k != \"__class_fullname__\"}\n    cls = SERIALIZABLE_REGISTRY[name]\n    if \"transforms\" in args:\n        args[\"transforms\"] = [\n            from_dict({\"transform\": t}, lambda_transforms=lambda_transforms) for t in args[\"transforms\"]\n        ]\n    return cls(**args)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "check_data_format",
                "second_doc": "\"\"\"\nValidates the specified data format to ensure it is supported for saving or loading configuration files, facilitating reliable and predictable behavior when working with different serialization methods.\n\nArgs:\n    data_format (str): The data format to be checked. Must be one of 'json' or 'yaml'.\n\nRaises:\n    ValueError: If an unsupported data format is provided.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "if data_format not in {\"json\", \"yaml\"}:\n        raise ValueError(\"Unknown data_format {}. Supported formats are: 'json' and 'yaml'\".format(data_format))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Take a transform pipeline, serialize it and save a serialized version to a file\n    using either json or yaml format.\n\n    Args:\n        transform (obj): Transform to serialize.\n        filepath (str): Filepath to write to.\n        data_format (str): Serialization format. Should be either `json` or 'yaml'.\n        on_not_implemented_error (str): Parameter that describes what to do if a transform doesn't implement\n            the `to_dict` method. If 'raise' then `NotImplementedError` is raised, if `warn` then the exception will be\n            ignored and no transform arguments will be saved.\n    \"\"\"",
                "first_doc": "\"\"\"\nSaves a transformation to a file in the specified data format (JSON or YAML).\n\nArgs:\n    transform: The transformation object to be saved.\n    filepath: The destination file path where the transformation will be stored.\n\nReturns:\n    None: This method does not return a value.\n\"\"\"",
                "method_name": "save",
                "second_doc": "\"\"\"\nSerializes a transformation pipeline to a file in the chosen data format (JSON or YAML), ensuring the transformation configuration can be persisted and reused across different projects or sessions.\n\nArgs:\n    transform: The transformation pipeline object to be serialized and saved.\n    filepath: The path to the file where the transformation will be written.\n    data_format: The format in which to serialize the transformation ('json' or 'yaml').\n    on_not_implemented_error: Optional handler for serialization errors when certain attributes are not supported.\n\nReturns:\n    None: This function writes the transformation to a file and does not return a value.\n\nWhy:\n    Storing transformation pipelines as files allows consistent reproducibility and sharing of data augmentation strategies, facilitating easier collaboration and experimentation workflows.\n\"\"\"",
                "source_code": "check_data_format(data_format)\n    transform_dict = to_dict(transform, on_not_implemented_error=on_not_implemented_error)\n    dump_fn = json.dump if data_format == \"json\" else yaml.safe_dump\n    with open(filepath, \"w\") as f:\n        dump_fn(transform_dict, f)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Load a serialized pipeline from a json or yaml file and construct a transform pipeline.\n\n    Args:\n        transform (obj): Transform to serialize.\n        filepath (str): Filepath to read from.\n        data_format (str): Serialization format. Should be either `json` or 'yaml'.\n        lambda_transforms (dict): A dictionary that contains lambda transforms, that is instances of the Lambda class.\n            This dictionary is required when you are restoring a pipeline that contains lambda transforms. Keys\n            in that dictionary should be named same as `name` arguments in respective lambda transforms from\n            a serialized pipeline.\n    \"\"\"",
                "first_doc": "\"\"\"\nLoads and parses a data transformation configuration file.\n\nChecks the specified data format and uses the appropriate loader to parse the configuration file. Returns an object created from the parsed dictionary.\n\nArgs:\n    filepath: Path to the configuration file to be loaded.\n\nReturns:\n    Object generated from the parsed transformation dictionary.\n\"\"\"",
                "method_name": "load",
                "second_doc": "\"\"\"\nReads a transformation configuration file and constructs a corresponding object with the specified augmentations.\n\nThis method interprets the data format (JSON or YAML), loads the configuration, and generates an object that encapsulates the augmentation pipeline as defined in the file. This enables reproducibility and sharing of complex augmentation setups by loading them from files rather than defining them programmatically.\n\nArgs:\n    filepath: str\n        Path to the configuration file to be loaded.\n    data_format: str\n        Format of the configuration file, typically 'json' or 'yaml'.\n    lambda_transforms: dict, optional\n        Custom lambda transforms to be used when constructing transformations.\n\nReturns:\n    An object representing the composed data transformations derived from the parsed configuration.\n\"\"\"",
                "source_code": "check_data_format(data_format)\n    load_fn = json.load if data_format == \"json\" else yaml.safe_load\n    with open(filepath) as f:\n        transform_dict = load_fn(f)\n    return from_dict(transform_dict, lambda_transforms=lambda_transforms)"
            },
            "type": "function"
        }
    ],
    "albumentations/albumentations/core/six.py": [
        {
            "details": {
                "docstring": "\"\"\"Class decorator for creating a class with a metaclass.\"\"\"",
                "first_doc": "\"\"\"\nA decorator that applies a given metaclass to the class being defined.\n\nThis decorator modifies the target class so that it uses the specified metaclass, correctly handling any __slots__, __dict__, and __weakref__ attributes for compatibility.\n\nArgs:\n    metaclass: The metaclass to apply to the decorated class.\n\nReturns:\n    function: A wrapper that creates the class using the specified metaclass.\n\"\"\"",
                "method_name": "add_metaclass",
                "second_doc": "\"\"\"\nDecorator that dynamically assigns a specified metaclass to the decorated class, ensuring proper handling of special class attributes such as __slots__, __dict__, and __weakref__, to maintain compatibility and expected behavior.\n\nThis adjustment is crucial for supporting advanced class customizations and behaviors, which may be required for seamless integration, extension, or modification within complex augmentation pipelines.\n\nArgs:\n    metaclass (type): The metaclass to be assigned to the decorated class.\n\nReturns:\n    function: A wrapper that constructs and returns the class using the provided metaclass with preserved attributes.\n\"\"\"",
                "source_code": "def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get(\"__slots__\")\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop(\"__dict__\", None)\n        orig_vars.pop(\"__weakref__\", None)\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n\n    return wrapper"
            },
            "type": "function"
        }
    ],
    "albumentations/albumentations/core/transforms_interface.py": [
        {
            "details": {
                "docstring": "\"\"\"Convert input argument to min-max tuple\n    Args:\n        param (scalar, tuple or list of 2+ elements): Input value.\n            If value is scalar, return value would be (offset - value, offset + value).\n            If value is tuple, return value would be value + offset (broadcasted).\n        low:  Second element of tuple can be passed as optional argument\n        bias: An offset factor added to each element\n    \"\"\"",
                "first_doc": "\"\"\"\nConverts the input parameter into a tuple, applying optional lower bound or bias offset.\n\nArgs:\n    param: The value to be converted to a tuple. It may be a scalar (int or float), a list, or a tuple.\n\nReturns:\n    tuple: A tuple representing the converted form of the input value, potentially adjusted by a lower bound or bias.\n\nRaises:\n    ValueError: If both 'low' and 'bias' arguments are provided, or if 'param' is not a scalar, list, or tuple.\n\"\"\"",
                "method_name": "to_tuple",
                "second_doc": "\"\"\"\nNormalizes the input parameter into a tuple format, with optional adjustment based on a lower bound or bias. This facilitates consistent handling of transformation parameters, supporting both scalar and sequence inputs and enabling downstream augmentation logic to work reliably with uniform data types.\n\nArgs:\n    param (int, float, list, tuple, or None): The value to standardize into a tuple. Accepts a scalar (int or float), list, tuple, or None.\n    low (int or float, optional): If specified, defines a lower bound for tuple construction from a scalar param.\n    bias (int or float, optional): If specified, offsets all tuple elements by this amount.\n\nReturns:\n    tuple or None: A tuple representation of the input (with optional bias applied), or None if param is None.\n\nRaises:\n    ValueError: If both 'low' and 'bias' are provided at the same time, or if 'param' is not a recognized type (int, float, list, or tuple).\n\"\"\"",
                "source_code": "if low is not None and bias is not None:\n        raise ValueError(\"Arguments low and bias are mutually exclusive\")\n\n    if param is None:\n        return param\n\n    if isinstance(param, (int, float)):\n        if low is None:\n            param = -param, +param\n        else:\n            param = (low, param) if low < param else (param, low)\n    elif isinstance(param, (list, tuple)):\n        param = tuple(param)\n    else:\n        raise ValueError(\"Argument param must be either scalar (int, float) or tuple\")\n\n    if bias is not None:\n        return tuple([bias + x for x in param])\n\n    return tuple(param)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the base parameters for a transformation, configuring its application probability, deterministic behavior, and state tracking.\n\nArgs:\n    p (float): Probability of applying the transform. Default is 0.5.\n    always_apply (bool): Whether to always apply the transform, regardless of the probability. Default is False.\n\nReturns:\n    None\n\nWhy:\n    This method sets up the key properties needed for managing when and how a transformation is applied, ensuring consistent and controllable augmentation behavior for each data sample, and allowing for reproducibility and replay of the transformations during data processing.\n\"\"\"",
                    "source_code": "self.p = p\n        self.always_apply = always_apply\n        self._additional_targets = {}\n\n        # replay mode params\n        self.deterministic = False\n        self.save_key = \"replay\"\n        self.params = {}\n        self.replay_mode = False\n        self.applied_in_replay = False"
                },
                {
                    "docstring": null,
                    "method_name": "__call__",
                    "second_doc": "\"\"\"\nApply the transformation to the input data, conditionally based on configuration such as probability, deterministic mode, or replay settings.\n\nThis method determines whether to perform the transformation using the provided parameters and control flags, and then applies the transformation accordingly. The conditional logic ensures reproducibility, supports randomized or forced application, and enables reverse-engineering of applied transformations through replay.\n\nArgs:\n    force_apply (bool, optional): If True, forces the transformation to be applied regardless of probability or other criteria.\n    **kwargs: Additional named arguments corresponding to input data (such as images, masks, bounding boxes, etc.) required by the transformation and for determining parameter dependencies.\n\nReturns:\n    dict: The input data, either augmented according to the transformation or unchanged, depending on the logic of probability, deterministic application, and replay mode.\n\"\"\"",
                    "source_code": "if self.replay_mode:\n            if self.applied_in_replay:\n                return self.apply_with_params(self.params, **kwargs)\n            else:\n                return kwargs\n\n        if (random.random() < self.p) or self.always_apply or force_apply:\n            params = self.get_params()\n\n            if self.targets_as_params:\n                assert all(key in kwargs for key in self.targets_as_params), \"{} requires {}\".format(\n                    self.__class__.__name__, self.targets_as_params\n                )\n                targets_as_params = {k: kwargs[k] for k in self.targets_as_params}\n                params_dependent_on_targets = self.get_params_dependent_on_targets(targets_as_params)\n                params.update(params_dependent_on_targets)\n            if self.deterministic:\n                if self.targets_as_params:\n                    warn(\n                        self.get_class_fullname() + \" could work incorrectly in ReplayMode for other input data\"\n                        \" because its' params depend on targets.\"\n                    )\n                kwargs[self.save_key][id(self)] = deepcopy(params)\n            return self.apply_with_params(params, **kwargs)\n\n        return kwargs"
                },
                {
                    "docstring": null,
                    "method_name": "apply_with_params",
                    "second_doc": "\"\"\"\nApply a set of transformation parameters to each non-None input argument using appropriate target-specific functions.\n\nArgs:\n    params (dict or None): Transformation parameters to be applied. If None, the original input arguments are returned unmodified.\n    **kwargs: Inputs (such as images, masks, bounding boxes, etc.) to which the transformations will be applied.\n\nReturns:\n    dict: A mapping from each input argument name to its transformed value, or None if the input argument was None.\n\nWhy:\n    This method ensures that transformation parameters are systematically applied to the correct inputs, considering their specific dependencies, enabling consistent and targeted data augmentation essential for downstream computer vision processing.\n\"\"\"",
                    "source_code": "if params is None:\n            return kwargs\n        params = self.update_params(params, **kwargs)\n        res = {}\n        for key, arg in kwargs.items():\n            if arg is not None:\n                target_function = self._get_target_function(key)\n                target_dependencies = {k: kwargs[k] for k in self.target_dependence.get(key, [])}\n                res[key] = target_function(arg, **dict(params, **target_dependencies))\n            else:\n                res[key] = None\n        return res"
                },
                {
                    "docstring": null,
                    "method_name": "set_deterministic",
                    "second_doc": "\"\"\"\nConfigure whether the transformation is applied deterministically and specify a key for saving transformation state.\n\nThis method exists to control the reproducibility of transformation outcomes by toggling determinism and assigning a unique identifier for storing internal states. This is important to ensure consistent results and reliable behavior in transformation pipelines during training or testing.\n\nArgs:\n    flag (bool): If True, the transformation will produce repeatable, deterministic results.\n    save_key (str): Unique identifier for saving the transformation state. Must not be \"params\".\n\nReturns:\n    BasicTransform: The transform instance with updated deterministic and save_key attributes.\n\"\"\"",
                    "source_code": "assert save_key != \"params\", \"params save_key is reserved\"\n        self.deterministic = flag\n        self.save_key = save_key\n        return self"
                },
                {
                    "docstring": null,
                    "method_name": "__repr__",
                    "second_doc": "\"\"\"\nReturn a string representation of the transformation instance, including its initialization parameters.\n\nThis method provides a clear and informative summary of the transform's configuration, making it easier to understand, debug, and log complex augmentation pipelines.\n\nReturns:\n    str: Human-readable summary of the transformation and its parameters.\n\"\"\"",
                    "source_code": "state = self.get_base_init_args()\n        state.update(self.get_transform_init_args())\n        return \"{name}({args})\".format(name=self.__class__.__name__, args=format_args(state))"
                },
                {
                    "docstring": null,
                    "method_name": "_get_target_function",
                    "second_doc": "\"\"\"\nRetrieve the appropriate transformation function for a given target key, considering any specific mappings for additional target types. This allows application of consistent and correct transformations to different data components during augmentation.\n\nArgs:\n    key (str): The identifier for the data component whose transformation function is required (e.g., 'image', 'mask').\n\nReturns:\n    Callable: The transformation function associated with the given key. Returns an identity function if no specific function is found for the key.\n\"\"\"",
                    "source_code": "transform_key = key\n        if key in self._additional_targets:\n            transform_key = self._additional_targets.get(key, None)\n\n        target_function = self.targets.get(transform_key, lambda x, **p: x)\n        return target_function"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApply the transformation to the input data.\n\nThis method must be implemented by subclasses to define specific image or data augmentation logic required for data preprocessing and model training. It is designed as an abstract method to enforce a uniform interface and delegate responsibility for the actual transformation implementation to derived classes.\n\nArgs:\n    *args: Positional arguments containing data to be transformed (e.g., images, masks, keypoints).\n    **kwargs: Additional keyword arguments.\n\nReturns:\n    Transformed data as defined by the implementation in a concrete subclass.\n\nRaises:\n    NotImplementedError: If the method is called on the base class directly, indicating a subclass must implement it.\n\"\"\"",
                    "source_code": "raise NotImplementedError"
                },
                {
                    "docstring": null,
                    "method_name": "get_params",
                    "second_doc": "\"\"\"\nRetrieve parameters required for the transformation.\n\nThis method returns an empty dictionary, indicating that no additional parameters are needed for this transformation. This standardizes how transforms interact and are composed within augmentation pipelines.\n\nReturns:\n    dict: An empty dictionary, as this transformation does not require parameters.\n\"\"\"",
                    "source_code": "return {}"
                },
                {
                    "docstring": null,
                    "method_name": "targets",
                    "second_doc": "\"\"\"\nSpecifies the mapping of transformation targets for derived transform classes.\n\nThis property is intended to be overridden by subclasses to define which types of data (such as images, masks, bounding boxes, or keypoints) a particular transformation should operate on and how those data types should be processed.\n\nWhy: This mechanism enables the transformation framework to intelligently apply operations to a variety of relevant inputs, supporting flexible and extensible augmentation pipelines for computer vision tasks.\n\nArgs:\n    None\n\nReturns:\n    dict: A mapping where keys are target names (e.g., \"image\", \"mask\") and values are callables or transformation instructions for handling those targets.\n\nRaises:\n    NotImplementedError: If the property is not overridden in a subclass.\n\"\"\"",
                    "source_code": "raise NotImplementedError"
                },
                {
                    "docstring": null,
                    "method_name": "update_params",
                    "second_doc": "\"\"\"\nUpdate the transformation parameters dictionary with properties required for applying the transform, including image dimensions and relevant transformation attributes if present.\n\nArgs:\n    params (dict): A dictionary containing the current parameters for the transformation.\n    **kwargs: Additional keyword arguments, expected to include 'image' as a NumPy array.\n\nReturns:\n    dict: The updated parameters dictionary including information about the image size and any extra transformation details (such as interpolation method or fill value, if applicable).\n\nThis method ensures that all necessary contextual information (like image shape and transformation settings) is available for consistent and accurate application of the transformation.\n\"\"\"",
                    "source_code": "if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        params.update({\"cols\": kwargs[\"image\"].shape[1], \"rows\": kwargs[\"image\"].shape[0]})\n        return params"
                },
                {
                    "docstring": null,
                    "method_name": "target_dependence",
                    "second_doc": "\"\"\"\nProvides information about dependencies required for applying this transformation to targets such as masks, bounding boxes, or keypoints. This enables consistent and correct augmentation across different target types.\n\nReturns:\n    dict: An empty dictionary indicating that this transformation does not require any additional information about target dependencies.\n\"\"\"",
                    "source_code": "return {}"
                },
                {
                    "docstring": "\"\"\"Add targets to transform them the same way as one of existing targets\n        ex: {'target_image': 'image'}\n        ex: {'obj1_mask': 'mask', 'obj2_mask': 'mask'}\n        by the way you must have at least one object with key 'image'\n\n        Args:\n            additional_targets (dict): keys - new target name, values - old target name. ex: {'image2': 'image'}\n        \"\"\"",
                    "first_doc": "\"\"\"\nAdds new targets to be transformed in the same way as existing targets.\n\nThis method allows you to specify additional targets to undergo the same transformations as one of the existing targets. For example, you can use this to ensure multiple masks or images are handled identically during augmentation.\n\nArgs:\n    additional_targets: A dictionary where each key is a new target name and the corresponding value is the name of an existing target to mimic. For example, {'image2': 'image'} or {'obj1_mask': 'mask'}.\n\nClass Fields Initialized:\n    _additional_targets: Stores the mapping from new target names to existing target names to determine how each new target should be transformed.\n\nReturns:\n    None: This method does not return a value.\n\"\"\"",
                    "method_name": "add_targets",
                    "second_doc": "\"\"\"\nRegisters additional targets to be processed with the same transformations as existing targets, ensuring that related data (such as paired images or masks) undergo identical augmentation steps for consistency across all data elements.\n\nArgs:\n    additional_targets (dict): A mapping where each key is the name of a new target to be added, and each value is the name of an existing target whose transformations should be replicated. For example, {'image2': 'image'} or {'obj1_mask': 'mask'}.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self._additional_targets = additional_targets"
                },
                {
                    "docstring": null,
                    "method_name": "targets_as_params",
                    "second_doc": "\"\"\"\nReturns a list of target names that are processed as parameters by this transform.  \n\nSince the base transform does not modify any additional targets beyond the input, this property returns an empty list to indicate that no extra parameters are required. This design allows for straightforward extension by subclasses that may need to handle additional types of data.\n\nReturns:\n    list: An empty list indicating no parameterized targets for the base transform.\n\"\"\"",
                    "source_code": "return []"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nAbstract method to compute and return transform parameters that depend on the current sample's targets. This enables dynamic adjustment of augmentation parameters based on input annotations, ensuring context-aware transformations.\n\nArgs:\n    targets (Any): Data associated with the sample (such as labels, bounding boxes, or keypoints) used to determine how parameters should be generated.\n\nReturns:\n    Dict[str, Any]: A dictionary containing parameter names and their computed values, dependent on the provided targets.\n\nRaises:\n    NotImplementedError: This method should be overridden in subclasses to provide specific parameter computation logic.\n\"\"\"",
                    "source_code": "raise NotImplementedError(\n            \"Method get_params_dependent_on_targets is not implemented in class \" + self.__class__.__name__\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "get_class_fullname",
                    "second_doc": "\"\"\"\nGet the fully qualified name of the class, including its module.\n\nThis method constructs a unique identifier for the class, which facilitates consistent tracking, serialization, and deserialization of transformation objects in complex augmentation pipelines.\n\nArgs:\n    cls (type): The class for which to obtain the full name.\n\nReturns:\n    str: The full name of the class in the format 'module_name.class_name'.\n\"\"\"",
                    "source_code": "return \"{cls.__module__}.{cls.__name__}\".format(cls=cls)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nRaise an error to enforce implementation of a method needed for extracting the initialization arguments required to reconstruct the transform instance. \n\nThis mechanism is essential for serialization and deserialization processes, ensuring that custom transforms can be accurately saved and restored when building augmentation pipelines.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError: If the method is not implemented in a subclass, signaling that serialization support is missing.\n\"\"\"",
                    "source_code": "raise NotImplementedError(\n            \"Class {name} is not serializable because the `get_transform_init_args_names` method is not \"\n            \"implemented\".format(name=self.get_class_fullname())\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "get_base_init_args",
                    "second_doc": "\"\"\"\nCollects the initialization parameters of the transformation, allowing these settings to be replicated or logged for reproducibility.\n\nArgs:\n    self: The instance of the BasicTransform class.\n\nReturns:\n    dict: A dictionary containing the keys \"always_apply\" and \"p\" with their corresponding instance values.\n\"\"\"",
                    "source_code": "return {\"always_apply\": self.always_apply, \"p\": self.p}"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args",
                    "second_doc": "\"\"\"\nGathers and returns the initialization arguments for the transform as a dictionary.\n\nThis method collects the current values of the transform's initialization parameters, enabling reproducibility and serialization of transformation configurations.\n\nReturns:\n    dict: A dictionary mapping argument names to their corresponding values derived from the transform's current state.\n\"\"\"",
                    "source_code": "return {k: getattr(self, k) for k in self.get_transform_init_args_names()}"
                },
                {
                    "docstring": null,
                    "method_name": "_to_dict",
                    "second_doc": "\"\"\"\nSerialize the transform's state to a dictionary, including class metadata and initialization parameters.\n\nThis facilitates saving and reconstructing transform configurations, enabling consistent and reproducible augmentation pipelines.\n\nReturns:\n    dict: A dictionary containing the full class name and all initialization arguments required to recreate the transform.\n\"\"\"",
                    "source_code": "state = {\"__class_fullname__\": self.get_class_fullname()}\n        state.update(self.get_base_init_args())\n        state.update(self.get_transform_init_args())\n        return state"
                },
                {
                    "docstring": null,
                    "method_name": "get_dict_with_id",
                    "second_doc": "\"\"\"\nGenerate a dictionary representation of the transformation instance, including a unique identifier for tracking or reproducibility purposes.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary containing the serialized attributes of the transformation along with its unique instance identifier.\n\"\"\"",
                    "source_code": "d = self._to_dict()\n        d[\"id\"] = id(self)\n        return d"
                }
            ],
            "name": "BasicTransform",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "targets",
                    "second_doc": "\"\"\"\nDefines the mapping between data types (such as images, masks, bounding boxes, and keypoints) and the corresponding transformation functions to ensure that each type is correctly and consistently processed during augmentation.\n\nReturns:\n    dict: A dictionary mapping data types to their respective transformation methods within the class.\n\"\"\"",
                    "source_code": "return {\n            \"image\": self.apply,\n            \"mask\": self.apply_to_mask,\n            \"masks\": self.apply_to_masks,\n            \"bboxes\": self.apply_to_bboxes,\n            \"keypoints\": self.apply_to_keypoints,\n        }"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nAbstract method for applying a transformation to a bounding box.\nThis function serves as a template for subclasses, ensuring that any transform which affects images can also be consistently extended to operate on associated bounding box annotations. This is necessary to maintain coherence between the transformed image and the spatial annotation data during augmentation.\n\nArgs:\n    bbox: The bounding box to be transformed, typically represented as a list, tuple, or similar structure containing bounding box coordinates.\n\nReturns:\n    The transformed bounding box in the same format as the input, with coordinates adjusted to reflect the transformation applied.\n\nRaises:\n    NotImplementedError: This method must be implemented by subclasses that modify bounding boxes along with images.\n\"\"\"",
                    "source_code": "raise NotImplementedError(\"Method apply_to_bbox is not implemented in class \" + self.__class__.__name__)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nThis method serves as a placeholder for applying a transformation to a keypoint, requiring implementation in subclasses to ensure that keypoint data is consistently handled alongside corresponding image augmentations.\n\nArgs:\n    keypoint: The keypoint data to be transformed.\n\nReturns:\n    The transformed keypoint.\n\nRaises:\n    NotImplementedError: If the method is not implemented in a subclass.\n\"\"\"",
                    "source_code": "raise NotImplementedError(\"Method apply_to_keypoint is not implemented in class \" + self.__class__.__name__)"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bboxes",
                    "second_doc": "\"\"\"\nApplies a transformation to each bounding box in the input list, processing only the coordinates while preserving any additional information attached to the box. This ensures that bounding boxes remain correctly aligned with their associated image data after transformations.\n\nArgs:\n    bboxes (list): A list of bounding boxes, where each bounding box is a tuple of at least four elements representing coordinates, followed by optional additional data.\n    **params: Additional parameters required for the transformation.\n\nReturns:\n    list: A list of transformed bounding boxes, with coordinates updated according to the applied transformation and additional fields left intact.\n\"\"\"",
                    "source_code": "return [self.apply_to_bbox(tuple(bbox[:4]), **params) + tuple(bbox[4:]) for bbox in bboxes]"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoints",
                    "second_doc": "\"\"\"\nApplies the transformation to a list of keypoints by individually transforming the first four elements of each keypoint and retaining any additional data unchanged.\n\nThis approach ensures that keypoints are accurately transformed in accordance with the same rules applied to corresponding images, maintaining spatial relationships and annotation consistency.\n\nArgs:\n    keypoints (list): A list of keypoints, where each keypoint is a tuple with at least four elements. Additional elements, if present, are preserved.\n\nReturns:\n    list: A list of transformed keypoints with unchanged extra elements appended.\n\"\"\"",
                    "source_code": "return [self.apply_to_keypoint(tuple(keypoint[:4]), **params) + tuple(keypoint[4:]) for keypoint in keypoints]"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nApply the transformation to a mask image, ensuring that nearest-neighbor interpolation is used to preserve discrete label values.\n\nArgs:\n    img (numpy.ndarray): Mask image to be transformed.\n    params (dict): Parameters for the transformation, such as interpolation methods.\n\nReturns:\n    numpy.ndarray: Transformed mask image with label integrity preserved.\n    \nWhy:\n    Nearest-neighbor interpolation is applied to masks to prevent the mixing of class labels during the augmentation process, which is crucial for tasks like semantic segmentation where each pixel's class must remain discrete and unchanged.\n\"\"\"",
                    "source_code": "return self.apply(img, **{k: cv2.INTER_NEAREST if k == \"interpolation\" else v for k, v in params.items()})"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_masks",
                    "second_doc": "\"\"\"\nApply the transformation to each mask in the provided list using the given parameters.\n\nThis method ensures that all masks associated with an image are transformed consistently, which is essential for tasks like semantic segmentation where the spatial alignment between images and masks must be preserved during augmentation.\n\nArgs:\n    masks (list): A list of mask arrays to be transformed.\n    **params: Additional parameters to control the specific transformation logic.\n\nReturns:\n    list: A list containing the transformed masks.\n\"\"\"",
                    "source_code": "return [self.apply_to_mask(mask, **params) for mask in masks]"
                }
            ],
            "name": "DualTransform",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "targets",
                    "second_doc": "\"\"\"\nReturns a dictionary specifying which data targets should have transformations applied, mapping the \"image\" key to the associated transformation method.\n\nThis ensures that only the image data is modified, preserving other types of annotations or data associated with a sample.\n\nReturns:\n    dict: A dictionary mapping the string \"image\" to the transform function for image-only augmentation.\n\"\"\"",
                    "source_code": "return {\"image\": self.apply}"
                }
            ],
            "name": "ImageOnlyTransform",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoint",
                    "second_doc": "\"\"\"\nReturns the input keypoint without applying any transformation. \n\nThis method is used to maintain the integrity of keypoint data when no operation is required during an augmentation pipeline, ensuring that the pipeline remains consistent even when no changes are applied.\n\nArgs:\n    keypoint: The keypoint to be processed, typically represented as a tuple or list of coordinates.\n\nReturns:\n    The unmodified input keypoint.\n\"\"\"",
                    "source_code": "return keypoint"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_bbox",
                    "second_doc": "\"\"\"\nReturn the input bounding box unchanged.\n\nThis method is used to ensure that bounding boxes remain unaltered when no transformation is applied, maintaining consistency within augmentation pipelines.\n\nArgs:\n    bbox: The bounding box to process, typically represented as a tuple or list.\n\nReturns:\n    The same bounding box provided as input, without modification.\n\"\"\"",
                    "source_code": "return bbox"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nReturns the input image without any modifications, serving as a placeholder operation primarily for pipeline consistency or for cases where no augmentation is required.\n\nArgs:\n    img: The input image to be passed through unchanged.\n\nReturns:\n    The original input image, unaltered.\n\"\"\"",
                    "source_code": "return img"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nReturn the input mask unchanged, ensuring that no augmentation or transformation is applied. This can be useful when defining a transformation pipeline where certain elements should remain unaltered to maintain consistency or compatibility with downstream processes.\n\nArgs:\n    img: The mask or image to be passed through without modification.\n\nReturns:\n    The same mask or image that was provided as input.\n\"\"\"",
                    "source_code": "return img"
                }
            ],
            "name": "NoOp",
            "type": "class"
        }
    ],
    "albumentations/albumentations/core/utils.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "format_args",
                "second_doc": "\"\"\"\nFormats a dictionary of arguments into a string representation suitable for displaying or logging parameter configurations.\n\nArgs:\n    args_dict (dict): A dictionary containing argument names as keys and their corresponding values.\n\nReturns:\n    str: A string where each argument is represented as 'key=value', with string values enclosed in single quotes and all pairs joined by commas.\n\nWhy:\n    This method provides a readable and standardized way to represent argument sets, which helps with the inspection, debugging, and reproducibility of parameterized operations.\n\"\"\"",
                "source_code": "formatted_args = []\n    for k, v in args_dict.items():\n        if isinstance(v, string_types):\n            v = \"'{}'\".format(v)\n        formatted_args.append(\"{}={}\".format(k, v))\n    return \", \".join(formatted_args)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the parameters needed to define the output data format and which fields correspond to labels.\n\nArgs:\n    format: Specifies the desired output format for the data.\n    label_fields: Indicates which fields contain label information to be processed alongside the data.\n\nReturns:\n    None\n\nWhy:\n    This method establishes essential configuration settings that determine how augmented data and corresponding labels are structured and identified, enabling consistent and correct application of subsequent data processing or augmentation steps.\n\"\"\"",
                    "source_code": "self.format = format\n        self.label_fields = label_fields"
                },
                {
                    "docstring": null,
                    "method_name": "_to_dict",
                    "second_doc": "\"\"\"\nConvert instance attributes to a dictionary representation for serialization or further processing.\n\nThis method helps facilitate interoperability and ease of integration with other tools by providing a standard format for attributes.\n\nReturns:\n    dict: A dictionary containing the instance's 'format' and 'label_fields' attributes.\n\"\"\"",
                    "source_code": "return {\"format\": self.format, \"label_fields\": self.label_fields}"
                }
            ],
            "name": "Params",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the DataProcessor instance by setting up the main and additional data fields to track during processing, and initializes the data length counter.\n\nArgs:\n    params (dict): Configuration parameters needed for processing the data.\n    additional_targets (dict, optional): Optional mapping of additional target names to their associated data fields. If a target is mapped to the default data name, it is included for tracking.\n\nReturns:\n    None\n\nWhy:\n    This setup allows the processor to correctly organize and reference all relevant data fields for subsequent transformation or augmentation operations, ensuring that both primary and associated targets are handled consistently.\n\"\"\"",
                    "source_code": "self.params = params\n        self.data_fields = [self.default_data_name]\n        if additional_targets is not None:\n            for k, v in additional_targets.items():\n                if v == self.default_data_name:\n                    self.data_fields.append(k)\n        self.data_length = 0"
                },
                {
                    "docstring": null,
                    "method_name": "default_data_name",
                    "second_doc": "\"\"\"\nAbstract property that specifies the default name used to identify a particular dataset.\n\nThis method enforces that each subclass defines a consistent reference name for datasets it processes, promoting clarity and interoperability in augmentation workflows.\n\nReturns:\n    str: The default dataset name specific to the data processor implementation.\n\nRaises:\n    NotImplementedError: If the property is accessed without being overridden in a subclass.\n\"\"\"",
                    "source_code": "raise NotImplementedError"
                },
                {
                    "docstring": null,
                    "method_name": "ensure_data_valid",
                    "second_doc": "\"\"\"\nValidates the format and consistency of input data to prevent errors during further image processing and augmentation operations.\n\nEnsuring data validity at this stage helps guarantee robust and reliable behavior throughout the data transformation pipeline.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "pass"
                },
                {
                    "docstring": null,
                    "method_name": "ensure_transforms_valid",
                    "second_doc": "\"\"\"\nValidates that the transformation pipeline applied to input data is structured correctly and can be executed without issues. This is important to prevent errors during data processing and model training by ensuring that augmentation operations are well-defined and compatible.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "pass"
                },
                {
                    "docstring": null,
                    "method_name": "postprocess",
                    "second_doc": "\"\"\"\nPost-processes the data by filtering and converting specified data fields to ensure consistency with the transformed image dimensions. This method ensures that all relevant components of the data sample are compatible and valid after augmentation steps, removing extraneous label fields to prepare the data for subsequent stages.\n\nArgs:\n    data (dict): Dictionary containing an \"image\" field and additional data fields that correspond to targets to be post-processed.\n\nReturns:\n    dict: The input dictionary with filtered, validated, and cleaned data fields, ready for use in further processing or modeling.\n\"\"\"",
                    "source_code": "rows, cols = data[\"image\"].shape[:2]\n\n        for data_name in self.data_fields:\n            data[data_name] = self.filter(data[data_name], rows, cols)\n            data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"from\")\n\n        data = self.remove_label_fields_from_data(data)\n        return data"
                },
                {
                    "docstring": null,
                    "method_name": "preprocess",
                    "second_doc": "\"\"\"\nPreprocesses the input data by ensuring compatibility and consistency between image and associated data fields.\n\nArgs:\n    data (dict): A dictionary containing the image and its related data fields to be processed.\n\nReturns:\n    dict: The updated dictionary with label fields added and data fields converted to match the image dimensions.\n\nWhy:\n    Standardizing and synchronizing image-related data is essential for accurate application of further transformations and augmentation steps, ensuring all components are properly aligned for downstream computer vision tasks.\n\"\"\"",
                    "source_code": "data = self.add_label_fields_to_data(data)\n\n        rows, cols = data[\"image\"].shape[:2]\n        for data_name in self.data_fields:\n            data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=\"to\")"
                },
                {
                    "docstring": null,
                    "method_name": "check_and_convert",
                    "second_doc": "\"\"\"\nValidates and converts data according to the specified format, ensuring compatibility with downstream image processing and augmentation workflows.\n\nArgs:\n    data: The input data to validate or convert, such as an image or annotation.\n    rows: The number of rows (height) of the input data.\n    cols: The number of columns (width) of the input data.\n    direction: Indicates the conversion direction. If \"to\", data is converted to the target format; otherwise, conversion is performed in the opposite direction.\n\nReturns:\n    The validated or converted data, matching the required format for further processing or augmentation.\n\nWhy:\n    This method ensures that input data conforms to expected structures and formats, which is crucial for reliable and consistent data handling during image transformation workflows.\n\"\"\"",
                    "source_code": "if self.params.format == \"albumentations\":\n            self.check(data, rows, cols)\n            return data\n        else:\n            if direction == \"to\":\n                return self.convert_to_albumentations(data, rows, cols)\n            else:\n                return self.convert_from_albumentations(data, rows, cols)"
                },
                {
                    "docstring": null,
                    "method_name": "filter",
                    "second_doc": "\"\"\"\nAbstract method for selecting data items that meet specific criteria or conditions. This enables downstream data processing or augmentation steps to operate only on relevant data, improving the efficiency and effectiveness of data transformation pipelines.\n\nArgs:\n    *args: Variable length argument list that allows flexibility for different selection criteria.\n    **kwargs: Arbitrary keyword arguments for specifying filtering conditions.\n\nReturns:\n    Filtered data or a subset of data meeting the specified conditions, with the exact return type defined by the implementing subclass.\n\"\"\"",
                    "source_code": "pass"
                },
                {
                    "docstring": null,
                    "method_name": "check",
                    "second_doc": "\"\"\"\nCheck if the input data meets the necessary requirements for processing. This ensures the subsequent augmentation or transformation steps operate on valid and compatible data formats.\n\nArgs:\n    data: The input data to be validated, typically an image or associated metadata.\n\nReturns:\n    bool: True if the input data passes all validation checks; False otherwise.\n\"\"\"",
                    "source_code": "pass"
                },
                {
                    "docstring": null,
                    "method_name": "convert_to_albumentations",
                    "second_doc": "\"\"\"\nConverts the internal data processing pipeline into a format compatible with the Albumentations library, allowing seamless integration of advanced augmentation strategies.\n\nArgs:\n    None\n\nReturns:\n    An Albumentations-compatible transformation pipeline that can be applied to image data.\n\"\"\"",
                    "source_code": "pass"
                },
                {
                    "docstring": null,
                    "method_name": "convert_from_albumentations",
                    "second_doc": "\"\"\"\nConvert data formatted according to albumentations conventions into a structure compatible with this processor.\n\nThis method enables seamless integration of data produced by external augmentation libraries, ensuring efficient preprocessing workflows and maintaining consistency within data pipelines.\n\nArgs:\n    albumentations_data: Data (such as images, masks, bounding boxes, or keypoints) formatted by albumentations.\n\nReturns:\n    Processed data converted into the internal representation required for further processing or model input.\n\"\"\"",
                    "source_code": "pass"
                },
                {
                    "docstring": null,
                    "method_name": "add_label_fields_to_data",
                    "second_doc": "\"\"\"\nAppends specified label fields as additional elements to each corresponding data entry in the dataset, modifying data records to incorporate annotation or target information alongside input features.\n\nArgs:\n    data (dict): A dictionary where keys are data field names and values are lists of data samples.\n\nReturns:\n    dict: The updated dictionary with label fields appended to each data entry for fields specified in self.data_fields.\n    \nWhy:\n    This method integrates auxiliary information, such as annotations or targets, with the core data samples to facilitate tasks that require joint access to both features and their associated metadata, particularly in preprocessing pipelines.\n\"\"\"",
                    "source_code": "if self.params.label_fields is None:\n            return data\n        for data_name in self.data_fields:\n            for field in self.params.label_fields:\n                data_with_added_field = []\n                for d, field_value in zip(data[data_name], data[field]):\n                    self.data_length = len(list(d))\n                    data_with_added_field.append(list(d) + [field_value])\n                data[data_name] = data_with_added_field\n        return data"
                },
                {
                    "docstring": null,
                    "method_name": "remove_label_fields_from_data",
                    "second_doc": "\"\"\"\nRemoves label fields from the input data and extracts their values into separate entries within the data dictionary.\n\nThis method processes structured annotation data, isolating specified label attributes (such as class IDs or scores) from their original lists (e.g., bounding box arrays) and storing them independently. This organization helps streamline downstream usage and ensures compatibility with subsequent data processing or modeling tasks.\n\nArgs:\n    data (dict): A dictionary containing image annotation information, where label fields are stored within the main arrays under each data field.\n\nReturns:\n    dict: The modified data dictionary, where label fields have been extracted and separated from the original annotation arrays.\n\"\"\"",
                    "source_code": "if self.params.label_fields is None:\n            return data\n        for data_name in self.data_fields:\n            for idx, field in enumerate(self.params.label_fields):\n                field_values = []\n                for bbox in data[data_name]:\n                    field_values.append(bbox[self.data_length + idx])\n                data[field] = field_values\n\n            data[data_name] = [d[: self.data_length] for d in data[data_name]]\n        return data"
                }
            ],
            "name": "DataProcessor",
            "type": "class"
        }
    ],
    "albumentations/albumentations/imgaug/__init__.py": [],
    "albumentations/albumentations/imgaug/transforms.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the BasicIAATransform with probability and application settings.\n\nArgs:\n    always_apply (bool): If True, the transformation will be applied to every data sample.\n    p (float): The probability of applying the transformation.\n\nReturns:\n    None\n\nThis method ensures that the transformation integrates seamlessly within the augmentation pipeline, providing control over when and how often it is applied to input samples.\n\"\"\"",
                    "source_code": "super(BasicIAATransform, self).__init__(always_apply, p)"
                },
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nReturns a no-operation (Noop) image augmentation transform, indicating that no changes will be applied to the input image. \n\nThis allows for the creation of flexible augmentation pipelines where certain steps can be explicitly defined as doing nothing, which can simplify configuration and ensure consistent processing flows.\n\nReturns:\n    iaa.Noop: An albumentations-compatible transform that performs no operation on the input data.\n\"\"\"",
                    "source_code": "return iaa.Noop()"
                },
                {
                    "docstring": null,
                    "method_name": "__call__",
                    "second_doc": "\"\"\"\nExecutes the transformation pipeline, applying the defined augmentation to the input data.\n\nThis method allows for the consistent application of transformation logic defined in parent classes, ensuring reliable and repeatable processing of dataset images during augmentation routines.\n\nArgs:\n    **kwargs: Arbitrary keyword arguments containing data (such as images, masks, bounding boxes, or keypoints) required for the transformation.\n\nReturns:\n    The augmented data, structured in the same format as the input, after applying the configured transformations.\n\"\"\"",
                    "source_code": "return super(BasicIAATransform, self).__call__(**kwargs)"
                },
                {
                    "docstring": null,
                    "method_name": "update_params",
                    "second_doc": "\"\"\"\nUpdates the transformation parameters by ensuring the processor operates deterministically, which helps maintain consistency across repeated augmentations or reproductions.\n\nArgs:\n    params (dict): Dictionary of current transformation parameters.\n    **kwargs: Additional keyword arguments that may provide extra configuration.\n\nReturns:\n    dict: Updated transformation parameters with a deterministic processor included.\n\nWhy:\n    This method ensures that the augmentation process produces consistent results across runs, which is important for reproducibility, especially during validation or testing scenarios.\n\"\"\"",
                    "source_code": "params = super(BasicIAATransform, self).update_params(params, **kwargs)\n        params[\"deterministic_processor\"] = self.processor.to_deterministic()\n        return params"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nApplies a deterministic image augmentation process to the input image.\n\nThis method processes the given image to produce an augmented version using a preconfigured, deterministic augmentation pipeline. By consistently transforming images in the same way, it helps maintain reproducibility in data preprocessing workflows and supports the generation of varied training samples for machine learning models.\n\nArgs:\n    img (numpy.ndarray): The input image to be augmented.\n\nReturns:\n    numpy.ndarray: The augmented image produced by the deterministic processor.\n\"\"\"",
                    "source_code": "return deterministic_processor.augment_image(img)"
                }
            ],
            "name": "BasicIAATransform",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "apply_to_bboxes",
                    "second_doc": "\"\"\"\nApply the same deterministic geometric transformation process to a set of bounding boxes, ensuring their coordinates remain consistent with augmented images.\n\nThis method is necessary to preserve the spatial relationships between bounding boxes and images after augmentation, preventing misalignment during tasks such as object detection.\n\nArgs:\n    bboxes (list): A list of bounding boxes, each typically represented as a list with at least four elements corresponding to box coordinates and optionally additional attributes.\n    rows (int): Number of image rows (height).\n    cols (int): Number of image columns (width).\n    deterministic_processor: An augmentation processor that applies a fixed (deterministic) sequence of transformations.\n\nReturns:\n    list: The transformed bounding boxes, converted back to the original expected coordinate format.\n\"\"\"",
                    "source_code": "if len(bboxes):\n            bboxes = convert_bboxes_from_albumentations(bboxes, \"pascal_voc\", rows=rows, cols=cols)\n\n            bboxes_t = ia.BoundingBoxesOnImage([ia.BoundingBox(*bbox[:4]) for bbox in bboxes], (rows, cols))\n            bboxes_t = deterministic_processor.augment_bounding_boxes([bboxes_t])[0].bounding_boxes\n            bboxes_t = [\n                [bbox.x1, bbox.y1, bbox.x2, bbox.y2] + list(bbox_orig[4:])\n                for (bbox, bbox_orig) in zip(bboxes_t, bboxes)\n            ]\n\n            bboxes = convert_bboxes_to_albumentations(bboxes_t, \"pascal_voc\", rows=rows, cols=cols)\n        return bboxes"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_keypoints",
                    "second_doc": "\"\"\"\nTransforms keypoints to match the augmented image by converting their format, applying the deterministic augmentation pipeline, and converting them back. This maintains the consistency and correctness of keypoint positions after image transformations.\n\nArgs:\n    keypoints (list): List of keypoints to be transformed, where each keypoint is typically a tuple or list containing coordinates and possibly additional information.\n    rows (int): Number of rows (height) of the image.\n    cols (int): Number of columns (width) of the image.\n    deterministic_processor: Augmentation processor to apply the deterministic image transformation.\n\nReturns:\n    list: Transformed keypoints in the same format as input, with updated coordinates corresponding to the augmented image.\n\"\"\"",
                    "source_code": "if len(keypoints):\n            keypoints = convert_keypoints_from_albumentations(keypoints, \"xy\", rows=rows, cols=cols)\n            keypoints_t = ia.KeypointsOnImage([ia.Keypoint(*kp[:2]) for kp in keypoints], (rows, cols))\n            keypoints_t = deterministic_processor.augment_keypoints([keypoints_t])[0].keypoints\n\n            bboxes_t = [[kp.x, kp.y] + list(kp_orig[2:]) for (kp, kp_orig) in zip(keypoints_t, keypoints)]\n\n            keypoints = convert_keypoints_to_albumentations(bboxes_t, \"xy\", rows=rows, cols=cols)\n        return keypoints"
                }
            ],
            "name": "DualIAATransform",
            "type": "class"
        },
        {
            "methods": [],
            "name": "ImageOnlyIAATransform",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the IAACropAndPad augmentation with the specified cropping and padding parameters. This allows for controlled modification of image borders during augmentation, which can help introduce variability and adaptability in visual data.\n\nArgs:\n    always_apply (bool): If True, the augmentation will be applied to every image.\n    p (float): Probability of applying the augmentation.\n    px (int, tuple, or None): Amount of pixels to crop or pad on each side of the image.\n    percent (float, tuple, or None): Proportion of image size to crop or pad on each side.\n    pad_mode (str): Padding mode to use if padding is applied (e.g., 'constant', 'edge').\n    pad_cval (number or tuple): Value to be used for constant padding.\n    keep_size (bool): Whether to keep the original image size after transformation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(IAACropAndPad, self).__init__(always_apply, p)\n        self.px = px\n        self.percent = percent\n        self.pad_mode = pad_mode\n        self.pad_cval = pad_cval\n        self.keep_size = keep_size"
                },
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nCreates an imgaug CropAndPad augmentation configured with the current parameters.\n\nThis property prepares a cropping and padding operation, allowing flexible image size adjustment as part of an augmentation pipeline. Using such augmentations increases variability in the dataset, which aids in making models more robust to scale and boundary effects.\n\nReturns:\n    iaa.CropAndPad: Configured imgaug augmentation for cropping and padding images based on set parameters (px, percent, pad_mode, pad_cval, keep_size).\n\"\"\"",
                    "source_code": "return iaa.CropAndPad(self.px, self.percent, self.pad_mode, self.pad_cval, self.keep_size)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to serialize and deserialize the transformation.\n\nReturns:\n    tuple: A tuple containing the argument names ('px', 'percent', 'pad_mode', 'pad_cval', 'keep_size') used to configure the crop and pad operation.\n\nWhy:\n    Providing a standardized way to access initialization argument names helps support the process of saving, loading, and reproducing transformation pipelines, ensuring consistency and traceability in augmentation workflows.\n\"\"\"",
                    "source_code": "return (\"px\", \"percent\", \"pad_mode\", \"pad_cval\", \"keep_size\")"
                }
            ],
            "name": "IAACropAndPad",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nCreates and returns an image augmentation operator that horizontally flips input images with 100% probability.\n\nThis is done to increase data diversity during training, helping models become less sensitive to the orientation of objects in the images.\n\nReturns:\n    iaa.Fliplr: An augmenter instance configured to perform a horizontal flip on every input image.\n\"\"\"",
                    "source_code": "return iaa.Fliplr(1)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required for serialization and reproducibility of this transform.  \nArgs:\n    None  \nReturns:\n    tuple: An empty tuple, indicating that this transform does not require any specific initialization arguments to be saved or restored.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "IAAFliplr",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nCreates a vertical flip augmentation that inverts images along the y-axis with a probability of 1.  \n\nThis is useful for increasing dataset diversity by generating vertically mirrored versions of the original images.  \n\nReturns:  \n    iaa.Flipud: An image augmentation object configured to always flip images vertically.\n\"\"\"",
                    "source_code": "return iaa.Flipud(1)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments for this transformation.\n\nSince this specific transformation does not require any parameters for initialization, an empty tuple is returned. This facilitates consistency and interoperability within transformation pipelines, allowing the transformation to be easily serialized and deserialized even when no arguments are needed.\n\nReturns:\n    tuple: An empty tuple, as there are no initialization arguments for this transformation.\n\"\"\"",
                    "source_code": "return ()"
                }
            ],
            "name": "IAAFlipud",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the IAAEmboss transformation with specified parameters to control the embossing effect applied to images.\n\nThis method sets up the necessary parameters for the emboss filter, defining the range of blending (alpha) and the strength of the effect. Parameter ranges are converted to tuples for consistent internal processing, enabling controlled variability in transformations.\n\nArgs:\n    always_apply (bool): Whether to always apply the transformation.\n    p (float): Probability of applying the transformation.\n    alpha (float or tuple of floats): Range for blending the source image and its embossed version.\n    strength (float or tuple of floats): Range controlling the intensity of the embossing effect.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(IAAEmboss, self).__init__(always_apply, p)\n        self.alpha = to_tuple(alpha, 0.0)\n        self.strength = to_tuple(strength, 0.0)"
                },
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nCreates an Emboss augmentation transform with the specified alpha and strength parameters. \n\nThis property allows users to configure and retrieve an embossing operation, which enhances edge features within images by simulating a 3D embossed effect. Such augmentation increases the diversity of visual features presented during image processing pipelines, supporting more robust model training.\n\nArgs:\n    self: Instance of the IAAEmboss class containing the alpha and strength attributes.\n\nReturns:\n    An instance of imgaug.augmenters.Emboss configured with the given alpha and strength values.\n\"\"\"",
                    "source_code": "return iaa.Emboss(self.alpha, self.strength)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to configure the emboss effect transform. This allows for consistent serialization, configuration, and reproducibility of the transformation.\n\nReturns:\n    tuple: A tuple containing the argument names (\"alpha\", \"strength\") used to initialize the transform.\n\"\"\"",
                    "source_code": "return (\"alpha\", \"strength\")"
                }
            ],
            "name": "IAAEmboss",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the IAASuperpixels transformation, setting the probability of replacement and the number of superpixel segments. This allows control over how much of the image is modified by superpixel segmentation, supporting diverse and challenging augmented datasets.\n\nArgs:\n    always_apply (bool): Whether to always apply this transform.\n    p (float): Probability of applying the transform.\n    p_replace (float): Probability of replacing a pixel with its superpixel value.\n    n_segments (int): Number of superpixels to generate.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(IAASuperpixels, self).__init__(always_apply, p)\n        self.p_replace = p_replace\n        self.n_segments = n_segments"
                },
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nReturns an instance of the Superpixels image transformation with the current configuration.\n\nThis property provides access to a transformation that can oversegment an image into superpixels, allowing for flexible manipulation of image regional composition to support downstream processing or augmentation steps.\n\nReturns:\n    iaa.Superpixels: Configured superpixel transformation that segments images according to the specified `p_replace` and `n_segments` parameters.\n\"\"\"",
                    "source_code": "return iaa.Superpixels(p_replace=self.p_replace, n_segments=self.n_segments)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to recreate the transform. This enables consistent serialization and reproducibility of transformation configurations.\n\nReturns:\n    tuple: A tuple containing the names of the initialization arguments ('p_replace', 'n_segments').\n\"\"\"",
                    "source_code": "return (\"p_replace\", \"n_segments\")"
                }
            ],
            "name": "IAASuperpixels",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the sharpening transformation by configuring the strength and blending of the effect.  \nArgs:\n    always_apply (bool): Whether to apply the transformation to every input regardless of the probability parameter.\n    p (float): Probability of applying the transformation.\n    alpha (float, tuple of float): Range for blending factor between the original and sharpened image.\n    lightness (float, tuple of float): Range for the lightness factor applied during sharpening.\n\nReturns:\n    None\n\nThe method sets up how strongly and in what manner sharpening will be applied, enabling flexible control over augmentation parameters for generating diverse transformed images.\n\"\"\"",
                    "source_code": "super(IAASharpen, self).__init__(always_apply, p)\n        self.alpha = to_tuple(alpha, 0)\n        self.lightness = to_tuple(lightness, 0)"
                },
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nProvides an instance of the image sharpening augmentation, allowing adjustment of image sharpness for transformation pipelines.\n\nReturns:\n    iaa.Sharpen: An augmenter configured with the specified alpha and lightness values to control the sharpening effect.\n\"\"\"",
                    "source_code": "return iaa.Sharpen(self.alpha, self.lightness)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the parameters required to initialize the sharpening transformation, allowing consistent initialization, serialization, and reproducibility of augmentation pipelines.\n\nReturns:\n    tuple: Names of the parameters (\"alpha\", \"lightness\") used during transformation initialization.\n\"\"\"",
                    "source_code": "return (\"alpha\", \"lightness\")"
                }
            ],
            "name": "IAASharpen",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the parameters for applying additive Gaussian noise to input images, controlling the noise distribution's mean, scale, and whether the noise is added per channel. This setup enables the generation of diverse and realistically perturbed data, which can aid in making trained models more resilient to variations and noise in real-world inputs.\n\nArgs:\n    loc (float): Mean of the Gaussian noise to be added.\n    scale (float, tuple of float): Standard deviation or range of standard deviation values for the Gaussian noise.\n    per_channel (bool): If True, noise is sampled and applied independently for each channel.\n    always_apply (bool): If True, the transform will be always applied. Defaults to False.\n    p (float): Probability of applying the transformation. Defaults to 0.5.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(IAAAdditiveGaussianNoise, self).__init__(always_apply, p)\n        self.loc = loc\n        self.scale = to_tuple(scale, 0.0)\n        self.per_channel = per_channel"
                },
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nCreates an instance of imgaug's AdditiveGaussianNoise augmenter configured with the current parameters.\n\nThis property allows for flexible addition of random Gaussian noise to images, which is commonly used to simulate sensor noise or variations during training, encouraging models to learn more robust features.\n\nArgs:\n    None\n\nReturns:\n    iaa.AdditiveGaussianNoise: An augmenter object from the imgaug library set up with the specified mean (loc), standard deviation (scale), and per-channel options for noise.\n\"\"\"",
                    "source_code": "return iaa.AdditiveGaussianNoise(self.loc, self.scale, self.per_channel)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to reproduce the noise transformation configuration. \n\nThis supports consistent serialization and deserialization of the transformation, ensuring reproducibility in data augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the initialization argument names ('loc', 'scale', 'per_channel').\n\"\"\"",
                    "source_code": "return (\"loc\", \"scale\", \"per_channel\")"
                }
            ],
            "name": "IAAAdditiveGaussianNoise",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the piecewise affine transformation parameters for image augmentation, enabling localized, non-linear geometric distortions within an image by dividing it into a grid structure.\n\nArgs:\n    always_apply (bool): Whether to always apply this transformation.\n    p (float): Probability of applying the transformation.\n    scale (float or tuple): Range for scaling grid distortion magnitude.\n    nb_rows (int): Number of grid rows for piecewise transformation.\n    nb_cols (int): Number of grid columns for piecewise transformation.\n    order (int): Interpolation order to use for transformation.\n    cval (int or float): Value used for points outside image boundaries.\n    mode (str): Pixel extrapolation method.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(IAAPiecewiseAffine, self).__init__(always_apply, p)\n        self.scale = to_tuple(scale, 0.0)\n        self.nb_rows = nb_rows\n        self.nb_cols = nb_cols\n        self.order = order\n        self.cval = cval\n        self.mode = mode"
                },
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nCreates and returns a PiecewiseAffine image transformation configured with the current parameters.\n\nThis property constructs a piecewise affine transformation using the specified settings to apply local, non-linear distortions to an image. Applying such transformations increases the diversity of training samples by simulating complex geometric variations, which can improve the robustness of computer vision models.\n\nArgs:\n    None\n\nReturns:\n    iaa.PiecewiseAffine: An instance of the PiecewiseAffine augmentation with configured scale, grid size, interpolation order, fill value, and border mode.\n\"\"\"",
                    "source_code": "return iaa.PiecewiseAffine(self.scale, self.nb_rows, self.nb_cols, self.order, self.cval, self.mode)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the initialization arguments required to configure the piecewise affine transformation.\n\nThis method helps ensure that all necessary parameters are available for reproducibility and serialization of the transformation, which is essential for creating consistent augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the names of the parameters used for initialization (\"scale\", \"nb_rows\", \"nb_cols\", \"order\", \"cval\", \"mode\").\n\"\"\"",
                    "source_code": "return (\"scale\", \"nb_rows\", \"nb_cols\", \"order\", \"cval\", \"mode\")"
                }
            ],
            "name": "IAAPiecewiseAffine",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the parameters for applying affine transformations, allowing control over scaling, translation, rotation, and shearing of input images. Setting these parameters enables flexible modification of images, which is often used to improve the robustness of models by exposing them to a wider range of geometric variations.\n\nArgs:\n    always_apply (bool): If True, the transformation will be applied to every image.\n    p (float): Probability of applying the transform.\n    scale (float, tuple): Scaling factor(s) for resizing the image.\n    translate_percent (float, tuple): Amount of translation as a percentage of the image size.\n    translate_px (int, tuple): Amount of translation in pixels.\n    rotate (float, tuple): Angle(s) in degrees to rotate the image.\n    shear (float, tuple): Shear angle(s) in degrees.\n    order (int): Interpolation order used for resizing.\n    cval (numeric): Value used for padding if required.\n    mode (str): Pixel filling mode for areas outside the boundaries.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(IAAAffine, self).__init__(always_apply, p)\n        self.scale = to_tuple(scale, 1.0)\n        self.translate_percent = to_tuple(translate_percent, 0)\n        self.translate_px = to_tuple(translate_px, 0)\n        self.rotate = to_tuple(rotate)\n        self.shear = to_tuple(shear)\n        self.order = order\n        self.cval = cval\n        self.mode = mode"
                },
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nCreates an affine transformation processor with the current parameter settings.\n\nThis property constructs an affine transformation object using the stored scaling, translation, rotation, skew, interpolation, fill value, and border handling parameters, so these transformations can be consistently applied to images to modify their spatial arrangement for downstream tasks.\n\nArgs:\n    None\n\nReturns:\n    iaa.Affine: An affine transformation instance configured with the stored parameters.\n\"\"\"",
                    "source_code": "return iaa.Affine(\n            self.scale,\n            self.translate_percent,\n            self.translate_px,\n            self.rotate,\n            self.shear,\n            self.order,\n            self.cval,\n            self.mode,\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of arguments required to initialize the affine transformation, facilitating consistent parameter handling across different transformation processes.\n\nReturns:\n    tuple: Argument names used for initializing the transformation, including scale, translation (percent and pixels), rotation, shear, interpolation order, constant value, and mode.\n\"\"\"",
                    "source_code": "return (\"scale\", \"translate_percent\", \"translate_px\", \"rotate\", \"shear\", \"order\", \"cval\", \"mode\")"
                }
            ],
            "name": "IAAAffine",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an instance for applying a perspective transformation to an image, configuring the transformation's probability, scaling limits, and output size behavior.\n\nArgs:\n    always_apply (bool): If set to True, the transformation will be applied to every input.\n    p (float): Probability of applying the transformation.\n    scale (float or tuple of float): Range for scaling the transformation effect.\n    keep_size (bool): If True, the output image size remains the same as the input.\n\nReturns:\n    None\n\nWhy:\n    The method sets up transformation parameters to allow flexible and controlled modification of image perspective, thereby supporting the creation of diverse augmented versions of input data that help improve model robustness.\n\"\"\"",
                    "source_code": "super(IAAPerspective, self).__init__(always_apply, p)\n        self.scale = to_tuple(scale, 1.0)\n        self.keep_size = keep_size"
                },
                {
                    "docstring": null,
                    "method_name": "processor",
                    "second_doc": "\"\"\"\nConstructs a perspective transformation operation to apply geometric modifications to images, which can help introduce spatial variance during data preprocessing.\n\nReturns:\n    iaa.PerspectiveTransform: An object representing the configured perspective transformation, ready to be used in an image augmentation pipeline.\n\"\"\"",
                    "source_code": "return iaa.PerspectiveTransform(self.scale, keep_size=self.keep_size)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the arguments required to initialize the perspective transformation, ensuring configuration consistency and enabling reproducibility of augmentation pipelines.\n\nReturns:\n    tuple: A tuple containing the names of the required initialization arguments (\"scale\", \"keep_size\").\n\"\"\"",
                    "source_code": "return (\"scale\", \"keep_size\")"
                }
            ],
            "name": "IAAPerspective",
            "type": "class"
        }
    ],
    "albumentations/albumentations/pytorch/__init__.py": [],
    "albumentations/albumentations/pytorch/functional.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "img_to_tensor",
                "second_doc": "\"\"\"\nConverts a NumPy image array to a PyTorch tensor with channels-first format and appropriate scaling, optionally normalizing the resulting tensor.\n\nThis method facilitates seamless integration of image data into deep learning pipelines by ensuring consistent tensor formatting and value ranges expected by neural networks.\n\nArgs:\n    im (np.ndarray): Input image array in HWC format, typically with dtype np.uint8 or float.\n    normalize (dict, optional): Normalization parameters to be passed to torch.nn.functional.normalize. If provided, the tensor is normalized accordingly.\n\nReturns:\n    torch.Tensor: The processed image tensor in CHW format, scaled to [0, 1] if dtype is uint8, and float32 type. Returns a normalized tensor if normalization parameters are given.\n\"\"\"",
                "source_code": "tensor = torch.from_numpy(np.moveaxis(im / (255.0 if im.dtype == np.uint8 else 1), -1, 0).astype(np.float32))\n    if normalize is not None:\n        return F.normalize(tensor, **normalize)\n    return tensor"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "mask_to_tensor",
                "second_doc": "\"\"\"\nConverts a segmentation mask to a PyTorch tensor, normalizing and reformatting its representation based on the number of classes and activation type. This standardization ensures the mask has the appropriate format and dtype required for downstream model training and evaluation.\n\nArgs:\n    mask (np.ndarray): Input segmentation mask, either 2D or 3D, possibly multi-channel.\n    num_classes (int): Number of classes for segmentation (used to select the correct mask format).\n    sigmoid (bool): Whether to use sigmoid activation (binary/multi-label) or softmax (multiclass).\n\nReturns:\n    torch.Tensor: Processed mask in tensor format with correct layout and normalization for model input.\n\"\"\"",
                "source_code": "if num_classes > 1:\n        if not sigmoid:\n            # softmax\n            long_mask = np.zeros((mask.shape[:2]), dtype=np.int64)\n            if len(mask.shape) == 3:\n                for c in range(mask.shape[2]):\n                    long_mask[mask[..., c] > 0] = c\n            else:\n                long_mask[mask > 127] = 1\n                long_mask[mask == 0] = 0\n            mask = long_mask\n        else:\n            mask = np.moveaxis(mask / (255.0 if mask.dtype == np.uint8 else 1), -1, 0).astype(np.float32)\n    else:\n        mask = np.expand_dims(mask / (255.0 if mask.dtype == np.uint8 else 1), 0).astype(np.float32)\n    return torch.from_numpy(mask)"
            },
            "type": "function"
        }
    ],
    "albumentations/albumentations/pytorch/transforms.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "img_to_tensor",
                "second_doc": "\"\"\"\nConvert a numpy image array to a PyTorch tensor with channel-first format and float32 dtype, optionally applying normalization.\n\nArgs:\n    im (np.ndarray): Input image array, typically with values in the range [0, 255] for uint8 or [0, 1] for floating point types.\n    normalize (dict, optional): Parameters for normalization using torch.nn.functional.normalize. If provided, applies normalization to the output tensor.\n\nReturns:\n    torch.Tensor: The processed tensor suitable for input to PyTorch models, with optional normalization applied.\n\nWhy:\n    This method preprocesses images into a format compatible with deep learning frameworks, ensuring proper value scaling and layout for model training or inference, and optionally applying normalization to standardize representation.\n\"\"\"",
                "source_code": "tensor = torch.from_numpy(np.moveaxis(im / (255.0 if im.dtype == np.uint8 else 1), -1, 0).astype(np.float32))\n    if normalize is not None:\n        return F.normalize(tensor, **normalize)\n    return tensor"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "mask_to_tensor",
                "second_doc": "\"\"\"\nConverts a mask array into a PyTorch tensor formatted for use in segmentation tasks, ensuring compatibility with deep learning models that require masks to be encoded in class or probability format.\n\nThis method processes the input mask depending on the number of classes and the type of activation used\u2014either transforming class masks to class indices or normalizing them to probabilities. This conversion is essential for preparing data in a structure suitable for training or inferring with segmentation neural networks.\n\nArgs:\n    mask (np.ndarray): Input mask array, typically representing label classes or probabilities.\n    num_classes (int): Number of segmentation classes.\n    sigmoid (bool): Whether to apply sigmoid normalization (for multi-label or binary masks).\n\nReturns:\n    torch.Tensor: The processed mask as a tensor, ready for use as a model input or target.\n\"\"\"",
                "source_code": "if num_classes > 1:\n        if not sigmoid:\n            # softmax\n            long_mask = np.zeros((mask.shape[:2]), dtype=np.int64)\n            if len(mask.shape) == 3:\n                for c in range(mask.shape[2]):\n                    long_mask[mask[..., c] > 0] = c\n            else:\n                long_mask[mask > 127] = 1\n                long_mask[mask == 0] = 0\n            mask = long_mask\n        else:\n            mask = np.moveaxis(mask / (255.0 if mask.dtype == np.uint8 else 1), -1, 0).astype(np.float32)\n    else:\n        mask = np.expand_dims(mask / (255.0 if mask.dtype == np.uint8 else 1), 0).astype(np.float32)\n    return torch.from_numpy(mask)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ToTensor transformation with options for class number specification, sigmoid activation, and normalization. This setup enables consistent conversion of input data into tensor format, ensuring compatibility with deep learning frameworks and preprocessing standards.\n\nArgs:\n    num_classes (int): The number of target classes for encoding purposes.\n    sigmoid (bool): Whether to apply sigmoid activation to the output.\n    normalize (bool): Whether to normalize the tensor after conversion.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ToTensor, self).__init__(always_apply=True, p=1.0)\n        self.num_classes = num_classes\n        self.sigmoid = sigmoid\n        self.normalize = normalize\n        warnings.warn(\n            \"ToTensor is deprecated and will be replaced by ToTensorV2 \" \"in albumentations 0.5.0\", DeprecationWarning\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "__call__",
                    "second_doc": "\"\"\"\nConverts image and mask data contained in the input dictionary to tensor format suitable for further processing in deep learning pipelines. Applies normalization if specified, and handles any additional targets defined by the user.\n\nArgs:\n    **kwargs: Arbitrary keyword arguments containing at least an \"image\" key and optionally a \"mask\" key, as well as any additional targets. Each value should contain image or mask data to be transformed.\n\nReturns:\n    dict: The input dictionary with images and masks converted to tensor representations.\n\nWhy:\n    Converting images and masks to tensors ensures compatibility with subsequent steps in a computer vision pipeline, such as model training or evaluation, and maintains data consistency across transformations.\n\"\"\"",
                    "source_code": "kwargs.update({\"image\": img_to_tensor(kwargs[\"image\"], self.normalize)})\n        if \"mask\" in kwargs.keys():\n            kwargs.update({\"mask\": mask_to_tensor(kwargs[\"mask\"], self.num_classes, sigmoid=self.sigmoid)})\n\n        for k, _v in kwargs.items():\n            if self._additional_targets.get(k) == \"image\":\n                kwargs.update({k: img_to_tensor(kwargs[k], self.normalize)})\n            if self._additional_targets.get(k) == \"mask\":\n                kwargs.update({k: mask_to_tensor(kwargs[k], self.num_classes, sigmoid=self.sigmoid)})\n        return kwargs"
                },
                {
                    "docstring": null,
                    "method_name": "targets",
                    "second_doc": "\"\"\"\nProperty that should specify the targets handled by the transformation, such as images, masks, or other annotations.\n\nThe property is intentionally left unimplemented to enforce that subclasses define the relevant targets for their specific use cases. This ensures that each transformation operates only on the intended types of data elements and prevents incorrect application to unsupported data.\n\nRaises:\n    NotImplementedError: Always raised to require subclasses to implement this property.\n\"\"\"",
                    "source_code": "raise NotImplementedError"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns the names of the arguments required to initialize the transformation, ensuring that all necessary parameters are tracked for reproducibility and correct application of the transform.\n\nReturns:\n    tuple: A tuple containing the names of parameters (\"num_classes\", \"sigmoid\", \"normalize\") needed to initialize the transformation.\n\"\"\"",
                    "source_code": "return \"num_classes\", \"sigmoid\", \"normalize\""
                }
            ],
            "name": "ToTensor",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ToTensorV2 transformation, configuring whether it should always be applied and with what probability. This setup is essential for defining the transformation's behavior during the application of a data processing pipeline.\n\nArgs:\n    always_apply (bool): If set to True, the transformation will be applied to every input without randomness.\n    p (float): Probability of applying the transformation. Must be between 0 and 1.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ToTensorV2, self).__init__(always_apply=always_apply, p=p)"
                },
                {
                    "docstring": null,
                    "method_name": "targets",
                    "second_doc": "\"\"\"\nDefines the mapping of each data target to the corresponding transformation function, ensuring input images and masks are converted appropriately before further processing.\n\nReturns:\n    dict: A dictionary with target types as keys (\"image\", \"mask\") and their corresponding processing methods as values.\n\"\"\"",
                    "source_code": "return {\"image\": self.apply, \"mask\": self.apply_to_mask}"
                },
                {
                    "docstring": null,
                    "method_name": "apply",
                    "second_doc": "\"\"\"\nConverts an image from a NumPy array with shape (height, width, channels) to a PyTorch tensor with shape (channels, height, width).\n\nThis transformation standardizes input image format for efficient batch processing in deep learning workflows.\n\nArgs:\n    img (numpy.ndarray): Image array with shape (H, W, C).\n\nReturns:\n    torch.Tensor: Transposed image as a PyTorch tensor with shape (C, H, W).\n\"\"\"",
                    "source_code": "return torch.from_numpy(img.transpose(2, 0, 1))"
                },
                {
                    "docstring": null,
                    "method_name": "apply_to_mask",
                    "second_doc": "\"\"\"\nConverts a given mask from a NumPy array to a PyTorch tensor to ensure compatibility with PyTorch operations.\n\nArgs:\n    mask (numpy.ndarray): The segmentation mask to be converted.\n\nReturns:\n    torch.Tensor: The mask represented as a PyTorch tensor.\n\"\"\"",
                    "source_code": "return torch.from_numpy(mask)"
                },
                {
                    "docstring": null,
                    "method_name": "get_transform_init_args_names",
                    "second_doc": "\"\"\"\nReturns a list of argument names required to initialize the transform.\n\nReturns:\n    list: An empty list, indicating that this transform does not require any initialization arguments.\n\nThis design ensures compatibility within transformation pipelines, allowing seamless integration even when no additional parameters are needed for serialization or reproducibility.\n\"\"\"",
                    "source_code": "return []"
                },
                {
                    "docstring": null,
                    "method_name": "get_params_dependent_on_targets",
                    "second_doc": "\"\"\"\nRetrieve parameters that depend on multiple related data elements, such as images and corresponding annotations, during a tensor conversion transformation.\n\nSince this method returns an empty dictionary, it indicates that this transformation does not require any target-dependent parameters, ensuring compatibility and flexibility when applied in various data processing pipelines.\n\nReturns:\n    dict: An empty dictionary as this transformation does not need parameters dependent on targets.\n\"\"\"",
                    "source_code": "return {}"
                }
            ],
            "name": "ToTensorV2",
            "type": "class"
        }
    ],
    "albumentations/benchmark/benchmark.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "parse_args",
                "second_doc": "\"\"\"\nParses command-line arguments to configure image benchmarking parameters such as input data location, the number of images, the number of benchmark runs, output display options, and package version reporting. This enables users to customize how performance measurements are conducted and reported, supporting flexible experimentation and reproducibility.\n\nArgs:\n    None\n\nReturns:\n    argparse.Namespace: An object containing the parsed command-line arguments with their values.\n\"\"\"",
                "source_code": "parser = argparse.ArgumentParser(description=\"Augmentation libraries performance benchmark\")\n    parser.add_argument(\"-d\", \"--data-dir\", required=True, metavar=\"DIR\", help=\"path to a directory with images\")\n    parser.add_argument(\n        \"-i\", \"--images\", default=2000, type=int, metavar=\"N\", help=\"number of images for benchmarking (default: 2000)\"\n    )\n    parser.add_argument(\n        \"-r\", \"--runs\", default=5, type=int, metavar=\"N\", help=\"number of runs for each benchmark (default: 5)\"\n    )\n    parser.add_argument(\n        \"--show-std\", dest=\"show_std\", action=\"store_true\", help=\"show standard deviation for benchmark runs\"\n    )\n    parser.add_argument(\"-p\", \"--print-package-versions\", action=\"store_true\", help=\"print versions of packages\")\n    return parser.parse_args()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "print_package_versions",
                "second_doc": "\"\"\"\nPrints the version information for Python and a set of popular image processing and machine learning libraries installed in the current environment.\n\nThis information is collected to help ensure reproducibility, facilitate debugging, and aid in identifying compatibility issues that may arise due to differences between library versions.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "packages = [\n        \"albumentations\",\n        \"imgaug\",\n        \"torchvision\",\n        \"keras\",\n        \"numpy\",\n        \"opencv-python\",\n        \"scikit-image\",\n        \"scipy\",\n        \"pillow\",\n        \"pillow-simd\",\n        \"Augmentor\",\n        \"solt\",\n    ]\n    package_versions = {\"python\": sys.version}\n    for package in packages:\n        try:\n            package_versions[package] = pkg_resources.get_distribution(package).version\n        except pkg_resources.DistributionNotFound:\n            pass\n    print(package_versions)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "read_img_pillow",
                "second_doc": "\"\"\"\nReads an image file from the given path using the Pillow library and ensures it is in RGB format.  \nThis allows consistent handling of images with potentially varying formats or color channels before further processing.\n\nArgs:\n    path (str): The file path to the image to be loaded.\n\nReturns:\n    PIL.Image.Image: The loaded image converted to RGB color mode.\n\"\"\"",
                "source_code": "with open(path, \"rb\") as f:\n        img = Image.open(f)\n        return img.convert(\"RGB\")"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "read_img_cv2",
                "second_doc": "\"\"\"\nReads an image from the specified file path using OpenCV, then converts its color space from BGR to RGB. This ensures compatibility with libraries and functions that expect image data in the RGB format.\n\nArgs:\n    filepath (str): Path to the image file to be read.\n\nReturns:\n    numpy.ndarray: The image loaded into a NumPy array with color channels in RGB order.\n\"\"\"",
                "source_code": "img = cv2.imread(filepath)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "format_results",
                "second_doc": "\"\"\"\nFormats statistical summaries of image processing speeds for display purposes.\n\nThis function computes the average throughput (in images per second) from a list of measurements, optionally appending the standard deviation to highlight variability in performance. Presenting these statistics helps users compare and assess efficiency at a glance.\n\nArgs:\n    images_per_second_for_aug (list or None): Collection of throughput measurements for a given augmentation. If None, indicates unavailable results.\n    show_std (bool): Whether to include the standard deviation in the formatted output.\n\nReturns:\n    str: A formatted string with the average images per second, optionally including the standard deviation, or \"-\" if no data is available.\n\"\"\"",
                "source_code": "if images_per_second_for_aug is None:\n        return \"-\"\n    result = str(math.floor(np.mean(images_per_second_for_aug)))\n    if show_std:\n        result += \" \u00b1 {}\".format(math.ceil(np.std(images_per_second_for_aug)))\n    return result"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__str__",
                    "second_doc": "\"\"\"\nReturns a string representation of the test class, primarily for clear identification in logs or reports.\n\nReturns:\n    str: The name of the class.\n\"\"\"",
                    "source_code": "return self.__class__.__name__"
                },
                {
                    "docstring": null,
                    "method_name": "imgaug",
                    "second_doc": "\"\"\"\nApplies a predefined image augmentation pipeline to the input image using the configured transformation strategy.\n\nThis method enables systematic modification of input data, facilitating increased data diversity and model robustness during experimentation and evaluation.\n\nArgs:\n    img (numpy.ndarray): Input image to be augmented.\n\nReturns:\n    numpy.ndarray: The augmented image after transformation.\n\"\"\"",
                    "source_code": "return self.imgaug_transform.augment_image(img)"
                },
                {
                    "docstring": null,
                    "method_name": "augmentor",
                    "second_doc": "\"\"\"\nApplies a predefined augmentation operation to the input image and returns a NumPy array representation of the processed image.  \nThis method standardizes the preprocessing of image data to ensure consistency and reliability in subsequent analysis or model training.\n\nArgs:\n    img: Input image to be augmented.\n\nReturns:\n    np.ndarray: Augmented image as a NumPy array of uint8 type.\n\"\"\"",
                    "source_code": "img = self.augmentor_op.perform_operation([img])[0]\n        return np.array(img, np.uint8, copy=True)"
                },
                {
                    "docstring": null,
                    "method_name": "solt",
                    "second_doc": "\"\"\"\nApplies a sequence of image transformations to the input image using a predefined augmentation pipeline.\n\nThis method processes an input image by encapsulating it in a data structure and passing it through a stream of augmentation operations, ultimately extracting and returning the transformed image. This automated transformation facilitates robust preparation of image data for tasks such as model training or evaluation.\n\nArgs:\n    img: Input image to be augmented.\n\nReturns:\n    The augmented version of the input image as produced by the transformation pipeline.\n\"\"\"",
                    "source_code": "dc = sld.DataContainer(img, \"I\")\n        dc = self.solt_stream(dc)\n        return dc.data[0]"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision",
                    "second_doc": "\"\"\"\nApply a predefined sequence of image transformations using a torchvision-style pipeline and convert the result to a NumPy array.\n\nThis method ensures the input image is processed in a consistent manner before further use or evaluation, facilitating interoperability with NumPy-based workflows.\n\nArgs:\n    img: The input image to be transformed.\n\nReturns:\n    np.ndarray: The transformed image as a NumPy array of type uint8.\n\"\"\"",
                    "source_code": "img = self.torchvision_transform(img)\n        return np.array(img, np.uint8, copy=True)"
                },
                {
                    "docstring": null,
                    "method_name": "is_supported_by",
                    "second_doc": "\"\"\"\nCheck if a particular augmentation library is supported by verifying the presence of corresponding transform or pipeline attributes in the class.\n\nArgs:\n    library (str): The name of the augmentation library to check for support.\n\nReturns:\n    bool: True if the class has the required attribute(s) for the specified library, False otherwise.\n\nThis method enables dynamic compatibility checks with various augmentation libraries, allowing flexible integration based on available transformations.\n\"\"\"",
                    "source_code": "if library == \"imgaug\":\n            return hasattr(self, \"imgaug_transform\")\n        elif library == \"augmentor\":\n            return hasattr(self, \"augmentor_op\") or hasattr(self, \"augmentor_pipeline\")\n        elif library == \"solt\":\n            return hasattr(self, \"solt_stream\")\n        elif library == \"torchvision\":\n            return hasattr(self, \"torchvision_transform\")\n        else:\n            return hasattr(self, library)"
                },
                {
                    "docstring": null,
                    "method_name": "run",
                    "second_doc": "\"\"\"\nApplies a specified image transformation to each image in the provided list.\n\nThis method iterates over a batch of images and invokes the transformation function associated with the given library name. Running this process helps test the performance, compatibility, or effects of applying different augmentation pipelines to images in bulk.\n\nArgs:\n    library (str): The name of the transformation library method to retrieve and apply.\n    imgs (list): A list of image objects to be transformed.\n\nReturns:\n    None: The method performs transformations in-place and does not return any value.\n\"\"\"",
                    "source_code": "transform = getattr(self, library)\n        for img in imgs:\n            transform(img)"
                }
            ],
            "name": "BenchmarkTest",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the HorizontalFlip class by setting up equivalent horizontal flip operations across multiple image augmentation libraries, ensuring consistent transformation behavior.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Establishing horizontal flip operations for different libraries enables unified augmentation pipelines and ensures consistent preprocessing steps when experimenting with various frameworks or integrating multiple augmentation strategies.\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.Fliplr(p=1)\n        self.augmentor_op = Operations.Flip(probability=1, top_bottom_left_right=\"LEFT_RIGHT\")\n        self.solt_stream = slc.Stream([slt.RandomFlip(p=1, axis=1)])"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nFlip the input image horizontally.\n\nThis method selects the most appropriate algorithm to apply a horizontal flip based on the image's dimensions, channel count, and data type to ensure both compatibility and optimal performance.\n\nArgs:\n    img (numpy.ndarray): Input image to be flipped. Should be a 2D or 3D NumPy array.\n\nReturns:\n    numpy.ndarray: Horizontally flipped image with the same dtype and shape as the input.\n\"\"\"",
                    "source_code": "if img.ndim == 3 and img.shape[2] > 1 and img.dtype == np.uint8:\n            return albumentations.hflip_cv2(img)\n        else:\n            return albumentations.hflip(img)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nApplies a horizontal flip to the input image using torchvision, reversing the order of pixels along the horizontal axis.\n\nThis operation increases image diversity, which helps improve the robustness and generalization of machine learning models during training.\n\nArgs:\n    img (PIL.Image or Tensor): The input image to be horizontally flipped.\n\nReturns:\n    PIL.Image or Tensor: The horizontally flipped image.\n\"\"\"",
                    "source_code": "return torchvision.hflip(img)"
                },
                {
                    "docstring": null,
                    "method_name": "keras",
                    "second_doc": "\"\"\"\nFlips the given image horizontally using Keras utilities to create a contiguous array.\n\nThis operation is performed to diversify the dataset by generating mirror images, which can help machine learning models become more robust to variations in object orientation.\n\nArgs:\n    img (np.ndarray): The input image to be flipped.\n\nReturns:\n    np.ndarray: The horizontally flipped version of the input image as a contiguous array.\n\"\"\"",
                    "source_code": "return np.ascontiguousarray(keras.flip_axis(img, axis=1))"
                },
                {
                    "docstring": null,
                    "method_name": "imgaug",
                    "second_doc": "\"\"\"\nApplies a horizontal flip transformation to the input image, returning it as a contiguous NumPy array to ensure compatibility and optimal memory layout for downstream processing.\n\nArgs:\n    img (numpy.ndarray): Input image to be augmented.\n\nReturns:\n    numpy.ndarray: Horizontally flipped image stored in a contiguous memory block.\n\"\"\"",
                    "source_code": "return np.ascontiguousarray(self.imgaug_transform.augment_image(img))"
                }
            ],
            "name": "HorizontalFlip",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes vertical flip augmentation transforms for use with multiple image augmentation backends.\n\nThis constructor prepares three different vertical flipping operations from separate augmentation libraries, all configured to flip images vertically with probability 1. This allows seamless integration and comparison across pipelines that may rely on different frameworks.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.Flipud(p=1)\n        self.augmentor_op = Operations.Flip(probability=1, top_bottom_left_right=\"TOP_BOTTOM\")\n        self.solt_stream = slc.Stream([slt.RandomFlip(p=1, axis=0)])"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nApplies a vertical flip transformation to the input image, producing a mirrored version along the vertical axis.\nThis augmentation increases the variability of the dataset, which can help reduce overfitting and make image-based models more robust.\n\nArgs:\n    img (numpy.ndarray): The input image to be vertically flipped.\n\nReturns:\n    numpy.ndarray: The vertically flipped image.\n\"\"\"",
                    "source_code": "return albumentations.vflip(img)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nApply a vertical flip to the input image using torchvision's vflip function.\n\nThis transformation is used to increase the diversity of the dataset by creating vertically flipped versions of images, helping models become more invariant to the orientation of objects.\n\nArgs:\n    img (PIL.Image or torch.Tensor): Input image to be transformed.\n\nReturns:\n    PIL.Image or torch.Tensor: Vertically flipped image of the same type as the input.\n\"\"\"",
                    "source_code": "return torchvision.vflip(img)"
                },
                {
                    "docstring": null,
                    "method_name": "keras",
                    "second_doc": "\"\"\"\nFlip the input image vertically along the first axis to augment the dataset.\n\nArgs:\n    img (numpy.ndarray): The input image to be flipped.\n\nReturns:\n    numpy.ndarray: The vertically flipped image in contiguous array format.\n\nWhy:\n    Vertically flipping images increases data variation, which helps reduce overfitting and improves model robustness during training and evaluation.\n\"\"\"",
                    "source_code": "return np.ascontiguousarray(keras.flip_axis(img, axis=0))"
                },
                {
                    "docstring": null,
                    "method_name": "imgaug",
                    "second_doc": "\"\"\"\nApplies a vertical flip augmentation to the input image using a predefined transformation and ensures the output is stored in a contiguous memory block.\n\nThis method is used to introduce geometric diversity into the dataset, helping models generalize better by simulating images viewed from different perspectives.\n\nArgs:\n    img (numpy.ndarray): Input image to be augmented.\n\nReturns:\n    numpy.ndarray: Augmented image with a vertical flip, stored as a contiguous array in memory.\n\"\"\"",
                    "source_code": "return np.ascontiguousarray(self.imgaug_transform.augment_image(img))"
                }
            ],
            "name": "VerticalFlip",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes rotation transformations using multiple augmentation libraries, each configured with fixed 45-degree rotations. This setup ensures consistent application of rotation augmentations across different tools, facilitating comparative evaluation and integration in diverse data processing pipelines.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Using various implementations of rotation allows for flexibility, interoperability, and validation across libraries, supporting comprehensive testing and robust augmentation workflows.\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.Affine(rotate=(45, 45), order=1, mode=\"reflect\")\n        self.augmentor_op = Operations.RotateStandard(probability=1, max_left_rotation=45, max_right_rotation=45)\n        self.solt_stream = slc.Stream([slt.RandomRotate(p=1, rotation_range=(45, 45))], padding=\"r\")"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nRotate the input image by -45 degrees using a geometric transformation.\n\nThis method applies a fixed-angle rotation to the input image, which can help introduce consistent geometric variation. Such augmentation is often used to improve the robustness of image analysis models by exposing them to transformed versions of the data.\n\nArgs:\n    img: The input image to be rotated. Expected as a NumPy array or an image object compatible with Albumentations.\n\nReturns:\n    The rotated image, with the same type as the input.\n\"\"\"",
                    "source_code": "return albumentations.rotate(img, angle=-45)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nRotate the input image by 45 degrees counterclockwise using bilinear interpolation, adapting it for compatibility with torchvision transformations.\n\nArgs:\n    img (PIL.Image or ndarray): The image to be rotated.\n\nReturns:\n    PIL.Image: The rotated image.\n    \nWhy:\n    Rotating images exposes the model to varied orientations of data, which increases the robustness and generalization of computer vision models during training.\n\"\"\"",
                    "source_code": "return torchvision.rotate(img, angle=-45, resample=Image.BILINEAR)"
                },
                {
                    "docstring": null,
                    "method_name": "keras",
                    "second_doc": "\"\"\"\nApplies a 45-degree affine rotation to the input image using Keras' transformation utilities.\n\nThis method is used to simulate rotated versions of an image, helping to expose a model to different orientations of data during augmentation and improve its ability to generalize to unseen examples.\n\nArgs:\n    img (numpy.ndarray): Input image to be rotated.\n\nReturns:\n    numpy.ndarray: The rotated image with the same shape as the input.\n\"\"\"",
                    "source_code": "return keras.apply_affine_transform(img, theta=45, channel_axis=2, fill_mode=\"reflect\")"
                }
            ],
            "name": "Rotate",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes multiple brightness adjustment transformations from different augmentation libraries with fixed parameters.\n\nThis setup allows for the consistent application of brightness modification across different augmentation frameworks, ensuring experiment reproducibility and enabling straightforward comparison of results when integrating with various image augmentation pipelines.\n\nArgs:\n    self: Instance of the Brightness class.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.Add((127, 127), per_channel=False)\n        self.augmentor_op = Operations.RandomBrightness(probability=1, min_factor=1.5, max_factor=1.5)\n        self.solt_stream = slc.Stream([slt.ImageRandomBrightness(p=1, brightness_range=(127, 127))])"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nApplies a brightness adjustment to the input image using a fixed intensity value to modify the image's pixel values.\n\nThis method enhances or reduces the luminance in the image, which can be beneficial for exposing models to variations in lighting conditions during pre-processing or augmentation steps.\n\nArgs:\n    img (numpy.ndarray): The input image to be adjusted.\n\nReturns:\n    numpy.ndarray: The brightness-adjusted image.\n\"\"\"",
                    "source_code": "return albumentations.brightness_contrast_adjust(img, beta=0.5, beta_by_max=True)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nIncrease the brightness of the input image by a fixed factor to generate varied image samples.\n\nArgs:\n    img (PIL.Image or Tensor): Input image to be adjusted.\n\nReturns:\n    PIL.Image or Tensor: Brightness-adjusted image.\n    \nThis method enhances image brightness to introduce diversity in training data, which helps improve the robustness of computer vision models.\n\"\"\"",
                    "source_code": "return torchvision.adjust_brightness(img, brightness_factor=1.5)"
                },
                {
                    "docstring": null,
                    "method_name": "keras",
                    "second_doc": "\"\"\"\nApplies a brightness shift to the input image using a predefined factor, then converts the result to an unsigned 8-bit integer array. This operation helps introduce controlled brightness variations, increasing the diversity of training data and aiding models in learning to handle different lighting conditions.\n\nArgs:\n    img (numpy.ndarray): Input image to be augmented.\n\nReturns:\n    numpy.ndarray: Brightness-shifted image as an unsigned 8-bit integer array.\n\"\"\"",
                    "source_code": "return keras.apply_brightness_shift(img, brightness=1.5).astype(np.uint8)"
                }
            ],
            "name": "Brightness",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Contrast class by setting up multiple image contrast adjustment transformations using different augmentation libraries. This allows for consistent benchmarking and comparison of contrast modifications across frameworks.\n\nArgs:\n    self: Instance of the Contrast class.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.Multiply((1.5, 1.5), per_channel=False)\n        self.augmentor_op = Operations.RandomContrast(probability=1, min_factor=1.5, max_factor=1.5)\n        self.solt_stream = slc.Stream([slt.ImageRandomContrast(p=1, contrast_range=(1.5, 1.5))])"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nAdjusts the contrast of the input image by applying a brightness-contrast transformation with a fixed alpha value of 1.5.\n\nThis operation modifies the pixel intensity distribution to create higher visual distinction between lighter and darker regions, which can make important features more noticeable or simulate varied lighting conditions during preprocessing.\n\nArgs:\n    img (numpy.ndarray): Input image to be processed.\n\nReturns:\n    numpy.ndarray: Image with adjusted brightness and increased contrast.\n\"\"\"",
                    "source_code": "return albumentations.brightness_contrast_adjust(img, alpha=1.5)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nAdjusts the contrast of an input image using a predefined factor to create variation in image appearance.\n\nThis method increases the contrast of the given image to generate a wider range of visual examples, which can help models generalize better to different lighting and color conditions in real-world data.\n\nArgs:\n    img: An input image in a format compatible with torchvision's adjust_contrast function.\n\nReturns:\n    The input image with its contrast adjusted by a factor of 1.5.\n\"\"\"",
                    "source_code": "return torchvision.adjust_contrast(img, contrast_factor=1.5)"
                }
            ],
            "name": "Contrast",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes multiple brightness and contrast adjustment pipelines using different augmentation libraries to ensure consistent application of these transformations. \n\nThis setup allows users to compare, combine, or test augmentation techniques from diverse sources within a unified interface, enhancing the flexibility and reliability of image preprocessing workflows.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.Sequential(\n            [iaa.Multiply((1.5, 1.5), per_channel=False), iaa.Add((127, 127), per_channel=False)]\n        )\n        self.augmentor_pipeline = Pipeline()\n        self.augmentor_pipeline.add_operation(\n            Operations.RandomBrightness(probability=1, min_factor=1.5, max_factor=1.5)\n        )\n        self.augmentor_pipeline.add_operation(Operations.RandomContrast(probability=1, min_factor=1.5, max_factor=1.5))\n        self.solt_stream = slc.Stream(\n            [\n                slt.ImageRandomBrightness(p=1, brightness_range=(127, 127)),\n                slt.ImageRandomContrast(p=1, contrast_range=(1.5, 1.5)),\n            ]\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nAdjusts the brightness and contrast of the input image using predefined scaling parameters to create a new version of the image with altered visual properties.\n\nThis transformation is applied to introduce controlled variations in image data, which can help improve the robustness and generalization ability of models trained with these images.\n\nArgs:\n    img (numpy.ndarray): Input image to be transformed.\n\nReturns:\n    numpy.ndarray: Image with adjusted brightness and contrast.\n\"\"\"",
                    "source_code": "return albumentations.brightness_contrast_adjust(img, alpha=1.5, beta=0.5, beta_by_max=True)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nAdjusts the brightness and contrast of the input image to predefined levels.\n\nThis method systematically enhances the brightness and contrast of an image to expand the range of pixel values, aiding in the creation of more diverse training data for computer vision models.\n\nArgs:\n    img (PIL.Image or Tensor): The input image to be augmented.\n\nReturns:\n    PIL.Image or Tensor: The transformed image with increased brightness and contrast.\n\"\"\"",
                    "source_code": "img = torchvision.adjust_brightness(img, brightness_factor=1.5)\n        img = torchvision.adjust_contrast(img, contrast_factor=1.5)\n        return img"
                },
                {
                    "docstring": null,
                    "method_name": "augmentor",
                    "second_doc": "\"\"\"\nApplies a sequence of augmentation operations to the input image using a predefined pipeline. This method helps generate varied versions of the input image, which can introduce useful variability and robustness for downstream tasks.\n\nArgs:\n    img (numpy.ndarray): The input image to be augmented.\n\nReturns:\n    numpy.ndarray: The augmented image as a copy in uint8 format.\n\"\"\"",
                    "source_code": "for operation in self.augmentor_pipeline.operations:\n            img, = operation.perform_operation([img])\n        return np.array(img, np.uint8, copy=True)"
                }
            ],
            "name": "BrightnessContrast",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the affine transformation used for shifting, scaling, and rotating images during augmentation. The chosen parameters enable modifications in image scale, rotation, and position, which contribute to creating diverse variations of the input data.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.Affine(\n            scale=(2, 2), rotate=(45, 45), translate_px=(50, 50), order=1, mode=\"reflect\"\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nApplies an affine transformation to the input image, including shifting, scaling, and rotation, to simulate various perspectives and positional variations. \n\nThis helps increase the diversity of training images by introducing controlled distortions, making models more robust to such changes during inference.\n\nArgs:\n    img (numpy.ndarray): The input image to be augmented.\n\nReturns:\n    numpy.ndarray: The transformed image after applying shift, scale, and rotation.\n\"\"\"",
                    "source_code": "return albumentations.shift_scale_rotate(img, angle=-45, scale=2, dx=0.2, dy=0.2)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nApplies an affine transformation to the input image, including rotation, translation, scaling, and resampling. This operation introduces controlled geometric variation to enhance model robustness during training.\n\nArgs:\n    img (PIL.Image): Input image to be transformed.\n\nReturns:\n    PIL.Image: The transformed image with applied affine operations.\n\"\"\"",
                    "source_code": "return torchvision.affine(img, angle=45, translate=(50, 50), scale=2, shear=0, resample=Image.BILINEAR)"
                },
                {
                    "docstring": null,
                    "method_name": "keras",
                    "second_doc": "\"\"\"\nApplies an affine transformation to the input image, including rotation by 45 degrees, translation by (50, 50) pixels, and scaling by 0.5 along both axes, with pixels outside the boundaries handled by reflection. This operation introduces controlled geometric variations to the input, increasing image diversity and helping models become more robust to spatial changes.\n\nArgs:\n    img (numpy.ndarray): The input image to be transformed.\n\nReturns:\n    numpy.ndarray: The transformed image after applying the affine transformation.\n\"\"\"",
                    "source_code": "return keras.apply_affine_transform(img, theta=45, tx=50, ty=50, zx=0.5, zy=0.5, fill_mode=\"reflect\")"
                }
            ],
            "name": "ShiftScaleRotate",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ShiftHSV class by setting up two image augmentation pipelines\u2014one using imgaug and the other using solt\u2014that shift the hue and saturation values of input images. \n\nThis aids in simulating color variations, increasing the diversity of the dataset during model training and helping to improve the model's robustness to color changes.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.AddToHueAndSaturation((20, 20), per_channel=False)\n        self.solt_stream = slc.Stream([slt.ImageRandomHSV(p=1, h_range=(20, 20), s_range=(20, 20), v_range=(20, 20))])"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nApplies a shift to the hue, saturation, and value channels of the input image to modify its color characteristics. This augmentation introduces color variation, which helps models become more robust to changes in lighting and scene conditions during training.\n\nArgs:\n    img (numpy.ndarray): The input image on which to apply the HSV shift.\n\nReturns:\n    numpy.ndarray: The image with adjusted hue, saturation, and value channels.\n\"\"\"",
                    "source_code": "return albumentations.shift_hsv(img, hue_shift=20, sat_shift=20, val_shift=20)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nApplies a sequence of color adjustments\u2014hue, saturation, and brightness shifts\u2014to the input image. This enhances diversity in visual appearance, promoting better model robustness to variations in color properties encountered during practical use.\n\nArgs:\n    img (PIL.Image or torch.Tensor): Input image to be transformed.\n\nReturns:\n    PIL.Image or torch.Tensor: The transformed image with adjusted hue, saturation, and brightness.\n\"\"\"",
                    "source_code": "img = torchvision.adjust_hue(img, hue_factor=0.1)\n        img = torchvision.adjust_saturation(img, saturation_factor=1.2)\n        img = torchvision.adjust_brightness(img, brightness_factor=1.2)\n        return img"
                }
            ],
            "name": "ShiftHSV",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an instance of the Solarize transformation, preparing it to modify image pixel values as part of a data augmentation pipeline.\n\nArgs:\n    (No arguments for initialization)\n\nReturns:\n    None\n\nThe initialization sets up the Solarize augmentation, allowing for future configuration or parameters to be added for this specific transformation.\n\"\"\"",
                    "source_code": "pass"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nApplies the solarize transformation to the input image, inverting all pixel values above a certain threshold to create enhanced contrast effects. This transformation is useful for increasing image diversity in augmentation pipelines, which can help models better generalize to varied data.\n\nArgs:\n    img (numpy.ndarray): Input image to be transformed.\n\nReturns:\n    numpy.ndarray: Image resulting from the solarize transformation.\n\"\"\"",
                    "source_code": "return albumentations.solarize(img)"
                },
                {
                    "docstring": null,
                    "method_name": "pillow",
                    "second_doc": "\"\"\"\nApply a solarization effect to the input image, inverting all pixel values above a threshold to introduce visual variations.\n\nArgs:\n    img (PIL.Image): The input image to be transformed.\n\nReturns:\n    PIL.Image: The solarized image, with pixel values inverted above the threshold.\n    \nThis transformation alters pixel intensities, generating diverse visual examples that can help improve the robustness of image processing models.\n\"\"\"",
                    "source_code": "return ImageOps.solarize(img)"
                }
            ],
            "name": "Solarize",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Equalize class by setting up two different histogram equalization transformations from separate libraries. This enables flexible application of histogram equalization as an image preprocessing technique during data augmentation, potentially improving the robustness of models to varying lighting conditions.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.AllChannelsHistogramEqualization()\n        self.augmentor_op = Operations.HistogramEqualisation(probability=1)"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nApplies histogram equalization to the input image to enhance its contrast and make features more distinguishable.\n\nThis operation is performed to improve the visibility of image details, which can be beneficial for subsequent image analysis or model training tasks.\n\nArgs:\n    img (numpy.ndarray): The input image array to be equalized.\n\nReturns:\n    numpy.ndarray: The resulting image after histogram equalization.\n\"\"\"",
                    "source_code": "return albumentations.equalize(img)"
                },
                {
                    "docstring": null,
                    "method_name": "pillow",
                    "second_doc": "\"\"\"\nApply histogram equalization to the input image to enhance its contrast.\n\nThis method redistributes pixel intensity values to make the histogram of the image more uniform, improving the visibility of features which can benefit downstream image processing or analysis tasks.\n\nArgs:\n    img (PIL.Image): The image to be equalized.\n\nReturns:\n    PIL.Image: The image after histogram equalization.\n\"\"\"",
                    "source_code": "return ImageOps.equalize(img)"
                }
            ],
            "name": "Equalize",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes transformation pipelines to crop images to a fixed size of 64x64 pixels using multiple augmentation libraries.\n\nThis setup ensures consistency across different augmentation frameworks by enforcing a uniform image size, which is often required before feeding images into machine learning models.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.CropToFixedSize(width=64, height=64)\n        self.augmentor_op = Operations.Crop(probability=1, width=64, height=64, centre=False)\n        self.solt_stream = slc.Stream([slt.CropTransform(crop_size=(64, 64), crop_mode=\"r\")])"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nPerforms a deterministic crop of the provided image to a size of 64x64 pixels, starting from the top-left corner. This operation is useful for extracting a fixed-size region from an image, often required for standardizing input dimensions in image processing workflows.\n\nArgs:\n    img (numpy.ndarray): The input image to be cropped.\n\nReturns:\n    numpy.ndarray: The resulting 64x64 pixel cropped image.\n\"\"\"",
                    "source_code": "return albumentations.random_crop(img, crop_height=64, crop_width=64, h_start=0, w_start=0)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nApplies a fixed-size crop of 64x64 pixels from the top-left corner of the input image to standardize its dimensions before further processing or augmentation steps.\n\nArgs:\n    img (PIL.Image or Tensor): Input image to be cropped.\n\nReturns:\n    Cropped image of size 64x64 pixels in the same format as the input.\n\"\"\"",
                    "source_code": "return torchvision.crop(img, i=0, j=0, h=64, w=64)"
                }
            ],
            "name": "RandomCrop64",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes augmentation pipelines across multiple libraries to standardize and expand input image variations by cropping to a fixed smaller size and then resizing to a larger dimension. This ensures consistent preprocessing and enhances model robustness by exposing it to varied but controlled input transforms.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.augmentor_pipeline = Pipeline()\n        self.augmentor_pipeline.add_operation(Operations.Crop(probability=1, width=64, height=64, centre=False))\n        self.augmentor_pipeline.add_operation(\n            Operations.Resize(probability=1, width=512, height=512, resample_filter=\"BILINEAR\")\n        )\n        self.imgaug_transform = iaa.Sequential(\n            [iaa.CropToFixedSize(width=64, height=64), iaa.Scale(size=512, interpolation=\"linear\")]\n        )\n        self.solt_stream = slc.Stream(\n            [slt.CropTransform(crop_size=(64, 64), crop_mode=\"r\"), slt.ResizeTransform(resize_to=(512, 512))]\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nApplies a fixed 64x64 crop to the top-left corner of the input image, then resizes the cropped region to 512x512 pixels. \n\nThis transformation standardizes the size and content of input images to ensure consistency for downstream processing or modeling tasks.\n\nArgs:\n    img (numpy.ndarray): Input image to be cropped and resized.\n\nReturns:\n    numpy.ndarray: The transformed image of shape (512, 512, C), where C is the number of channels.\n\"\"\"",
                    "source_code": "img = albumentations.random_crop(img, crop_height=64, crop_width=64, h_start=0, w_start=0)\n        return albumentations.resize(img, height=512, width=512)"
                },
                {
                    "docstring": null,
                    "method_name": "augmentor",
                    "second_doc": "\"\"\"\nApplies a sequence of image augmentation operations to the input image, sequentially transforming it according to a predefined pipeline.\n\nThis method systematically modifies the image to introduce variability, which is essential for increasing the robustness and generalization abilities of downstream computer vision models.\n\nArgs:\n    img: The input image to be augmented, typically as a numpy array.\n\nReturns:\n    numpy.ndarray: The augmented image as a NumPy array of type uint8.\n\"\"\"",
                    "source_code": "for operation in self.augmentor_pipeline.operations:\n            img, = operation.perform_operation([img])\n        return np.array(img, np.uint8, copy=True)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nApplies a fixed crop of size 64x64 to the input image and then resizes the resulting region to 512x512.\n\nThis method ensures that all input images are uniformly processed to a consistent output size, which is important for maintaining compatibility and stability in downstream image processing or model training pipelines.\n\nArgs:\n    img: The input image to be cropped and resized.\n\nReturns:\n    The cropped and resized image as a tensor or array, depending on the input type and torchvision's return type.\n\"\"\"",
                    "source_code": "img = torchvision.crop(img, i=0, j=0, h=64, w=64)\n        return torchvision.resize(img, (512, 512))"
                }
            ],
            "name": "RandomSizedCrop_64_512",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ShiftRGB class by creating an image transformation that adds a constant value to all channels of an image without altering the individual channel balance.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Increasing overall pixel intensities without changing per-channel relationships helps simulate lighting conditions and boosts the diversity of augmented images, which can aid in training more robust computer vision models.\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.Add((100, 100), per_channel=False)"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nShifts the red, green, and blue color channels of the input image by a fixed value.\n\nThis operation alters the intensity of each color channel independently, which can help models become more invariant to lighting and color variations encountered in real-world images.\n\nArgs:\n    img (numpy.ndarray): Input image to be transformed.\n\nReturns:\n    numpy.ndarray: Image with shifted RGB channels.\n\"\"\"",
                    "source_code": "return albumentations.shift_rgb(img, r_shift=100, g_shift=100, b_shift=100)"
                },
                {
                    "docstring": null,
                    "method_name": "keras",
                    "second_doc": "\"\"\"\nApply a channel shift transformation to the input image by modifying its color channels.\n\nThis method perturbs the pixel values along the specified channel axis, introducing variability that helps prevent overfitting during model training by exposing the model to diverse color distributions.\n\nArgs:\n    img (numpy.ndarray): The image to be transformed.\n\nReturns:\n    numpy.ndarray: The transformed image with its color channels shifted.\n\"\"\"",
                    "source_code": "return keras.apply_channel_shift(img, intensity=100, channel_axis=2)"
                }
            ],
            "name": "ShiftRGB",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the PadToSize512 class by creating a stream that automatically pads input images to a size of 512x512 pixels with specified padding.\n\nThis ensures consistency in image dimensions, which simplifies further image processing and model input requirements.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.solt_stream = slc.Stream([slt.PadTransform(pad_to=(512, 512), padding=\"r\")])"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nPads the input image so that its dimensions are at least 512x512 pixels.\n\nThis ensures that the processed images meet minimum size requirements, which may be necessary for consistency in further image processing or model input expectations.\n\nArgs:\n    img: The input image as a NumPy array or compatible format.\n\nReturns:\n    The padded image with a minimum height and width of 512 pixels.\n\"\"\"",
                    "source_code": "return albumentations.pad(img, min_height=512, min_width=512)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nPads the input image to ensure both width and height are at least 512 pixels, using reflection padding as needed.\nThis helps maintain consistent input size for downstream image processing operations, improving model compatibility and data pipeline robustness.\n\nArgs:\n    img (PIL.Image or Tensor): The input image to be padded.\n\nReturns:\n    PIL.Image or Tensor: The padded image with dimensions no smaller than 512x512.\n\"\"\"",
                    "source_code": "if img.size[0] < 512:\n            img = torchvision.pad(img, (int((1 + 512 - img.size[0]) / 2), 0), padding_mode=\"reflect\")\n        if img.size[1] < 512:\n            img = torchvision.pad(img, (0, int((1 + 512 - img.size[1]) / 2)), padding_mode=\"reflect\")\n        return img"
                }
            ],
            "name": "PadToSize512",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes multiple resizing transformation operators with a fixed size of 512x512 pixels using different augmentation libraries to ensure consistent image preprocessing and facilitate interoperability across various toolchains.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.imgaug_transform = iaa.Scale(size=512, interpolation=\"linear\")\n        self.solt_stream = slc.Stream([slt.ResizeTransform(resize_to=(512, 512))])\n        self.augmentor_op = Operations.Resize(probability=1, width=512, height=512, resample_filter=\"BILINEAR\")"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nResizes the input image to a fixed size of 512x512 pixels.\n\nThis method ensures consistent input dimensions for downstream processing, which is crucial for maintaining compatibility with models or pipelines that require uniform image sizes.\n\nArgs:\n    img: The input image to be resized.\n\nReturns:\n    The resized image with a height and width of 512 pixels each.\n\"\"\"",
                    "source_code": "return albumentations.resize(img, height=512, width=512)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nResizes the input image to a fixed size of 512x512 pixels using the torchvision library.\n\nThis ensures all images conform to a consistent format required by downstream image processing and machine learning workflows.\n\nArgs:\n    img (PIL.Image or torch.Tensor): The image to be resized.\n\nReturns:\n    PIL.Image or torch.Tensor: The resized image with shape (512, 512).\n\"\"\"",
                    "source_code": "return torchvision.resize(img, (512, 512))"
                }
            ],
            "name": "Resize512",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Gamma class by setting up an image processing stream that applies gamma correction with a fixed gamma value. \n\nThis ensures consistent adjustment of image brightness and contrast, which is crucial for standardizing input data in computer vision workflows.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.solt_stream = slc.Stream([slt.ImageGammaCorrection(p=1, gamma_range=(0.5, 0.5))])"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nApplies a gamma correction transformation to the input image using a fixed gamma value of 0.5 to adjust image brightness and contrast. This operation introduces photometric variation, which can help models learn to handle different lighting conditions.\n\nArgs:\n    img (numpy.ndarray): The input image to be transformed.\n\nReturns:\n    numpy.ndarray: The gamma-corrected image.\n\"\"\"",
                    "source_code": "return albumentations.gamma_transform(img, gamma=0.5)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nApplies gamma correction to an image using a fixed gamma value of 0.5.\n\nThis transformation adjusts the brightness and contrast of the input image non-linearly, which helps introduce varied lighting conditions and intensity distributions into the dataset. Such adjustments are useful for increasing the diversity of training samples, promoting better model robustness to varying illumination.\n\nArgs:\n    img (PIL.Image or Tensor): The input image to be gamma-adjusted.\n\nReturns:\n    PIL.Image or Tensor: The gamma-corrected image.\n\"\"\"",
                    "source_code": "return torchvision.adjust_gamma(img, gamma=0.5)"
                }
            ],
            "name": "Gamma",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes grayscale transformation operators from multiple augmentation libraries to ensure consistent grayscale conversion across different augmentation pipelines.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Setting up equivalent grayscale operations from various libraries allows seamless integration and consistency of grayscale preprocessing, regardless of the augmentation backend used during data pipeline construction.\n\"\"\"",
                    "source_code": "self.augmentor_op = Operations.Greyscale(probability=1)\n        self.imgaug_transform = iaa.Grayscale(alpha=1.0)\n        self.solt_stream = slc.Stream([slt.ImageColorTransform(mode=\"rgb2gs\")])"
                },
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nConverts the input image to grayscale using Albumentations' color transformation utility.\n\nThis method is used to standardize the image's color channels as a preprocessing step, which can help reduce model complexity or highlight structural features important for certain computer vision tasks.\n\nArgs:\n    img (numpy.ndarray): Input image to be converted to grayscale.\n\nReturns:\n    numpy.ndarray: Grayscale version of the input image.\n\"\"\"",
                    "source_code": "return albumentations.to_gray(img)"
                },
                {
                    "docstring": null,
                    "method_name": "torchvision_transform",
                    "second_doc": "\"\"\"\nConverts the input image to a 3-channel grayscale image by applying a grayscale transformation compatible with Torchvision.\n\nArgs:\n    img (PIL Image or Tensor): The input image to be converted to grayscale.\n\nReturns:\n    PIL Image or Tensor: The grayscale image with three identical channels.\n\nWhy:  \nStandardizing images to a consistent grayscale format facilitates seamless integration into data processing and augmentation pipelines, ensuring compatibility with models expecting three-channel input.\n\"\"\"",
                    "source_code": "return torchvision.to_grayscale(img, num_output_channels=3)"
                },
                {
                    "docstring": null,
                    "method_name": "solt",
                    "second_doc": "\"\"\"\nProcesses a grayscale image through a defined data pipeline and converts the resulting output back to an RGB format.\n\nArgs:\n    img (numpy.ndarray): Grayscale image represented as a NumPy array.\n\nReturns:\n    numpy.ndarray: The processed image converted to RGB format.\n\nWhy:\n    This method applies a sequence of augmentations or transformations to standardized grayscale image input before converting it into an RGB format, facilitating further integration into computer vision workflows that expect RGB data.\n\"\"\"",
                    "source_code": "dc = sld.DataContainer(img, \"I\")\n        dc = self.solt_stream(dc)\n        return cv2.cvtColor(dc.data[0], cv2.COLOR_GRAY2RGB)"
                },
                {
                    "docstring": null,
                    "method_name": "augmentor",
                    "second_doc": "\"\"\"\nApplies a grayscale augmentation operation to the input image and converts the result into a 3-channel image format, maintaining compatibility with models requiring such input.\n\nArgs:\n    img (numpy.ndarray): Input image to be processed.\n\nReturns:\n    numpy.ndarray: The processed 3-channel grayscale image.\n    \nWhy:\n    This method ensures processed images are in a format suitable for pipelines or models expecting 3-channel input, even after grayscale transformations are applied.\n\"\"\"",
                    "source_code": "img = self.augmentor_op.perform_operation([img])[0]\n        img = np.array(img, np.uint8, copy=True)\n        return np.dstack([img, img, img])"
                }
            ],
            "name": "Grayscale",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "albumentations",
                    "second_doc": "\"\"\"\nPosterizes the input image by reducing the number of bits for each pixel color channel to 4.\n\nArgs:\n    img (np.ndarray): The input image to be posterized.\n\nReturns:\n    np.ndarray: The posterized image with reduced color depth.\n\nWhy:\n    This method reduces the color complexity of the image, potentially making models more robust by exposing them to variations in color representation.\n\"\"\"",
                    "source_code": "return albumentations.posterize(img, 4)"
                },
                {
                    "docstring": null,
                    "method_name": "pillow",
                    "second_doc": "\"\"\"\nApplies posterization to the input image, reducing the number of bits for each color channel to 4. This transformation introduces non-linear color effects by limiting the amount of color information, which can help create varied augmented samples that challenge model robustness.\n\nArgs:\n    img (PIL.Image.Image): Input image to be posterized.\n\nReturns:\n    PIL.Image.Image: Posterized image with reduced color depth.\n\"\"\"",
                    "source_code": "return ImageOps.posterize(img, 4)"
                }
            ],
            "name": "Posterize",
            "type": "class"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "main",
                "second_doc": "\"\"\"\nRuns performance benchmarks for a suite of image augmentation operations across multiple augmentation libraries by measuring how many images per second each library can process for each operation. This provides a comparative analysis of augmentation speed, facilitating informed decisions about which library to use in a given scenario.\n\nArgs:\n    None. All configuration is handled via command-line arguments parsed by parse_args(), including the choice of libraries, number of images, number of runs, and data directory.\n\nReturns:\n    None. Prints a formatted benchmark report to stdout showing the processing speed (images per second) for each augmentation operation and library combination.\n\"\"\"",
                "source_code": "args = parse_args()\n    if args.print_package_versions:\n        print_package_versions()\n    images_per_second = defaultdict(dict)\n    libraries = [\"albumentations\", \"imgaug\", \"torchvision\", \"keras\", \"augmentor\", \"solt\", \"pillow\"]\n    data_dir = args.data_dir\n    paths = list(sorted(os.listdir(data_dir)))\n    paths = paths[: args.images]\n    imgs_cv2 = [read_img_cv2(os.path.join(data_dir, path)) for path in paths]\n    imgs_pillow = [read_img_pillow(os.path.join(data_dir, path)) for path in paths]\n\n    benchmarks = [\n        HorizontalFlip(),\n        VerticalFlip(),\n        Rotate(),\n        ShiftScaleRotate(),\n        Brightness(),\n        Contrast(),\n        BrightnessContrast(),\n        ShiftRGB(),\n        ShiftHSV(),\n        Gamma(),\n        Grayscale(),\n        RandomCrop64(),\n        PadToSize512(),\n        Resize512(),\n        RandomSizedCrop_64_512(),\n        Posterize(),\n        Solarize(),\n        Equalize(),\n    ]\n    for library in libraries:\n        imgs = imgs_pillow if library in (\"torchvision\", \"augmentor\", \"pillow\") else imgs_cv2\n        pbar = tqdm(total=len(benchmarks))\n        for benchmark in benchmarks:\n            pbar.set_description(\"Current benchmark: {} | {}\".format(library, benchmark))\n            benchmark_images_per_second = None\n            if benchmark.is_supported_by(library):\n                timer = Timer(lambda: benchmark.run(library, imgs))\n                run_times = timer.repeat(number=1, repeat=args.runs)\n                benchmark_images_per_second = [1 / (run_time / args.images) for run_time in run_times]\n            images_per_second[library][str(benchmark)] = benchmark_images_per_second\n            pbar.update(1)\n        pbar.close()\n    pd.set_option(\"display.width\", 1000)\n    df = pd.DataFrame.from_dict(images_per_second)\n    df = df.applymap(lambda r: format_results(r, args.show_std))\n    df = df[libraries]\n    augmentations = [str(i) for i in benchmarks]\n    df = df.reindex(augmentations)\n    print(df.head(len(augmentations)))"
            },
            "type": "function"
        }
    ],
    "albumentations/docs/conf.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__getattr__",
                    "second_doc": "\"\"\"\nHandles attribute access for the Mock class by returning a new MagicMock instance for any requested attribute.\n\nThis approach allows the Mock class to dynamically simulate the behavior of missing attributes or methods, facilitating the testing of code that interacts with objects whose full implementations may not be required.\n\nArgs:\n    cls: The Mock class itself, automatically provided due to the @classmethod decorator.\n    name: The name of the attribute being accessed.\n\nReturns:\n    MagicMock: A new MagicMock instance to represent the requested attribute or method.\n\"\"\"",
                    "source_code": "return MagicMock()"
                }
            ],
            "name": "Mock",
            "type": "class"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "get_version",
                "second_doc": "\"\"\"\nRetrieve the current version string of the library by parsing the __init__.py file.  \nThis enables consistent version management and helps ensure compatibility and reproducibility across different environments.\n\nArgs:\n    None\n\nReturns:\n    str: The version string of the library as specified in its __init__.py file.\n\"\"\"",
                "source_code": "current_dir = os.path.abspath(os.path.dirname(__file__))\n    root = os.path.dirname(current_dir)\n    version_file = os.path.join(root, \"albumentations\", \"__init__.py\")\n    with open(version_file) as f:\n        return re.search(r'^__version__ = [\\'\"]([^\\'\"]*)[\\'\"]', f.read(), re.M).group(1)"
            },
            "type": "function"
        }
    ],
    "albumentations/notebooks/example_bbox_keypoint_rotate.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "visualize",
                "second_doc": "\"\"\"\nDraws visual markers for provided keypoints and bounding boxes on the input image, aiding in verifying their correctness and localization.\n\nArgs:\n    image (numpy.ndarray): The input image to draw on.\n    keypoints (list of tuples or array): List of (x, y) coordinates representing keypoints.\n    bboxes (list of tuples or array): List of bounding boxes, each defined as (x_min, y_min, x_max, y_max).\n\nReturns:\n    numpy.ndarray: The image with the drawn keypoints and bounding boxes.\n\"\"\"",
                "source_code": "overlay = image.copy()\n    for kp in keypoints:\n        cv2.circle(overlay, (int(kp[0]), int(kp[1])), 20, (0, 200, 200), thickness=2, lineType=cv2.LINE_AA)\n\n    for box in bboxes:\n        cv2.rectangle(overlay, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (200, 0, 0), thickness=2)\n\n    return overlay"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "main",
                "second_doc": "\"\"\"\nDemonstrates the process of detecting image keypoints, generating bounding boxes, visualizing these features, and applying random geometric augmentations while updating related annotations. This helps illustrate how feature transformation affects both visual data and associated metadata.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "image = cv2.imread(\"images/image_1.jpg\")\n\n    keypoints = cv2.goodFeaturesToTrack(\n        cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), maxCorners=100, qualityLevel=0.5, minDistance=5\n    ).squeeze(1)\n\n    bboxes = [(kp[0] - 10, kp[1] - 10, kp[0] + 10, kp[1] + 10) for kp in keypoints]\n\n    disp_image = visualize(image, keypoints, bboxes)\n    plt.figure(figsize=(10, 10))\n    plt.imshow(cv2.cvtColor(disp_image, cv2.COLOR_RGB2BGR))\n    plt.tight_layout()\n    plt.show()\n\n    aug = A.Compose(\n        [A.ShiftScaleRotate(scale_limit=0.1, shift_limit=0.2, rotate_limit=10, always_apply=True)],\n        bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"bbox_labels\"]),\n        keypoint_params=A.KeypointParams(format=\"xy\"),\n    )\n\n    for _i in range(10):\n        data = aug(image=image, keypoints=keypoints, bboxes=bboxes, bbox_labels=np.ones(len(bboxes)))\n\n        aug_image = data[\"image\"]\n        aug_image = visualize(aug_image, data[\"keypoints\"], data[\"bboxes\"])\n\n        plt.figure(figsize=(10, 10))\n        plt.imshow(cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n        plt.tight_layout()\n        plt.show()"
            },
            "type": "function"
        }
    ],
    "albumentations/setup.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "get_version",
                "second_doc": "\"\"\"\nRetrieve the version number defined within the package's initialization file by reading its contents and extracting the version string via regular expression.  \nThis allows users and other tools to reliably identify which version of the library is installed, ensuring compatibility and aiding in troubleshooting.\n\nArgs:\n    None\n\nReturns:\n    str: The version string (e.g., \"1.2.3\") extracted from the package's __init__.py file.\n\"\"\"",
                "source_code": "current_dir = os.path.abspath(os.path.dirname(__file__))\n    version_file = os.path.join(current_dir, \"albumentations\", \"__init__.py\")\n    with io.open(version_file, encoding=\"utf-8\") as f:\n        return re.search(r'^__version__ = [\\'\"]([^\\'\"]*)[\\'\"]', f.read(), re.M).group(1)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "get_test_requirements",
                "second_doc": "\"\"\"\nDetermine the list of dependencies required to run tests, adapting for Python version compatibility.\n\nThis method ensures that the necessary testing tools are available, including compatibility support for older Python versions, so that the test suite can be executed reliably across various environments.\n\nArgs:\n    None\n\nReturns:\n    list: A list of strings representing the names of packages required for testing.\n\"\"\"",
                "source_code": "requirements = [\"pytest\"]\n    if sys.version_info < (3, 3):\n        requirements.append(\"mock\")\n    return requirements"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "get_long_description",
                "second_doc": "\"\"\"\nReads and returns the contents of the README.md file located in the project's base directory.  \nThis allows for dynamic retrieval of a detailed project description, ensuring that up-to-date information is accessible programmatically, for example when packaging or distributing the project.\n\nArgs:\n    None\n\nReturns:\n    str: The full text content of the README.md file.\n\"\"\"",
                "source_code": "base_dir = os.path.abspath(os.path.dirname(__file__))\n    with io.open(os.path.join(base_dir, \"README.md\"), encoding=\"utf-8\") as f:\n        return f.read()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"If some version version of main requirement installed, return main,\n    else return secondary.\n\n    \"\"\"",
                "first_doc": "\"\"\"\nSelects and returns a valid requirement from the provided options.\n\nChecks if the distribution specified by `main` is available. If the distribution is found, returns its string representation; otherwise, returns `secondary`.\n\nArgs:\n    main: The primary requirement specification to check for availability.\n    secondary: The fallback requirement to return if the primary is not found.\n\nReturns:\n    str: The valid requirement string, either `main` or `secondary` depending on availability.\n\"\"\"",
                "method_name": "choose_requirement",
                "second_doc": "\"\"\"\nDetermines which of the provided requirement options should be used based on availability.\n\nThis method verifies if the specified primary requirement can be resolved in the current environment. If it is available, its string representation is returned; if not, a secondary fallback option is provided. This approach ensures that the most suitable dependency is chosen automatically, helping to maintain functionality even when some packages are missing.\n\nArgs:\n    main (str): The primary requirement specification to check for availability.\n    secondary (str): The fallback requirement to use if the primary is not available.\n\nReturns:\n    str: The requirement string that is available in the environment\u2014either `main` or `secondary`.\n\"\"\"",
                "source_code": "try:\n        name = re.split(r\"[!<>=]\", main)[0]\n        get_distribution(name)\n    except DistributionNotFound:\n        return secondary\n\n    return str(main)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "get_install_requirements",
                "second_doc": "\"\"\"\nDetermine the list of package installation requirements by iterating through provided pairs of primary and secondary dependency options, selecting the appropriate version for each. This ensures compatibility and reliability of the environment when installing the library.\n\nArgs:\n    None\n\nReturns:\n    list: A list of selected dependency specification strings ready for installation.\n\"\"\"",
                "source_code": "for main, secondary in choose_install_requires:\n        install_requires.append(choose_requirement(main, secondary))\n\n    return install_requires"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/__init__.py": [],
    "albumentations/tests/compat.py": [],
    "albumentations/tests/conftest.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "pytest_ignore_collect",
                "second_doc": "\"\"\"\nDetermines whether to skip test collection for PyTorch-dependent tests when PyTorch and torchvision are not installed. This ensures that tests requiring unavailable dependencies do not cause errors during the test collection phase.\n\nArgs:\n    path: Path object representing the file to be considered for test collection.\n\nReturns:\n    bool: True if the test should be ignored (i.e., if the file matches 'test_pytorch.py' and PyTorch/torchvision are unavailable), otherwise False.\n\"\"\"",
                "source_code": "if not torch_available and path.fnmatch(\"test_pytorch.py\"):\n        warnings.warn(\n            UserWarning(\n                \"Tests that require PyTorch and torchvision were skipped because those libraries are not installed.\"\n            )\n        )\n        return True\n    return False"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "image",
                "second_doc": "\"\"\"\nGenerate a random RGB image as a NumPy array to serve as a synthetic input for image processing and transformation workflows.\n\nArgs:\n    None\n\nReturns:\n    numpy.ndarray: A 100x100 array representing a randomly generated RGB image, with pixel values ranging from 0 to 255.\n\nThis approach enables testing, demonstration, or validation of image handling functionalities without relying on external image files.\n\"\"\"",
                "source_code": "return np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "mask",
                "second_doc": "\"\"\"\nGenerate a random binary mask array, often used in the context of data augmentation or synthetic mask creation to enable testing or additional variability in vision tasks.\n\nReturns:\n    numpy.ndarray: A 100x100 array of unsigned 8-bit integers where each element is either 0 or 1, representing the generated binary mask.\n\"\"\"",
                "source_code": "return np.random.randint(low=0, high=2, size=(100, 100), dtype=np.uint8)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "bboxes",
                "second_doc": "\"\"\"\nReturns a predefined list of bounding boxes, each represented by their coordinates and label. \n\nThis method provides sample bounding box data that can be used for testing or demonstration purposes in workflows requiring annotated regions within images.\n\nArgs:\n    None\n\nReturns:\n    list: A list of bounding boxes, where each box is represented as [x_min, y_min, x_max, y_max, label].\n\"\"\"",
                "source_code": "return [[15, 12, 75, 30, 1], [55, 25, 90, 90, 2]]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "albumentations_bboxes",
                "second_doc": "\"\"\"\nProvides sample bounding box annotations in a normalized format for testing or demonstrating image augmentation workflows. This helps ensure that augmentation routines can be evaluated with realistic detection data.\n\nReturns:\n    list: A list of bounding boxes, each represented as [x_min, y_min, x_max, y_max, class_id], with coordinates normalized between 0 and 1.\n\"\"\"",
                "source_code": "return [[0.15, 0.12, 0.75, 0.30, 1], [0.55, 0.25, 0.90, 0.90, 2]]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "keypoints",
                "second_doc": "\"\"\"\nReturns a preset list of keypoints, each defined by coordinates and a label. \n\nThis method provides sample keypoints that can be used for testing, demonstration, or validation during image processing workflows, where consistent, repeatable keypoint data is required.\n\nReturns:\n    list: A list of keypoints, where each keypoint is represented as a list containing two points (x1, y1, x2, y2) and a label identifier.\n\"\"\"",
                "source_code": "return [[20, 30, 40, 50, 1], [20, 30, 60, 80, 2]]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "float_image",
                "second_doc": "\"\"\"\nGenerate a 100x100 RGB image represented as a NumPy array with pixel values sampled uniformly at random within the range [0.0, 1.0].\n\nThis method is used to produce synthetic float32 images for testing and validation purposes, ensuring that image processing pipelines operate on data that mimics the shape and type of real input images.\n\nArgs:\n    None\n\nReturns:\n    np.ndarray: A randomly generated image of shape (100, 100, 3) with float32 values in the range [0.0, 1.0].\n\"\"\"",
                "source_code": "return np.random.uniform(low=0.0, high=1.0, size=(100, 100, 3)).astype(\"float32\")"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_augmentations.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "test_image_only_augmentations",
                "second_doc": "\"\"\"\nTest that image-only augmentations correctly transform the image while preserving the mask data type and content.\n\nThis check ensures that applying augmentations to the image does not unintentionally alter or corrupt the associated mask, thereby maintaining the integrity of supervised data used in computer vision pipelines.\n\nArgs:\n    augmentation_cls: The augmentation class to instantiate and apply.\n    params: Dictionary of additional keyword arguments to pass to the augmentation class.\n    image: Input image to be augmented.\n    mask: Associated mask to be preserved during augmentation.\n\nReturns:\n    None. Asserts within the test ensure that the image and mask have uint8 data type after augmentation \n    and that the mask remains unchanged.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n    data = aug(image=image, mask=mask)\n    assert data[\"image\"].dtype == np.uint8\n    assert data[\"mask\"].dtype == np.uint8\n    assert np.array_equal(data[\"mask\"], mask)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_image_only_augmentations_with_float_values",
                "second_doc": "\"\"\"\nTest that various image-only augmentations correctly process floating-point images by preserving the image's float32 type and ensuring the associated mask remains unchanged.\n\nArgs:\n    augmentation_cls (type): The class of the image augmentation to apply.\n    params (dict): Parameters to initialize the augmentation.\n    \nReturns:\n    None\n\nWhy:\n    Ensuring that augmentations handle floating-point images without altering essential properties like data type or corresponding masks is critical for maintaining data integrity and compatibility across diverse workflows.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n    data = aug(image=float_image, mask=mask)\n    assert data[\"image\"].dtype == np.float32\n    assert data[\"mask\"].dtype == np.uint8\n    assert np.array_equal(data[\"mask\"], mask)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_dual_augmentations",
                "second_doc": "\"\"\"\nTest that various augmentation classes correctly transform both images and masks and maintain their data types.\n\nThis test ensures that dual transformations, which are expected to modify both inputs in tandem, preserve dtype consistency. Verifying this preserves pipeline reliability and compatibility with downstream processing.\n\nArgs:\n    augmentation_cls (type): The augmentation class to apply, parameterized by pytest.\n    params (dict): Additional parameters passed to the augmentation class.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the data types of the resulting image or mask are not np.uint8.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n    data = aug(image=image, mask=mask)\n    assert data[\"image\"].dtype == np.uint8\n    assert data[\"mask\"].dtype == np.uint8"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_dual_augmentations_with_float_values",
                "second_doc": "\"\"\"\nTests that a variety of augmentation classes correctly handle and preserve appropriate data types for both images and masks when given floating point image data. This ensures type consistency and correct transform application, which is essential for downstream processing and model training.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested.\n    params: Dictionary of parameters to initialize the augmentation.\n\nReturns:\n    None. Asserts within the test verify that image outputs are of type np.float32 and mask outputs are of type np.uint8.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n    data = aug(image=float_image, mask=mask)\n    assert data[\"image\"].dtype == np.float32\n    assert data[\"mask\"].dtype == np.uint8"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_imgaug_image_only_augmentations",
                "second_doc": "\"\"\"\nTest that image-only augmentations correctly transform the image while leaving the mask unchanged, and that both outputs retain the expected data type.\n\nArgs:\n    augmentation_cls: The image-only augmentation class to be tested, applied with probability 1.\n\nReturns:\n    None. Asserts that the augmented image and the mask remain uint8, and the mask is unchanged after augmentation.\n    \nWhy:\n    This ensures that image-only augmentations do not unintentionally modify the corresponding mask, which is crucial for maintaining the integrity of labeled data in tasks such as segmentation.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1)\n    data = aug(image=image, mask=mask)\n    assert data[\"image\"].dtype == np.uint8\n    assert data[\"mask\"].dtype == np.uint8\n    assert np.array_equal(data[\"mask\"], mask)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_imgaug_dual_augmentations",
                "second_doc": "\"\"\"\nTest that selected ImgAug-based augmentations correctly apply transformations to both images and masks, ensuring the output data maintains the expected datatype consistency.\n\nArgs:\n    augmentation_cls: The augmentation class to instantiate and apply, expected to be a subclass of ImgAug-based augmentations.\n\nReturns:\n    None. The function asserts correctness through tests and will raise an AssertionError if the output datatypes are incorrect.\n\nWhy:\n    Verifying datatype consistency for both image and mask ensures reliable integration of complex augmentations into data processing pipelines and prevents errors during downstream model training.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1)\n    data = aug(image=image, mask=mask)\n    assert data[\"image\"].dtype == np.uint8\n    assert data[\"mask\"].dtype == np.uint8"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_augmentations_wont_change_input",
                "second_doc": "\"\"\"\nTest that applying a given augmentation does not alter the original input image and mask arrays.\n\nThis ensures the augmentation functions operate without side effects on the inputs, which is important for maintaining data integrity throughout the augmentation pipeline.\n\nArgs:\n    augmentation_cls: The class of the augmentation to apply.\n    params: Dictionary of parameters to initialize the augmentation.\n    image: Input image array to test.\n    mask: Input mask array to test.\n\nReturns:\n    None. Asserts that the image and mask remain unchanged after augmentation.\n\"\"\"",
                "source_code": "image_copy = image.copy()\n    mask_copy = mask.copy()\n    aug = augmentation_cls(p=1, **params)\n    aug(image=image, mask=mask)\n    assert np.array_equal(image, image_copy)\n    assert np.array_equal(mask, mask_copy)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_augmentations_wont_change_float_input",
                "second_doc": "\"\"\"\nTest that applying the specified augmentation to a float-type image does not modify the input data.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested.\n    params: Dictionary of parameters to initialize the augmentation.\n\nReturns:\n    None\n\nWhy:\n    Ensures that certain augmentations, when applied with probability 1 to float-type image arrays, do not alter the original input. This validation is important to guarantee the safety and non-destructiveness of these operations on float-valued images, maintaining input integrity during augmentation pipelines.\n\"\"\"",
                "source_code": "float_image_copy = float_image.copy()\n    aug = augmentation_cls(p=1, **params)\n    aug(image=float_image)\n    assert np.array_equal(float_image, float_image_copy)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_augmentations_wont_change_shape_grayscale",
                "second_doc": "\"\"\"\nTest that applying the specified augmentation to images and masks does not alter their original shapes, regardless of the input being a grayscale (2D), grayscale with singleton channel (3D), or RGB (3D) array.\n\nThis method ensures that augmentations preserve the input dimensions, which is critical for seamless integration with downstream processing and model input requirements.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested.\n    params: Dictionary of parameters to initialize the augmentation class.\n\nReturns:\n    None. Asserts are used to verify that the image and mask shapes remain unchanged after augmentation.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n\n    # Test for grayscale image\n    image = np.zeros((224, 224), dtype=np.uint8)\n    mask = np.zeros((224, 224))\n    result = aug(image=image, mask=mask)\n    assert np.array_equal(image.shape, result[\"image\"].shape)\n    assert np.array_equal(mask.shape, result[\"mask\"].shape)\n\n    # Test for grayscale image with dummy dim\n    image_1ch = np.zeros((224, 224, 1), dtype=np.uint8)\n    mask_1ch = np.zeros((224, 224, 1))\n\n    result = aug(image=image_1ch, mask=mask_1ch)\n    assert np.array_equal(image_1ch.shape, result[\"image\"].shape)\n    assert np.array_equal(mask_1ch.shape, result[\"mask\"].shape)\n\n    # Test for RGB image\n    image_3ch = np.zeros((224, 224, 3), dtype=np.uint8)\n    mask_3ch = np.zeros((224, 224, 3))\n\n    result = aug(image=image_3ch, mask=mask_3ch)\n    assert np.array_equal(image_3ch.shape, result[\"image\"].shape)\n    assert np.array_equal(mask_3ch.shape, result[\"mask\"].shape)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_augmentations_wont_change_shape_rgb",
                "second_doc": "\"\"\"\nTests whether applying a given augmentation preserves the shape of RGB images and their masks.\n\nArgs:\n    augmentation_cls: The augmentation class to apply.\n    params: Dictionary of parameters to initialize the augmentation class.\n\nReturns:\n    None. Asserts that the image and mask output shapes remain unchanged after the augmentation is applied.\n\nWhy:\n    Ensuring that augmentations do not unintentionally alter the image or mask dimensions is crucial for maintaining data integrity and preventing downstream errors in computer vision pipelines.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n\n    # Test for RGB image\n    image_3ch = np.zeros((224, 224, 3), dtype=np.uint8)\n    mask_3ch = np.zeros((224, 224, 3))\n\n    result = aug(image=image_3ch, mask=mask_3ch)\n    assert np.array_equal(image_3ch.shape, result[\"image\"].shape)\n    assert np.array_equal(mask_3ch.shape, result[\"mask\"].shape)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_image_only_crop_around_bbox_augmentation",
                "second_doc": "\"\"\"\nTests that the cropping augmentation transforms the image data correctly near the specified bounding box, ensuring data type integrity after augmentation.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested.\n    params (dict): Parameters to initialize the augmentation.\n\nReturns:\n    None. Asserts that the augmented image has dtype np.uint8.\n    \nWhy:\n    Verifies that the augmentation pipeline preserves expected image format and type after transformation, which is crucial for reliable downstream processing in computer vision workflows.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n    annotations = {\"image\": image, \"cropping_bbox\": [-59, 77, 177, 231]}\n    data = aug(**annotations)\n    assert data[\"image\"].dtype == np.uint8"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_mask_fill_value",
                "second_doc": "\"\"\"\nTest that selected augmentations correctly apply the specified fill values to both images and masks when using a constant border mode.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested.\n    params: A dictionary of parameters for the augmentation, including 'border_mode', 'value' for the image, and 'mask_value' for the mask.\n\nReturns:\n    None. Asserts that the resulting image and mask are filled entirely with their respective provided fill values.\n\nWhy:\n    Ensuring that augmentations produce consistent and accurate results when handling borders and filling regions outside the original image is crucial to maintaining data integrity during preprocessing.\n\"\"\"",
                "source_code": "random.seed(42)\n    aug = augmentation_cls(p=1, **params)\n    input = {\"image\": np.zeros((512, 512), dtype=np.uint8) + 100, \"mask\": np.ones((512, 512))}\n    output = aug(**input)\n    assert (output[\"image\"] == 100).all()\n    assert (output[\"mask\"] == 1).all()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_multichannel_image_augmentations",
                "second_doc": "\"\"\"\nTest that various image augmentations correctly handle multi-channel image inputs and retain expected output properties.\n\nArgs:\n    augmentation_cls: The augmentation class to be applied.\n    params: A dictionary of parameters to initialize the augmentation.\n\nReturns:\n    None. Asserts that the augmented image maintains the same data type and number of channels as the input.\n\nWhy:\n    Verifies that augmentations operate correctly on input images with more than three channels, ensuring robustness and compatibility with multi-channel data commonly used in advanced vision applications.\n\"\"\"",
                "source_code": "image = np.zeros((512, 512, 6), dtype=np.uint8)\n    aug = augmentation_cls(p=1, **params)\n    data = aug(image=image)\n    assert data[\"image\"].dtype == np.uint8\n    assert data[\"image\"].shape[2] == 6"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_bbox.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "test_normalize_bbox",
                "second_doc": "\"\"\"\nTest that bounding box coordinates are correctly normalized relative to the given image dimensions.\n\nArgs:\n    bbox (tuple): The original bounding box coordinates.\n    expected (tuple): The expected normalized bounding box coordinates.\n\nReturns:\n    None. Asserts that the normalization output matches the expected result.\n\nWhy:\n    Verifies that bounding box normalization produces consistent and accurate results, which is essential for downstream tasks that depend on precise spatial annotation processing.\n\"\"\"",
                "source_code": "normalized_bbox = normalize_bbox(bbox, 200, 400)\n    assert normalized_bbox == expected"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_denormalize_bbox",
                "second_doc": "\"\"\"\nTest whether the denormalize_bbox function correctly converts bounding box coordinates from normalized form back to absolute pixel values given specific image dimensions.\n\nArgs:\n    bbox (tuple): Normalized bounding box coordinates, optionally with an additional value.\n    expected (tuple): The expected denormalized bounding box in pixel coordinates.\n\nReturns:\n    None. Asserts that the output of denormalize_bbox matches the expected result.\n\nWhy:\n    Ensures the accuracy and reliability of bounding box coordinate transformations, which is essential for precise object localization in image processing tasks.\n\"\"\"",
                "source_code": "denormalized_bbox = denormalize_bbox(bbox, 200, 400)\n    assert denormalized_bbox == expected"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_normalize_denormalize_bbox",
                "second_doc": "\"\"\"\nTest that a bounding box can be accurately normalized and then denormalized, verifying the consistency of coordinate transformations.\n\nArgs:\n    bbox (tuple): Bounding box coordinates to test, in absolute pixel values.\n\nReturns:\n    None. Asserts that applying normalization followed by denormalization returns the original bounding box values.\n\"\"\"",
                "source_code": "normalized_bbox = normalize_bbox(bbox, 200, 400)\n    denormalized_bbox = denormalize_bbox(normalized_bbox, 200, 400)\n    assert denormalized_bbox == bbox"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_denormalize_normalize_bbox",
                "second_doc": "\"\"\"\nTests that normalizing a denormalized bounding box reverses the transformation and restores the original coordinates.\n\nArgs:\n    bbox (tuple): A tuple representing normalized bounding box coordinates.\n\nReturns:\n    None\n\nWhy:  \nThis test ensures the correctness and consistency of coordinate transformations, which is crucial for accurately manipulating and interpreting bounding boxes during data processing pipelines in computer vision tasks.\n\"\"\"",
                "source_code": "denormalized_bbox = denormalize_bbox(bbox, 200, 400)\n    normalized_bbox = normalize_bbox(denormalized_bbox, 200, 400)\n    assert normalized_bbox == bbox"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_normalize_bboxes",
                "second_doc": "\"\"\"\nVerifies that the batch normalization of bounding boxes produces results consistent with normalizing each bounding box individually by comparing the outputs of both approaches.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensuring consistency between batch and individual normalization methods is essential to maintain the reliability and correctness of bounding box transformations during preprocessing or augmentation workflows.\n\"\"\"",
                "source_code": "bboxes = [(15, 25, 100, 200), (15, 25, 100, 200, 99)]\n    normalized_bboxes_1 = normalize_bboxes(bboxes, 200, 400)\n    normalized_bboxes_2 = [normalize_bbox(bboxes[0], 200, 400), normalize_bbox(bboxes[1], 200, 400)]\n    assert normalized_bboxes_1 == normalized_bboxes_2"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_denormalize_bboxes",
                "second_doc": "\"\"\"\nTests that the denormalization of bounding boxes from normalized coordinates to pixel coordinates produces consistent results across batch and single-box operations.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Verifying the consistency of bounding box denormalization helps ensure accurate and reliable conversion, which is essential for tasks that involve mapping annotations to original image dimensions.\n\"\"\"",
                "source_code": "bboxes = [(0.0375, 0.125, 0.25, 1.0), (0.0375, 0.125, 0.25, 1.0, 99)]\n    denormalized_bboxes_1 = denormalize_bboxes(bboxes, 200, 400)\n    denormalized_bboxes_2 = [denormalize_bbox(bboxes[0], 200, 400), denormalize_bbox(bboxes[1], 200, 400)]\n    assert denormalized_bboxes_1 == denormalized_bboxes_2"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_calculate_bbox_area",
                "second_doc": "\"\"\"\nTests whether the calculate_bbox_area function accurately computes the area of a given bounding box in terms of image size. This ensures that spatial transformations on bounding boxes maintain correct region calculations for downstream tasks relying on bounding box accuracy.\n\nArgs:\n    bbox (tuple): Coordinates of the bounding box, possibly in normalized format.\n    rows (int): Number of rows (height) in the image.\n    cols (int): Number of columns (width) in the image.\n    expected (int): The expected calculated area for this bounding box and image shape.\n\nReturns:\n    None: The function asserts correctness; it does not return a value.\n\"\"\"",
                "source_code": "area = calculate_bbox_area(bbox, rows, cols)\n    assert area == expected"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_bbox_to_albumentations",
                "second_doc": "\"\"\"\nTest that bounding boxes in various annotation formats are accurately converted to the expected Albumentations format.\n\nArgs:\n    bbox (tuple): The bounding box coordinates in the source format.\n    source_format (str): The format of the input bounding box, such as \"coco\", \"pascal_voc\", or \"yolo\".\n    expected (tuple): The expected bounding box in Albumentations format after conversion.\n\nReturns:\n    None\n\nWhy:\n    Ensures that bounding box format conversion works correctly, which is necessary for consistent processing and augmentation during computer vision data pipelines.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n\n    converted_bbox = convert_bbox_to_albumentations(\n        bbox, rows=image.shape[0], cols=image.shape[1], source_format=source_format\n    )\n    assert converted_bbox == expected"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_bbox_from_albumentations",
                "second_doc": "\"\"\"\nTest that bounding boxes are correctly converted from Albumentations' normalized format into various target formats.\n\nThis ensures that downstream tasks receive bounding box data in the expected coordinate system and structure, supporting consistent integration in image processing pipelines.\n\nArgs:\n    bbox (tuple): The bounding box in Albumentations' normalized format.\n    target_format (str): The format to convert the bounding box to (\"coco\", \"pascal_voc\", or \"yolo\").\n    expected (tuple): The expected converted bounding box in the target format.\n\nReturns:\n    None: Asserts that the conversion produces the expected bounding box.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    converted_bbox = convert_bbox_from_albumentations(\n        bbox, rows=image.shape[0], cols=image.shape[1], target_format=target_format\n    )\n    assert np.all(np.isclose(converted_bbox, expected))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_bbox_to_albumentations_and_back",
                "second_doc": "\"\"\"\nTests the round-trip conversion of bounding boxes to Albumentations format and back to their original format for various bbox types and formats. This ensures that the conversion utilities preserve the spatial information necessary for consistent object representation during dataset transformations.\n\nArgs:\n    bbox (tuple): The bounding box coordinates in the specified format.\n    bbox_format (str): The bounding box format (\"coco\", \"pascal_voc\", or \"yolo\").\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the bounding box after conversions does not match the original, indicating a loss or distortion of coordinate data.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    converted_bbox = convert_bbox_to_albumentations(\n        bbox, rows=image.shape[0], cols=image.shape[1], source_format=bbox_format\n    )\n    converted_back_bbox = convert_bbox_from_albumentations(\n        converted_bbox, rows=image.shape[0], cols=image.shape[1], target_format=bbox_format\n    )\n    assert np.all(np.isclose(converted_back_bbox, bbox))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_bboxes_to_albumentations",
                "second_doc": "\"\"\"\nTests whether converting a list of bounding boxes to the Albumentations format produces results consistent with converting each box individually. This ensures that batch conversion yields the same results as single-item conversion, maintaining data consistency and reliability in further image processing workflows.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "bboxes = [(20, 30, 40, 50), (30, 40, 50, 60, 99)]\n    image = np.ones((100, 100, 3))\n    converted_bboxes = convert_bboxes_to_albumentations(\n        bboxes, rows=image.shape[0], cols=image.shape[1], source_format=\"coco\"\n    )\n    converted_bbox_1 = convert_bbox_to_albumentations(\n        bboxes[0], rows=image.shape[0], cols=image.shape[1], source_format=\"coco\"\n    )\n    converted_bbox_2 = convert_bbox_to_albumentations(\n        bboxes[1], rows=image.shape[0], cols=image.shape[1], source_format=\"coco\"\n    )\n    assert converted_bboxes == [converted_bbox_1, converted_bbox_2]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_bboxes_from_albumentations",
                "second_doc": "\"\"\"\nTests the consistency of converting bounding boxes from the source annotation format to the format used by the augmentation utilities.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that batch and individual conversions of bounding boxes yield identical results, which is crucial for accurate and reliable spatial annotation transformations during image augmentation workflows.\n\"\"\"",
                "source_code": "bboxes = [(0.2, 0.3, 0.6, 0.8), (0.3, 0.4, 0.7, 0.9, 99)]\n    image = np.ones((100, 100, 3))\n    converted_bboxes = convert_bboxes_to_albumentations(\n        bboxes, rows=image.shape[0], cols=image.shape[1], source_format=\"coco\"\n    )\n    converted_bbox_1 = convert_bbox_to_albumentations(\n        bboxes[0], rows=image.shape[0], cols=image.shape[1], source_format=\"coco\"\n    )\n    converted_bbox_2 = convert_bbox_to_albumentations(\n        bboxes[1], rows=image.shape[0], cols=image.shape[1], source_format=\"coco\"\n    )\n    assert converted_bboxes == [converted_bbox_1, converted_bbox_2]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compose_with_bbox_noop",
                "second_doc": "\"\"\"\nTest that composing a pipeline with only a no-operation transform preserves the input image and bounding boxes exactly, regardless of the bounding box format or presence of labels.\n\nThis ensures that the augmentation pipeline does not introduce unintended changes when no transformations are applied, verifying the correctness of bounding box handling and pipeline configuration.\n\nArgs:\n    bboxes (list of tuple): List of bounding boxes in the specified format.\n    bbox_format (str): Format used for bounding boxes (e.g., \"coco\", \"pascal_voc\", \"yolo\").\n    labels (list or None): Optional labels corresponding to each bounding box.\n\nReturns:\n    None. Asserts that the output image and bounding boxes are unchanged.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    if labels is not None:\n        aug = Compose([NoOp(p=1.0)], bbox_params={\"format\": bbox_format, \"label_fields\": [\"labels\"]})\n        transformed = aug(image=image, bboxes=bboxes, labels=labels)\n    else:\n        aug = Compose([NoOp(p=1.0)], bbox_params={\"format\": bbox_format})\n        transformed = aug(image=image, bboxes=bboxes)\n    assert np.array_equal(transformed[\"image\"], image)\n    assert np.all(np.isclose(transformed[\"bboxes\"], bboxes))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compose_with_bbox_noop_error_label_fields",
                "second_doc": "\"\"\"\nTests that composing a NoOp transform with bounding box parameters raises an exception, verifying input validation for unsupported field configurations.\n\nArgs:\n    bboxes (list): List of bounding boxes to apply the augmentation to.\n    bbox_format (str): Format of the bounding boxes (e.g., \"coco\").\n\nReturns:\n    None: The method asserts that an exception is raised during augmentation.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    aug = Compose([NoOp(p=1.0)], bbox_params={\"format\": bbox_format})\n    with pytest.raises(Exception):\n        aug(image=image, bboxes=bboxes)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compose_with_bbox_noop_label_outside",
                "second_doc": "\"\"\"\nTest that applying a NoOp transformation with bounding boxes and various label formats preserves the original image and annotation data. This ensures that transformations with no effect do not alter input data, maintaining the integrity of inputs and annotations for further processing.\n\nArgs:\n    bboxes (list): List of bounding boxes in the specified format.\n    bbox_format (str): Format of the bounding box coordinates (e.g., 'pascal_voc').\n    labels (dict): Dictionary of label fields and their corresponding values associated with the bounding boxes.\n\nReturns:\n    None: The function uses assertions to verify correctness; no value is returned.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    aug = Compose([NoOp(p=1.0)], bbox_params={\"format\": bbox_format, \"label_fields\": list(labels.keys())})\n    transformed = aug(image=image, bboxes=bboxes, **labels)\n    assert np.array_equal(transformed[\"image\"], image)\n    assert transformed[\"bboxes\"] == bboxes\n    for k, v in labels.items():\n        assert transformed[k] == v"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_sized_crop_size",
                "second_doc": "\"\"\"\nTests whether randomly cropping an image to specified output dimensions correctly updates the image size and preserves the number of bounding boxes. This ensures that cropping transformations maintain data consistency, which is crucial for tasks involving images and their corresponding annotations.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    bboxes = [(0.2, 0.3, 0.6, 0.8), (0.3, 0.4, 0.7, 0.9, 99)]\n    aug = RandomSizedCrop(min_max_height=(70, 90), height=50, width=50, p=1.0)\n    transformed = aug(image=image, bboxes=bboxes)\n    assert transformed[\"image\"].shape == (50, 50, 3)\n    assert len(bboxes) == len(transformed[\"bboxes\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_resized_crop_size",
                "second_doc": "\"\"\"\nTests whether the RandomResizedCrop augmentation correctly modifies the image size and maintains the correspondence between input and output bounding boxes. This is important to ensure that cropping and resizing transformations applied to images also preserve the integrity and alignment of bounding box annotations, which are crucial for the accuracy of downstream tasks.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    bboxes = [(0.2, 0.3, 0.6, 0.8), (0.3, 0.4, 0.7, 0.9, 99)]\n    aug = RandomResizedCrop(height=50, width=50, p=1.0)\n    transformed = aug(image=image, bboxes=bboxes)\n    assert transformed[\"image\"].shape == (50, 50, 3)\n    assert len(bboxes) == len(transformed[\"bboxes\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_rotate",
                "second_doc": "\"\"\"\nTests that rotating an image with associated bounding boxes using a fixed angle limit preserves the correspondence and count between input and output bounding boxes.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensuring that image transformations maintain accurate bounding box associations is critical for tasks that require consistent object localization after data augmentation.\n\"\"\"",
                "source_code": "image = np.ones((192, 192, 3))\n    bboxes = [(78, 42, 142, 80)]\n    aug = Rotate(limit=15, p=1.0)\n    transformed = aug(image=image, bboxes=bboxes)\n    assert len(bboxes) == len(transformed[\"bboxes\"])"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_core.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "test_one_or_other",
                "second_doc": "\"\"\"\nTests that the OneOrOther augmentation applies either the first or the second provided transformation (but not both) to the input, ensuring mutual exclusivity in augmentation application for reliable augmentation logic.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This method verifies that only one transformation is executed per invocation, maintaining the intended behavior of conditional augmentations and thus supporting the creation of varied and controlled image augmentation pipelines.\n\"\"\"",
                "source_code": "first = MagicMock()\n    second = MagicMock()\n    augmentation = OneOrOther(first, second, p=1)\n    image = np.ones((8, 8))\n    augmentation(image=image)\n    assert first.called != second.called"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compose",
                "second_doc": "\"\"\"\nTests that the Compose class correctly applies a sequence of image augmentation transforms by verifying that each transform in the composition is called when the pipeline is executed.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "first = MagicMock()\n    second = MagicMock()\n    augmentation = Compose([first, second], p=1)\n    image = np.ones((8, 8))\n    augmentation(image=image)\n    assert first.called\n    assert second.called"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "oneof_always_apply_crash",
                "second_doc": "\"\"\"\nTests the deterministic application of a sequence of image augmentations, ensuring the pipeline returns valid results when all transforms are set to always apply.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "aug = Compose([HorizontalFlip(), Rotate(), OneOf([Blur(), MedianBlur()], p=1)], p=1)\n    image = np.ones((8, 8))\n    data = aug(image=image)\n    assert data"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_always_apply",
                "second_doc": "\"\"\"\nTest that transformations marked to always apply are executed regardless of the composed transform's probability, while others are subject to the set probability. \n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that transformations explicitly configured to run unconditionally in a composition pipeline adhere to the intended behavior, supporting reliable control over image processing workflows.\n\"\"\"",
                "source_code": "first = MagicMock(always_apply=True)\n    second = MagicMock(always_apply=False)\n    augmentation = Compose([first, second], p=0)\n    image = np.ones((8, 8))\n    augmentation(image=image)\n    assert first.called\n    assert not second.called"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_one_of",
                "second_doc": "\"\"\"\nTests that the OneOf augmentation correctly applies exactly one transformation from the list of possible transforms to the input image.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that the OneOf augmentation behaves as intended by selecting and applying only a single transformation, which is important for maintaining expected pipeline behavior and the validity of augmentation randomness.\n\"\"\"",
                "source_code": "transforms = [Mock(p=1) for _ in range(10)]\n    augmentation = OneOf(transforms, p=1)\n    image = np.ones((8, 8))\n    augmentation(image=image)\n    assert len([transform for transform in transforms if transform.called]) == 1"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_to_tuple",
                "second_doc": "\"\"\"\nTests the to_tuple function to ensure various input formats correctly yield expected (min, max) value pairs.\n\nThis method validates that different scalar and iterable inputs are converted into tuples representing a numerical range, with optional adjustments such as biasing and lower bounds. This is essential for standardizing input into range form, which is commonly needed in parameterizing data augmentations.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "assert to_tuple(10) == (-10, 10)\n    assert to_tuple(0.5) == (-0.5, 0.5)\n    assert to_tuple((-20, 20)) == (-20, 20)\n    assert to_tuple([-20, 20]) == (-20, 20)\n    assert to_tuple(100, low=30) == (30, 100)\n    assert to_tuple(10, bias=1) == (-9, 11)\n    assert to_tuple(100, bias=2) == (-98, 102)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_image_only_transform",
                "second_doc": "\"\"\"\nTest that the ImageOnlyTransform correctly applies its transformation to the image while leaving the mask unchanged, ensuring correct parameter passing and consistent augmentation behavior.\n\nArgs:\n    image (np.ndarray): The input image to which the transformation will be applied.\n    mask (np.ndarray): The corresponding mask associated with the input image.\n\nReturns:\n    None\n\nWhy:\n    This method verifies that the transformation modifies only the intended input (the image) without affecting other data (the mask), ensuring reliability and integrity in the augmentation process for tasks where masks should remain unchanged, such as semantic segmentation.\n\"\"\"",
                "source_code": "height, width = image.shape[:2]\n    with mock.patch.object(ImageOnlyTransform, \"apply\") as mocked_apply:\n        with mock.patch.object(ImageOnlyTransform, \"get_params\", return_value={\"interpolation\": cv2.INTER_LINEAR}):\n            aug = ImageOnlyTransform(p=1)\n            data = aug(image=image, mask=mask)\n            mocked_apply.assert_called_once_with(image, interpolation=cv2.INTER_LINEAR, cols=width, rows=height)\n            assert np.array_equal(data[\"mask\"], mask)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_dual_transform",
                "second_doc": "\"\"\"\nTests that the DualTransform augmentation is applied correctly and consistently to both images and corresponding masks using the appropriate interpolation methods. This helps ensure that paired data, such as images and segmentation masks, are transformed in a way that maintains their spatial alignment and integrity during augmentation.\n\nArgs:\n    image (np.ndarray): The input image to be augmented.\n    mask (np.ndarray): The corresponding mask that should be transformed in sync with the image.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "image_call = call(image, interpolation=cv2.INTER_LINEAR, cols=image.shape[1], rows=image.shape[0])\n    mask_call = call(mask, interpolation=cv2.INTER_NEAREST, cols=mask.shape[1], rows=mask.shape[0])\n    with mock.patch.object(DualTransform, \"apply\") as mocked_apply:\n        with mock.patch.object(DualTransform, \"get_params\", return_value={\"interpolation\": cv2.INTER_LINEAR}):\n            aug = DualTransform(p=1)\n            aug(image=image, mask=mask)\n            mocked_apply.assert_has_calls([image_call, mask_call], any_order=True)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_additional_targets",
                "second_doc": "\"\"\"\nTests that custom additional targets can be added and are correctly handled by the DualTransform augmentation, ensuring that the same transformation logic is applied consistently to both primary and additional images.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Verifying the correct handling of additional targets helps ensure transformations are consistent across multiple inputs, which is important when augmenting data with associated pairs (such as images and masks) to maintain alignment and training integrity in vision tasks.\n\"\"\"",
                "source_code": "image_call = call(image, interpolation=cv2.INTER_LINEAR, cols=image.shape[1], rows=image.shape[0])\n    image2_call = call(mask, interpolation=cv2.INTER_LINEAR, cols=mask.shape[1], rows=mask.shape[0])\n    with mock.patch.object(DualTransform, \"apply\") as mocked_apply:\n        with mock.patch.object(DualTransform, \"get_params\", return_value={\"interpolation\": cv2.INTER_LINEAR}):\n            aug = DualTransform(p=1)\n            aug.add_targets({\"image2\": \"image\"})\n            aug(image=image, image2=mask)\n            mocked_apply.assert_has_calls([image_call, image2_call], any_order=True)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_check_bboxes_with_correct_values",
                "second_doc": "\"\"\"\nTests that the check_bboxes function correctly processes lists of bounding box coordinates without raising exceptions.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that the bounding box validation logic can handle valid inputs robustly, helping maintain stability and reliability in data processing routines involving bounding boxes.\n\"\"\"",
                "source_code": "try:\n        check_bboxes([[0.1, 0.5, 0.8, 1.0], [0.2, 0.5, 0.5, 0.6, 99]])\n    except Exception as e:\n        pytest.fail(\"Unexpected Exception {!r}\".format(e))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_check_bboxes_with_values_less_than_zero",
                "second_doc": "\"\"\"\nTest that the check_bboxes function correctly raises a ValueError when provided with bounding box coordinates less than zero, ensuring validity of input bounding boxes for processing.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the expected ValueError message is not raised by check_bboxes.\n\"\"\"",
                "source_code": "with pytest.raises(ValueError) as exc_info:\n        check_bboxes([[0.2, 0.5, 0.5, 0.6, 99], [-0.1, 0.5, 0.8, 1.0]])\n    message = \"Expected x_min for bbox [-0.1, 0.5, 0.8, 1.0] to be in the range [0.0, 1.0], got -0.1.\"\n    assert str(exc_info.value) == message"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_check_bboxes_with_values_greater_than_one",
                "second_doc": "\"\"\"\nTest that the check_bboxes function raises a ValueError when bounding box coordinates exceed the allowed range, ensuring only valid bounding boxes are processed.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the expected ValueError with the correct message is not raised.\n\"\"\"",
                "source_code": "with pytest.raises(ValueError) as exc_info:\n        check_bboxes([[0.2, 0.5, 1.5, 0.6, 99], [0.1, 0.5, 0.8, 1.0]])\n    message = \"Expected x_max for bbox [0.2, 0.5, 1.5, 0.6, 99] to be in the range [0.0, 1.0], got 1.5.\"\n    assert str(exc_info.value) == message"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_check_bboxes_with_end_greater_that_start",
                "second_doc": "\"\"\"\nTest that the check_bboxes function correctly raises a ValueError when the coordinates of a bounding box are invalid, specifically when the maximum x-coordinate is less than or equal to the minimum x-coordinate. \n\nThis ensures data integrity by validating that bounding boxes meet expected format and value constraints before further processing.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the ValueError message does not match the expected message.\n\"\"\"",
                "source_code": "with pytest.raises(ValueError) as exc_info:\n        check_bboxes([[0.8, 0.5, 0.7, 0.6, 99], [0.1, 0.5, 0.8, 1.0]])\n    message = \"x_max is less than or equal to x_min for bbox [0.8, 0.5, 0.7, 0.6, 99].\"\n    assert str(exc_info.value) == message"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_per_channel_mono",
                "second_doc": "\"\"\"\nTests that applying per-channel image augmentation transforms to a single-channel (mono) image produces valid output. This ensures individual transformations are properly executed even when the input lacks multiple channels, supporting augmentation robustness and flexibility.\n\nArgs:\n    None\n\nReturns:\n    None. Asserts that the augmented data is returned without errors.\n\"\"\"",
                "source_code": "transforms = [Blur(), Rotate()]\n    augmentation = PerChannel(transforms, p=1)\n    image = np.ones((8, 8))\n    data = augmentation(image=image)\n    assert data"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_per_channel_multi",
                "second_doc": "\"\"\"\nTests the application of multiple image augmentation transforms independently to each channel of a multi-channel image.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that the per-channel augmentation behavior applies every transform correctly to all image channels, validating consistent and expected processing for multi-channel inputs.\n\"\"\"",
                "source_code": "transforms = [Blur(), Rotate()]\n    augmentation = PerChannel(transforms, p=1)\n    image = np.ones((8, 8, 5))\n    data = augmentation(image=image)\n    assert data"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_deterministic_oneof",
                "second_doc": "\"\"\"\nTest that using replayable augmentation with a random selection of transforms produces deterministic results when applied to the same input.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that the augmentation pipeline can consistently reproduce results using stored augmentation parameters, which is critical for reproducibility and debugging in image processing workflows.\n\"\"\"",
                "source_code": "aug = ReplayCompose([OneOf([HorizontalFlip(), Blur()])], p=1)\n    for i in range(10):\n        image = (np.random.random((8, 8)) * 255).astype(np.uint8)\n        image2 = np.copy(image)\n        data = aug(image=image)\n        assert \"replay\" in data\n        data2 = ReplayCompose.replay(data[\"replay\"], image=image2)\n        assert np.array_equal(data[\"image\"], data2[\"image\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_deterministic_one_or_other",
                "second_doc": "\"\"\"\nTests that augmentation pipelines with probabilistic application of one of two transforms produce deterministic and reproducible results when replayed.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "aug = ReplayCompose([OneOrOther(HorizontalFlip(), Blur())], p=1)\n    for i in range(10):\n        image = (np.random.random((8, 8)) * 255).astype(np.uint8)\n        image2 = np.copy(image)\n        data = aug(image=image)\n        assert \"replay\" in data\n        data2 = ReplayCompose.replay(data[\"replay\"], image=image2)\n        assert np.array_equal(data[\"image\"], data2[\"image\"])"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_find_dual_start_end.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "empty_aug1",
                "second_doc": "\"\"\"\nCreates a list of image augmentation transformations with minimal probability of application, effectively resulting in almost no augmentation. This setup is useful when you want to preserve the original input images during processing.\n\nReturns:\n    list: A list of Albumentations augmentation objects with low or zero probability, ensuring input images remain mostly unchanged.\n\"\"\"",
                "source_code": "return [\n        HorizontalFlip(p=0.001),\n        # IAAPiecewiseAffine(p=1.0),\n        OneOf(\n            [\n                # OpticalDistortion(p=0.1),\n                # GridDistortion(p=0.1),\n                # IAAPerspective(p=1.0),\n                # IAAAffine(p=1.0),\n                IAAPiecewiseAffine(p=1.0)\n            ],\n            p=0.0,\n        ),\n    ]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "empty_aug2",
                "second_doc": "\"\"\"\nCreates and returns a list of image augmentation transforms configured with specific probabilities for use in an augmentation pipeline. This setup helps evaluate model performance with minimal transformation by applying nearly no augmentations.\n\nReturns:\n    list: A list of Albumentations transform objects with their execution probabilities set.\n\"\"\"",
                "source_code": "return [\n        HorizontalFlip(p=0.001),\n        IAAPiecewiseAffine(p=1.0),\n        OneOf(\n            [\n                # OpticalDistortion(p=0.1),\n                # GridDistortion(p=0.1),\n                # IAAPerspective(p=1.0),\n                IAAAffine(p=1.0),\n                # IAAPiecewiseAffine(p=1.0),\n            ],\n            p=0.0,\n        ),\n    ]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "empty_aug3",
                "second_doc": "\"\"\"\nDefines a minimal image augmentation pipeline containing a selection of potential geometric distortions, all set with zero probability and thus effectively deactivated. This maintains a consistent augmentation framework structure while intentionally applying no transformations, which can help in scenarios such as debugging or benchmarking baseline model performance unaffected by augmentation.\n\nReturns:\n    list: A list of image augmentation composable objects, currently configured to perform no operation.\n\"\"\"",
                "source_code": "return [\n        # HorizontalFlip(p=0.001),\n        # IAAPiecewiseAffine(p=1.0),\n        OneOf(\n            [\n                OpticalDistortion(p=0.1),\n                GridDistortion(p=0.1),\n                # IAAPerspective(p=1.0),\n                # IAAAffine(p=1.0),\n                # IAAPiecewiseAffine(p=1.0),\n            ],\n            p=0.0,\n        )\n    ]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_strong_aug",
                "second_doc": "\"\"\"\nTests whether the Transforms object correctly initializes its start and end indices when constructed with specific augmentation functions.\n\nArgs:\n    aug (Callable): A callable augmentation function that returns a transformation pipeline.\n    start_end (list of int): Expected start and end indices for the Transforms object.\n\nReturns:\n    None: The test asserts correctness and does not return a value.\n\nWhy:\n    Ensuring proper initialization of transformation boundaries helps verify that augmentation pipelines work as intended and produce consistent results across different augmentation configurations.\n\"\"\"",
                "source_code": "assert Transforms(aug()).start_end == start_end"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_functional.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "test_vflip",
                "second_doc": "\"\"\"\nTest that the vertical flip operation correctly inverts images or masks along the vertical axis by comparing the output to the expected result.\n\nArgs:\n    target (str): Specifies whether to treat the input as an 'image' or 'mask' during the transformation.\n\nReturns:\n    None. Asserts internally that the vertically flipped result matches the predefined expected output, ensuring the transformation's correctness.\n\nWhy:\n    Verifying the accuracy of vertical flip operations ensures that fundamental geometric augmentations are performed reliably, which is critical for data consistency during preprocessing and model training.\n\"\"\"",
                "source_code": "img = np.array([[1, 1, 1], [0, 1, 1], [0, 0, 1]], dtype=np.uint8)\n    expected = np.array([[0, 0, 1], [0, 1, 1], [1, 1, 1]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    flipped_img = F.vflip(img)\n    assert np.array_equal(flipped_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_vflip_float",
                "second_doc": "\"\"\"\nTest that the vertical flip operation correctly reverses the order of rows in a floating-point image, ensuring the function's accuracy across different image formats.\n\nArgs:\n    target (str): Specifies the image format to be tested, such as \"image\" or \"image_4_channels\".\n\nReturns:\n    None. Asserts that the flipped image matches the expected result within numerical precision.\n\"\"\"",
                "source_code": "img = np.array([[0.4, 0.4, 0.4], [0.0, 0.4, 0.4], [0.0, 0.0, 0.4]], dtype=np.float32)\n    expected = np.array([[0.0, 0.0, 0.4], [0.0, 0.4, 0.4], [0.4, 0.4, 0.4]], dtype=np.float32)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    flipped_img = F.vflip(img)\n    assert_array_almost_equal_nulp(flipped_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_hflip",
                "second_doc": "\"\"\"\nTest that horizontal flipping correctly inverts the image or mask as expected.\n\nArgs:\n    target (str): Specifies the type of input data (\"image\" or \"mask\") to verify that horizontal flip is properly handled for different modalities.\n\nReturns:\n    None\n\nWhy:\n    Ensuring the accuracy of basic geometric augmentations like horizontal flipping is critical, as even small inconsistencies during augmentation can adversely affect model training or evaluation results.\n\"\"\"",
                "source_code": "img = np.array([[1, 1, 1], [0, 1, 1], [0, 0, 1]], dtype=np.uint8)\n    expected = np.array([[1, 1, 1], [1, 1, 0], [1, 0, 0]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    flipped_img = F.hflip(img)\n    assert np.array_equal(flipped_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_hflip_float",
                "second_doc": "\"\"\"\nTest that horizontal flipping of a floating point image produces the correct result for different image formats.\n\nArgs:\n    target (str): The target image format to test, either \"image\" or \"image_4_channels\".\n\nReturns:\n    None. Asserts that the horizontally flipped output matches the expected result within floating point tolerance.\n\nWhy:\n    Ensuring the horizontal flip operation handles floating point data correctly across various image formats is vital for building reliable augmentation pipelines in image processing workflows.\n\"\"\"",
                "source_code": "img = np.array([[0.4, 0.4, 0.4], [0.0, 0.4, 0.4], [0.0, 0.0, 0.4]], dtype=np.float32)\n    expected = np.array([[0.4, 0.4, 0.4], [0.4, 0.4, 0.0], [0.4, 0.0, 0.0]], dtype=np.float32)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    flipped_img = F.hflip(img)\n    assert_array_almost_equal_nulp(flipped_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_flip",
                "second_doc": "\"\"\"\nTests whether the random_flip transformation correctly applies different types of flips (vertical, horizontal, both) to an image or mask. This ensures the reliability and correctness of fundamental augmentation operations, which are vital for maintaining data integrity during augmentation.\n\nArgs:\n    target (str): Specifies whether the flip is tested on 'image' or 'mask'.\n    code (int): Indicates the flip type (0 = vertical, 1 = horizontal, -1 = both axes).\n    func (callable): Reference transformation function to compare the output against.\n\nReturns:\n    None. Asserts that the output of the random_flip transformation matches the manually applied flip.\n\"\"\"",
                "source_code": "img = np.array([[1, 1, 1], [0, 1, 1], [0, 0, 1]], dtype=np.uint8)\n    img = convert_2d_to_target_format([img], target=target)\n    assert np.array_equal(F.random_flip(img, code), func(img))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_flip_float",
                "second_doc": "\"\"\"\nTest that random_flip produces the expected transformation for float32 images with various flip codes and input formats.\n\nArgs:\n    target (str): The image format to convert to, e.g., \"image\" or \"image_4_channels\".\n    code (int): Flip code indicating the direction of flip (0 for vertical, 1 for horizontal, -1 for both axes).\n    func (callable): The function that constructs the expected manually-flipped image result.\n\nReturns:\n    None\n\nWhy:\n    This method verifies that the flipping logic consistently applies the correct geometric transformation to floating-point images, ensuring reliability across different data representations and flip directions.\n\"\"\"",
                "source_code": "img = np.array([[0.4, 0.4, 0.4], [0.0, 0.4, 0.4], [0.0, 0.0, 0.4]], dtype=np.float32)\n    img = convert_2d_to_target_format([img], target=target)\n    assert_array_almost_equal_nulp(F.random_flip(img, code), func(img))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_transpose",
                "second_doc": "\"\"\"\nTest that the transpose function correctly swaps the axes of input images, ensuring image shape transformations behave as expected for a variety of dimensionalities.\n\nArgs:\n    input_shape (tuple): The shape of the synthetic input image to be created and tested.\n    expected_shape (tuple): The expected shape after applying the transpose operation.\n\nReturns:\n    None. Asserts that the transposed image matches the expected shape.\n\nWhy:\n    Verifying that the transpose operation works properly is essential for consistent handling of data transformations in image processing workflows.\n\"\"\"",
                "source_code": "img = np.random.randint(low=0, high=256, size=input_shape, dtype=np.uint8)\n    transposed = F.transpose(img)\n    assert transposed.shape == expected_shape"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_transpose_float",
                "second_doc": "\"\"\"\nTest that the transpose function correctly reorders the axes of floating-point image arrays to match the expected output shapes.\n\nArgs:\n    input_shape (tuple): Shape of the input image array to be transposed.\n    expected_shape (tuple): Expected shape of the image array after transposition.\n\nReturns:\n    None\n\nWhy:\n    Ensures that the transpose operation properly modifies array dimensions for image data, which is crucial for compatibility with various image processing and augmentation tasks.\n\"\"\"",
                "source_code": "img = np.random.uniform(low=0.0, high=1.0, size=input_shape).astype(\"float32\")\n    transposed = F.transpose(img)\n    assert transposed.shape == expected_shape"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_rot90",
                "second_doc": "\"\"\"\nTest that rotating a 2D input array by 90 degrees produces the expected output for both image and mask targets.\n\nArgs:\n    target (str): Type of the data to test, either \"image\" or \"mask\".\n\nReturns:\n    None\n\nWhy:\n    Ensures that the 90-degree rotation transformation preserves consistency and correctness, which is critical for maintaining data integrity during augmentation processes in computer vision pipelines.\n\"\"\"",
                "source_code": "img = np.array([[0, 0, 1], [0, 0, 1], [0, 0, 1]], dtype=np.uint8)\n    expected = np.array([[1, 1, 1], [0, 0, 0], [0, 0, 0]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    rotated = F.rot90(img, factor=1)\n    assert np.array_equal(rotated, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_rot90_float",
                "second_doc": "\"\"\"\nTest that applying a 90-degree rotation to a 2D floating-point image produces the correct output for different image configurations.\n\nArgs:\n    target (str): Specifies the target image format to test, such as single-channel or multi-channel image representations.\n\nReturns:\n    None. Asserts correctness by comparing the rotated image to the expected result.\n    \nThis method ensures that rotation transformations are implemented accurately for floating-point images, which is essential for reliable preprocessing in downstream vision pipelines.\n\"\"\"",
                "source_code": "img = np.array([[0.0, 0.0, 0.4], [0.0, 0.0, 0.4], [0.0, 0.0, 0.4]], dtype=np.float32)\n    expected = np.array([[0.4, 0.4, 0.4], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], dtype=np.float32)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    rotated = F.rot90(img, factor=1)\n    assert_array_almost_equal_nulp(rotated, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_normalize",
                "second_doc": "\"\"\"\nTests the image normalization function to ensure it correctly adjusts pixel values using specified mean and standard deviation. This verification is important to guarantee that normalization operations produce predictable and consistent results, which can affect the output of computer vision pipelines.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.uint8) * 127\n    normalized = F.normalize(img, mean=50, std=3)\n    expected = (np.ones((100, 100, 3), dtype=np.float32) * 127 / 255 - 50) / 3\n    assert_array_almost_equal_nulp(normalized, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_normalize_float",
                "second_doc": "\"\"\"\nTests the normalization of a floating-point image array by applying the specified mean, standard deviation, and maximum pixel value, then verifies the output matches expected normalized values.\n\nThis ensures that image data is properly scaled and centered for further processing or model training, maintaining consistency and reliability in preprocessing pipelines.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.float32) * 0.4\n    normalized = F.normalize(img, mean=50, std=3, max_pixel_value=1.0)\n    expected = (np.ones((100, 100, 3), dtype=np.float32) * 0.4 - 50) / 3\n    assert_array_almost_equal_nulp(normalized, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compare_rotate_and_shift_scale_rotate",
                "second_doc": "\"\"\"\nTests whether the 'rotate' and 'shift_scale_rotate' functions with identical parameters produce the same output, ensuring consistent rotation behavior across both methods.\n\nArgs:\n    image (numpy.ndarray): The input image to be rotated and compared.\n\nReturns:\n    None\n\nWhy:\n    This method checks the equivalence of two rotation implementations to verify functional consistency between similar augmentation operations, helping to ensure transformation reliability within the library.\n\"\"\"",
                "source_code": "rotated_img_1 = F.rotate(image, angle=60)\n    rotated_img_2 = F.shift_scale_rotate(image, angle=60, scale=1, dx=0, dy=0)\n    assert np.array_equal(rotated_img_1, rotated_img_2)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compare_rotate_float_and_shift_scale_rotate_float",
                "second_doc": "\"\"\"\nTests whether the specialized rotation operation produces identical results to the more general shift-scale-rotate transformation when only rotation parameters are used.\n\nArgs:\n    float_image (np.ndarray): Input image of floating-point dtype to be rotated.\n\nReturns:\n    None. Asserts that the outputs from the two rotation approaches are identical.\n    \nWhy:\n    This ensures that both rotation implementations are consistent, which is important for maintaining predictable and reliable image transformations across different parts of the augmentation pipeline.\n\"\"\"",
                "source_code": "rotated_img_1 = F.rotate(float_image, angle=60)\n    rotated_img_2 = F.shift_scale_rotate(float_image, angle=60, scale=1, dx=0, dy=0)\n    assert np.array_equal(rotated_img_1, rotated_img_2)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_center_crop",
                "second_doc": "\"\"\"\nTests whether the center_crop function correctly extracts the central region from an input array for both images and masks. Ensures that cropping operations preserve expected content and format across different target types.\n\nArgs:\n    target (str): Specifies the type of input to test, either 'image' or 'mask'.\n\nReturns:\n    None. Raises an assertion error if the center-cropped result does not match the expected output.\n\"\"\"",
                "source_code": "img = np.array([[1, 1, 1, 1], [0, 1, 1, 1], [0, 0, 1, 1], [0, 0, 0, 1]], dtype=np.uint8)\n    expected = np.array([[1, 1], [0, 1]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    cropped_img = F.center_crop(img, 2, 2)\n    assert np.array_equal(cropped_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_center_crop_float",
                "second_doc": "\"\"\"\nTest that center cropping an image with float values produces the expected result for different image formats.\n\nArgs:\n    target (str): Specifies the format of the input image, such as \"image\" or \"image_4_channels\".\n\nReturns:\n    None: The test asserts that the cropped image matches the expected output and does not return a value.\n\nWhy:\n    Verifying center cropping with float data types ensures the robustness and consistency of image preprocessing functions, which is crucial for maintaining reliable input data during image transformation workflows.\n\"\"\"",
                "source_code": "img = np.array(\n        [[0.4, 0.4, 0.4, 0.4], [0.0, 0.4, 0.4, 0.4], [0.0, 0.0, 0.4, 0.4], [0.0, 0.0, 0.0, 0.4]], dtype=np.float32\n    )\n    expected = np.array([[0.4, 0.4], [0.0, 0.4]], dtype=np.float32)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    cropped_img = F.center_crop(img, 2, 2)\n    assert_array_almost_equal_nulp(cropped_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_center_crop_with_incorrectly_large_crop_size",
                "second_doc": "\"\"\"\nTest that an error is raised when attempting to crop an image with a size larger than the original image.\n\nThis method verifies the library's handling of invalid cropping requests by asserting that the appropriate exception and message are produced when a crop size exceeds image dimensions.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "img = np.ones((4, 4), dtype=np.uint8)\n    with pytest.raises(ValueError) as exc_info:\n        F.center_crop(img, 8, 8)\n    assert str(exc_info.value) == \"Requested crop size (8, 8) is larger than the image size (4, 4)\""
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_crop",
                "second_doc": "\"\"\"\nTests that the random cropping function correctly extracts the expected region from both images and masks, ensuring deterministic behavior when given specific crop parameters.\n\nArgs:\n    target (str): Specifies whether the test is applied to an \"image\" or a \"mask\".\n\nReturns:\n    None\n\nWhy:\n    Verifying the correctness and determinism of the cropping operation is essential for guaranteeing consistent data transformations during augmentation workflows, which in turn ensures reliable training data preparation for computer vision tasks.\n\"\"\"",
                "source_code": "img = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]], dtype=np.uint8)\n    expected = np.array([[5, 6], [9, 10]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    cropped_img = F.random_crop(img, crop_height=2, crop_width=2, h_start=0.5, w_start=0)\n    assert np.array_equal(cropped_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_crop_float",
                "second_doc": "\"\"\"\nTest that the random_crop function correctly crops floating-point images by comparing the cropped output to an expected result. This ensures reliability and precision when cropping images with float data types, which is important for consistent image augmentation outcomes.\n\nArgs:\n    target (str): Specifies the image format to use in the test, either \"image\" or \"image_4_channels\".\n\nReturns:\n    None. Asserts that the cropped image matches the expected result within numerical precision tolerances.\n\"\"\"",
                "source_code": "img = np.array(\n        [[0.01, 0.02, 0.03, 0.04], [0.05, 0.06, 0.07, 0.08], [0.09, 0.10, 0.11, 0.12], [0.13, 0.14, 0.15, 0.16]],\n        dtype=np.float32,\n    )\n    expected = np.array([[0.05, 0.06], [0.09, 0.10]], dtype=np.float32)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    cropped_img = F.random_crop(img, crop_height=2, crop_width=2, h_start=0.5, w_start=0)\n    assert_array_almost_equal_nulp(cropped_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_crop_with_incorrectly_large_crop_size",
                "second_doc": "\"\"\"\nTest that attempting to crop an image with dimensions smaller than the requested crop size correctly raises a ValueError.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    ValueError: If the requested crop dimensions exceed the image size.\n\nWhy:\n    The method ensures robust error handling by verifying that invalid crop requests produce clear, informative exceptions, preventing unexpected behaviors during augmentation.\n\"\"\"",
                "source_code": "img = np.ones((4, 4), dtype=np.uint8)\n    with pytest.raises(ValueError) as exc_info:\n        F.random_crop(img, crop_height=8, crop_width=8, h_start=0, w_start=0)\n    assert str(exc_info.value) == \"Requested crop size (8, 8) is larger than the image size (4, 4)\""
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_clip",
                "second_doc": "\"\"\"\nTests whether the clip function correctly limits the values in an image array to the specified range and data type, ensuring image data remains within valid bounds.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensuring pixel values do not exceed specified limits or data types prevents potential overflow or underflow issues during subsequent image processing, maintaining data integrity and compatibility with downstream tasks.\n\"\"\"",
                "source_code": "img = np.array([[-300, 0], [100, 400]], dtype=np.float32)\n    expected = np.array([[0, 0], [100, 255]], dtype=np.float32)\n    clipped = F.clip(img, dtype=np.uint8, maxval=255)\n    assert np.array_equal(clipped, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_clip_float",
                "second_doc": "\"\"\"\nTest that the clip function correctly limits all elements of a float32 image array to a specified maximum value and does not allow values below zero.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that image data remains within valid bounds after augmentation or preprocessing, which is important for model stability and performance.\n\"\"\"",
                "source_code": "img = np.array([[-0.02, 0], [0.5, 2.2]], dtype=np.float32)\n    expected = np.array([[0, 0], [0.5, 1.0]], dtype=np.float32)\n    clipped = F.clip(img, dtype=np.float32, maxval=1.0)\n    assert_array_almost_equal_nulp(clipped, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_pad",
                "second_doc": "\"\"\"\nTest that the pad function correctly extends a 2D array to the specified minimum height and width with the expected reflection logic.\n\nArgs:\n    target (str): Specifies whether the test is run on an image or a mask representation.\n\nReturns:\n    None\n\nWhy:\n    Ensures consistent and correct padding behavior, which is critical for maintaining valid input dimensions throughout transformation pipelines, thereby enabling reliable data augmentation and processing workflows.\n\"\"\"",
                "source_code": "img = np.array([[1, 2], [3, 4]], dtype=np.uint8)\n    expected = np.array([[4, 3, 4, 3], [2, 1, 2, 1], [4, 3, 4, 3], [2, 1, 2, 1]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    padded = F.pad(img, min_height=4, min_width=4)\n    assert np.array_equal(padded, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_pad_float",
                "second_doc": "\"\"\"\nTest that padding a floating-point image array to a specified size preserves numerical accuracy and produces the expected result.\n\nArgs:\n    target (str): The image format (\"image\" or \"image_4_channels\") to which the sample and expected arrays are converted during the test.\n\nReturns:\n    None. The test asserts that the padded array values closely match the precomputed expected output.\n\nThis method ensures the pad operation handles floating-point arrays correctly, which is critical for maintaining data integrity across preprocessing steps in image pipelines.\n\"\"\"",
                "source_code": "img = np.array([[0.1, 0.2], [0.3, 0.4]], dtype=np.float32)\n    expected = np.array(\n        [[0.4, 0.3, 0.4, 0.3], [0.2, 0.1, 0.2, 0.1], [0.4, 0.3, 0.4, 0.3], [0.2, 0.1, 0.2, 0.1]], dtype=np.float32\n    )\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    padded_img = F.pad(img, min_height=4, min_width=4)\n    assert_array_almost_equal_nulp(padded_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_rotate_from_shift_scale_rotate",
                "second_doc": "\"\"\"\nTests whether the rotation operation with specific parameters produces the expected output for both images and masks.\n\nThis method verifies the correctness of geometric transformations by comparing the result of a 90-degree rotation with a predefined expected output, ensuring that transformations maintain consistency and integrity across different data types.\n\nArgs:\n    target (str): Specifies the data type being tested; either \"image\" or \"mask\".\n\nReturns:\n    None: The test passes if the transformed output matches the expected result, otherwise an assertion error is raised.\n\"\"\"",
                "source_code": "img = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]], dtype=np.uint8)\n    expected = np.array([[0, 0, 0, 0], [4, 8, 12, 16], [3, 7, 11, 15], [2, 6, 10, 14]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    rotated_img = F.shift_scale_rotate(\n        img, angle=90, scale=1, dx=0, dy=0, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_CONSTANT\n    )\n    assert np.array_equal(rotated_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_rotate_float_from_shift_scale_rotate",
                "second_doc": "\"\"\"\nTests whether the shift_scale_rotate transformation accurately rotates a floating-point image by 90 degrees, ensuring the output matches the expected result.\n\nArgs:\n    target (str): Specifies the input image format, either a standard single-channel or a 4-channel image.\n\nReturns:\n    None. Asserts internally that the rotated image matches the expected values.\n\nWhy:\n    Verifying the correctness of geometric transformations on various image formats ensures that image augmentation workflows produce reliable and reproducible results, which is crucial for preparing datasets for computer vision tasks.\n\"\"\"",
                "source_code": "img = np.array(\n        [[0.01, 0.02, 0.03, 0.04], [0.05, 0.06, 0.07, 0.08], [0.09, 0.10, 0.11, 0.12], [0.13, 0.14, 0.15, 0.16]],\n        dtype=np.float32,\n    )\n    expected = np.array(\n        [[0.00, 0.00, 0.00, 0.00], [0.04, 0.08, 0.12, 0.16], [0.03, 0.07, 0.11, 0.15], [0.02, 0.06, 0.10, 0.14]],\n        dtype=np.float32,\n    )\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    rotated_img = F.shift_scale_rotate(\n        img, angle=90, scale=1, dx=0, dy=0, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_CONSTANT\n    )\n    assert_array_almost_equal_nulp(rotated_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_scale_from_shift_scale_rotate",
                "second_doc": "\"\"\"\nTest that the shift_scale_rotate function correctly scales an input without rotation or shift, ensuring the transformation produces expected pixel values for both images and masks.\n\nArgs:\n    target (str): The data type to test, either \"image\" or \"mask\".\n\nReturns:\n    None. Asserts that the scaled output matches the expected result to guarantee the accuracy of geometric transformation operations.\n\"\"\"",
                "source_code": "img = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]], dtype=np.uint8)\n    expected = np.array([[6, 7, 7, 8], [10, 11, 11, 12], [10, 11, 11, 12], [14, 15, 15, 16]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    scaled_img = F.shift_scale_rotate(\n        img, angle=0, scale=2, dx=0, dy=0, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_CONSTANT\n    )\n    assert np.array_equal(scaled_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_scale_float_from_shift_scale_rotate",
                "second_doc": "\"\"\"\nTest that the shift_scale_rotate transformation produces correct results on floating-point images for different input formats.\n\nThis test ensures the geometric transformation logic behaves as expected on float32 arrays, supporting consistency and reliability in image processing pipelines.\n\nArgs:\n    target (str): The input image format variant, such as a standard image or a multi-channel image.\n\nReturns:\n    None. Asserts that the transformed output matches the expected result with high numerical precision.\n\"\"\"",
                "source_code": "img = np.array(\n        [[0.01, 0.02, 0.03, 0.04], [0.05, 0.06, 0.07, 0.08], [0.09, 0.10, 0.11, 0.12], [0.13, 0.14, 0.15, 0.16]],\n        dtype=np.float32,\n    )\n    expected = np.array(\n        [[0.06, 0.07, 0.07, 0.08], [0.10, 0.11, 0.11, 0.12], [0.10, 0.11, 0.11, 0.12], [0.14, 0.15, 0.15, 0.16]],\n        dtype=np.float32,\n    )\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    scaled_img = F.shift_scale_rotate(\n        img, angle=0, scale=2, dx=0, dy=0, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_CONSTANT\n    )\n    assert_array_almost_equal_nulp(scaled_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_shift_x_from_shift_scale_rotate",
                "second_doc": "\"\"\"\nTest shifting a 2D array (image or mask) horizontally by applying a positive x-offset, then verify that the transformation produces the expected result.\n\nArgs:\n    target (str): Specifies whether the transformation is applied to an \"image\" or a \"mask\".\n\nReturns:\n    None. Asserts internally to validate correctness of the transformation.\n    \nWhy:\n    Ensures that the horizontal shift component of the affine transformation behaves as intended, which is crucial for maintaining data integrity and consistency in preprocessing steps.\n\"\"\"",
                "source_code": "img = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]], dtype=np.uint8)\n    expected = np.array([[0, 0, 1, 2], [0, 0, 5, 6], [0, 0, 9, 10], [0, 0, 13, 14]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    shifted_along_x_img = F.shift_scale_rotate(\n        img, angle=0, scale=1, dx=0.5, dy=0, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_CONSTANT\n    )\n    assert np.array_equal(shifted_along_x_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_shift_x_float_from_shift_scale_rotate",
                "second_doc": "\"\"\"\nTests that shifting a floating-point image along the x-axis with the shift_scale_rotate transformation produces the expected results, ensuring geometric transformations\u2014such as translations\u2014are handled accurately for various image formats.\n\nArgs:\n    target (str): Specifies the target format of the input image, e.g., \"image\" or \"image_4_channels\".\n\nReturns:\n    None\n\nWhy:\n    Verifies the correctness and reliability of geometric shift operations on images, which is essential for maintaining data integrity during augmentation processes used in training computer vision models.\n\"\"\"",
                "source_code": "img = np.array(\n        [[0.01, 0.02, 0.03, 0.04], [0.05, 0.06, 0.07, 0.08], [0.09, 0.10, 0.11, 0.12], [0.13, 0.14, 0.15, 0.16]],\n        dtype=np.float32,\n    )\n    expected = np.array(\n        [[0.00, 0.00, 0.01, 0.02], [0.00, 0.00, 0.05, 0.06], [0.00, 0.00, 0.09, 0.10], [0.00, 0.00, 0.13, 0.14]],\n        dtype=np.float32,\n    )\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    shifted_along_x_img = F.shift_scale_rotate(\n        img, angle=0, scale=1, dx=0.5, dy=0, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_CONSTANT\n    )\n    assert_array_almost_equal_nulp(shifted_along_x_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_shift_y_from_shift_scale_rotate",
                "second_doc": "\"\"\"\nTest that shifting an array along the vertical axis using an affine transformation produces the expected result for both images and masks.\n\nArgs:\n    target (str): Specifies whether the data format corresponds to an \"image\" or a \"mask\".\n\nReturns:\n    None. Asserts that the shifted output matches the expected array.\n\nWhy:\n    This method verifies the correctness of the vertical shift component in affine transformations, ensuring that data processed by the augmentation function behaves as intended across different data representations.\n\"\"\"",
                "source_code": "img = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]], dtype=np.uint8)\n    expected = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    shifted_along_y_img = F.shift_scale_rotate(\n        img, angle=0, scale=1, dx=0, dy=0.5, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_CONSTANT\n    )\n    assert np.array_equal(shifted_along_y_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_shift_y_float_from_shift_scale_rotate",
                "second_doc": "\"\"\"\nTest that applying a vertical fractional shift to a float32 image returns the correct output, verifying the precision and reliability of the transformation logic.\n\nArgs:\n    target (str): The target data type or format to which the input image and expected output should be converted.\n\nReturns:\n    None. Asserts that the shifted image matches the expected result within floating-point tolerance.\n\"\"\"",
                "source_code": "img = np.array(\n        [[0.01, 0.02, 0.03, 0.04], [0.05, 0.06, 0.07, 0.08], [0.09, 0.10, 0.11, 0.12], [0.13, 0.14, 0.15, 0.16]],\n        dtype=np.float32,\n    )\n    expected = np.array(\n        [[0.00, 0.00, 0.00, 0.00], [0.00, 0.00, 0.00, 0.00], [0.01, 0.02, 0.03, 0.04], [0.05, 0.06, 0.07, 0.08]],\n        dtype=np.float32,\n    )\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    shifted_along_y_img = F.shift_scale_rotate(\n        img, angle=0, scale=1, dx=0, dy=0.5, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_CONSTANT\n    )\n    assert_array_almost_equal_nulp(shifted_along_y_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_shift_rgb",
                "second_doc": "\"\"\"\nTest that shifting RGB channel values of an image by specific amounts correctly updates the pixel values and maintains the expected data type. This ensures that color adjustment operations behave predictably, supporting the reliability of image transformation utilities.\n\nArgs:\n    shift_params (tuple of int): Amounts to shift the R, G, B channels, respectively.\n    expected (tuple of int): Expected RGB values after shifting.\n\nReturns:\n    None: Asserts internally to verify correctness.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.uint8) * 127\n    r_shift, g_shift, b_shift = shift_params\n    img = F.shift_rgb(img, r_shift=r_shift, g_shift=g_shift, b_shift=b_shift)\n    expected_r, expected_g, expected_b = expected\n    assert img.dtype == np.dtype(\"uint8\")\n    assert (img[:, :, 0] == expected_r).all()\n    assert (img[:, :, 1] == expected_g).all()\n    assert (img[:, :, 2] == expected_b).all()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_shift_rgb_float",
                "second_doc": "\"\"\"\nTest that shifting the RGB channels of a floating-point image by specified values correctly updates each channel while maintaining the image's data type and value ranges.\n\nArgs:\n    shift_params (tuple): Amounts to shift the red, green, and blue channels, respectively.\n    expected (tuple): Expected resulting mean values for the R, G, and B channels after shifting.\n\nReturns:\n    None. Asserts within the test validate that the image channels are shifted as expected without altering the data type.\n    \nWhy:\n    Ensures that channel-wise color transformations behave correctly, enabling reliable data manipulation for downstream processing and model training.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.float32) * 0.4\n    r_shift, g_shift, b_shift = shift_params\n    img = F.shift_rgb(img, r_shift=r_shift, g_shift=g_shift, b_shift=b_shift)\n    expected_r, expected_g, expected_b = [\n        np.ones((100, 100), dtype=np.float32) * channel_value for channel_value in expected\n    ]\n    assert img.dtype == np.dtype(\"float32\")\n    assert_array_almost_equal_nulp(img[:, :, 0], expected_r)\n    assert_array_almost_equal_nulp(img[:, :, 1], expected_g)\n    assert_array_almost_equal_nulp(img[:, :, 2], expected_b)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_contrast",
                "second_doc": "\"\"\"\nTests that applying a random contrast adjustment to an image using a given alpha factor produces the expected pixel intensity values and maintains the image data type.\n\nArgs:\n    alpha (float): Factor by which to adjust image contrast.\n    expected (int): The expected pixel value after applying the contrast adjustment.\n\nReturns:\n    None: The function asserts for correct behavior and does not return a value.\n    \nWhy:\n    Verifying contrast adjustment ensures that image preprocessing produces consistent and reliable results, which is critical for robust data augmentation in visual model pipelines.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.uint8) * 127\n    img = F.brightness_contrast_adjust(img, alpha=alpha)\n    assert img.dtype == np.dtype(\"uint8\")\n    assert (img == expected).all()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_contrast_float",
                "second_doc": "\"\"\"\nTest that the brightness_contrast_adjust function correctly adjusts the contrast of floating-point images by a specified alpha value and returns the expected output within numerical precision.\n\nArgs:\n    alpha (float): The contrast factor to adjust the image by.\n    expected (float): The expected resulting pixel value after adjustment.\n\nReturns:\n    None. Asserts that the output image has the correct dtype and pixel values match the expected result within machine precision.\n\nWhy:\n    This method ensures that contrast adjustments on float32 images produce accurate and predictable results, maintaining data integrity required for reliable computer vision preprocessing and augmentation.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.float32) * 0.4\n    expected = np.ones((100, 100, 3), dtype=np.float32) * expected\n    img = F.brightness_contrast_adjust(img, alpha=alpha)\n    assert img.dtype == np.dtype(\"float32\")\n    assert_array_almost_equal_nulp(img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_brightness",
                "second_doc": "\"\"\"\nTests that the brightness_contrast_adjust function correctly applies brightness adjustments to images based on the specified beta parameter.\n\nArgs:\n    beta (float): The brightness adjustment factor to apply to the input image.\n    expected (int): The expected pixel value of the output image after adjustment.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the resulting image does not have the correct dtype or expected pixel values.\n\nWhy:\n    Ensuring precise and reliable adjustment of image brightness is essential for maintaining data consistency and quality when augmenting image datasets for model training or evaluation.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.uint8) * 100\n    img = F.brightness_contrast_adjust(img, beta=beta)\n    assert img.dtype == np.dtype(\"uint8\")\n    assert (img == expected).all()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_brightness_float",
                "second_doc": "\"\"\"\nTest that adjusting the brightness of a floating-point image produces correct and predictable output values for different beta parameters.\n\nArgs:\n    beta (float): Brightness adjustment factor to apply to the image.\n    expected (float): The expected pixel value after brightness adjustment.\n\nReturns:\n    None\n\nWhy:\n    Verifies the numerical correctness and consistency of the brightness adjustment operation for floating-point images, ensuring reliability in scenarios where precise intensity transformations are essential.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.float32) * 0.4\n    expected = np.ones_like(img) * expected\n    img = F.brightness_contrast_adjust(img, beta=beta)\n    assert img.dtype == np.dtype(\"float32\")\n    assert_array_almost_equal_nulp(img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_gamma_transform",
                "second_doc": "\"\"\"\nTests whether the gamma_transform function accurately modifies the pixel values of a uniform image according to the specified gamma value, and verifies the output data type and expected output. This ensures the correctness and reliability of intensity transformations within the augmentation pipeline.\n\nArgs:\n    gamma (float): The gamma correction value to apply to the image.\n    expected (int): The expected pixel value in the transformed image for the given gamma.\n\nReturns:\n    None. Asserts that the transformed image matches the expected output and data type.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.uint8)\n    img = F.gamma_transform(img, gamma=gamma)\n    assert img.dtype == np.dtype(\"uint8\")\n    assert (img == expected).all()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_gamma_transform_float",
                "second_doc": "\"\"\"\nTest that applying the gamma transform to a floating-point image yields the expected results for different gamma values.\n\nThis method checks the correctness of the gamma transformation implementation, ensuring numerical stability and precision when working with floating-point image data.\n\nArgs:\n    gamma (float): The gamma value to use for the transformation.\n    expected (float): The expected uniform output pixel value after the transformation is applied.\n\nReturns:\n    None. Asserts that the transformed image data matches the expected result within numerical tolerance.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.float32) * 0.4\n    expected = np.ones((100, 100, 3), dtype=np.float32) * expected\n    img = F.gamma_transform(img, gamma=gamma)\n    assert img.dtype == np.dtype(\"float32\")\n    assert np.allclose(img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_to_float_without_max_value_specified",
                "second_doc": "\"\"\"\nTest that the to_float function correctly converts images with various unsigned integer types to normalized float representations when no maximum value is specified.\n\nThis ensures image data is properly scaled for further processing and model compatibility.\n\nArgs:\n    dtype: The NumPy data type of the input image (uint8, uint16, or uint32).\n    divider: The maximum possible value for the given dtype, used for normalization.\n\nReturns:\n    None. Asserts that the output from to_float matches the expected normalized float image.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=dtype)\n    expected = img.astype(\"float32\") / divider\n    assert_array_almost_equal_nulp(F.to_float(img), expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_to_float_with_max_value_specified",
                "second_doc": "\"\"\"\nTests whether converting an image to float format with a specified maximum value correctly scales pixel intensities, ensuring consistency and precision in preprocessing pipelines.\n\nArgs:\n    max_value (float): The value by which each pixel of the image will be divided during conversion.\n\nReturns:\n    None. Asserts correctness of transformation by comparing output to expected values.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.uint16)\n    expected = img.astype(\"float32\") / max_value\n    assert_array_almost_equal_nulp(F.to_float(img, max_value=max_value), expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_to_float_unknown_dtype",
                "second_doc": "\"\"\"\nTest that to_float raises a RuntimeError when called on an image with an unsupported dtype and no max_value specified.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    RuntimeError: If the maximum value for the given image dtype cannot be inferred automatically.\n\nWhy:\n    This method verifies that the conversion function properly handles input data types lacking implicit scaling information by requiring users to specify normalization parameters explicitly, thus preventing unintended behavior during data preparation.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.int16)\n    with pytest.raises(RuntimeError) as exc_info:\n        F.to_float(img)\n    assert str(exc_info.value) == (\n        \"Can't infer the maximum value for dtype int16. You need to specify the maximum value manually by passing \"\n        \"the max_value argument\"\n    )"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_to_float_unknown_dtype_with_max_value",
                "second_doc": "\"\"\"\nTests the conversion of an integer-type image to a float representation when the data type is unknown, ensuring pixel values are normalized by a specified maximum value.\n\nArgs:\n    max_value (float): The maximum value by which to normalize the pixel values of the integer image.\n\nReturns:\n    None. Asserts that the converted float image closely matches the expected normalized output.\n    \nWhy:\n    This test verifies that images with various value ranges are correctly scaled to the standard floating-point range, which is essential for consistent processing and model input expectations in image transformations.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.int16)\n    expected = img.astype(\"float32\") / max_value\n    assert_array_almost_equal_nulp(F.to_float(img, max_value=max_value), expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_from_float_without_max_value_specified",
                "second_doc": "\"\"\"\nTests the behavior of converting a floating-point image array to an integer type without specifying a maximum value, ensuring that pixel values are correctly scaled to use the full dynamic range of the specified integer data type.\n\nArgs:\n    dtype: The target integer data type for conversion (e.g., np.uint8, np.uint16, np.uint32).\n    multiplier: The value by which to multiply the float image to match the target type's range.\n\nReturns:\n    None. Asserts that the converted output matches the expected result for numerical similarity.\n    \nWhy:\n    This test verifies that image data can be accurately and automatically scaled from float to integer representations, ensuring consistent augmentation results across different image types during preprocessing.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.float32)\n    expected = (img * multiplier).astype(dtype)\n    assert_array_almost_equal_nulp(F.from_float(img, np.dtype(dtype)), expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_from_float_with_max_value_specified",
                "second_doc": "\"\"\"\nTests the conversion of a floating-point image array to an unsigned integer format when a specific maximum value is given. Ensures that the scaling and type casting operations produce accurate results when handling different dynamic ranges.\n\nArgs:\n    max_value (float): The maximum value to which the float image array should be scaled before conversion.\n\nReturns:\n    None: This is a test function; assertions within the function validate correctness.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.float32)\n    expected = (img * max_value).astype(np.uint32)\n    assert_array_almost_equal_nulp(F.from_float(img, dtype=np.uint32, max_value=max_value), expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_scale",
                "second_doc": "\"\"\"\nTest the scaling operation on 2D data for both image and mask targets, ensuring that interpolation and resizing produce consistent and predictable outputs.\n\nArgs:\n    target (str): Specifies whether the scaling operation should be tested on an 'image' or a 'mask'.\n\nReturns:\n    None: The test asserts correctness of the scaling transformation by comparing the output with an expected result.\n\nWhy:\n    Consistent and correct resizing operations are essential for maintaining data integrity during preprocessing, which is critical for downstream computer vision tasks.\n\"\"\"",
                "source_code": "img = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=np.uint8)\n    expected = np.array(\n        [\n            [1, 1, 2, 2, 3, 3],\n            [2, 2, 2, 3, 3, 4],\n            [3, 3, 4, 4, 5, 5],\n            [5, 5, 5, 6, 6, 7],\n            [6, 6, 7, 7, 8, 8],\n            [8, 8, 8, 9, 9, 10],\n            [9, 9, 10, 10, 11, 11],\n            [10, 10, 11, 11, 12, 12],\n        ],\n        dtype=np.uint8,\n    )\n\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    scaled = F.scale(img, scale=2, interpolation=cv2.INTER_LINEAR)\n    assert np.array_equal(scaled, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_longest_max_size",
                "second_doc": "\"\"\"\nTest that the longest_max_size function correctly resizes an input array along its longest edge to a specified maximum, maintaining aspect ratio for both images and masks. This ensures resizing functionality produces expected outputs, which is important for consistent data preprocessing in computer vision pipelines.\n\nArgs:\n    target (str): Specifies whether the input array represents an 'image' or a 'mask', determining the target format for conversion and validation.\n\nReturns:\n    None. The function asserts that the resized array matches the expected result.\n\"\"\"",
                "source_code": "img = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=np.uint8)\n    expected = np.array([[2, 3], [6, 7], [10, 11]], dtype=np.uint8)\n\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    scaled = F.longest_max_size(img, max_size=3, interpolation=cv2.INTER_LINEAR)\n    assert np.array_equal(scaled, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_smallest_max_size",
                "second_doc": "\"\"\"\nTest that resizing a 2D array to a specified maximum size produces the expected output for both images and masks.\n\nThis method verifies that the resizing function maintains spatial structure while limiting the largest dimension, which is essential for efficient processing and consistent input sizes in computer vision pipelines.\n\nArgs:\n    target (str): The type of input to test, either 'image' or 'mask'.\n\nReturns:\n    None. Asserts that the resized array matches the expected result.\n\"\"\"",
                "source_code": "img = np.array(\n        [[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]], dtype=np.uint8\n    )\n    expected = np.array([[2, 4, 5, 7], [10, 11, 13, 14], [17, 19, 20, 22]], dtype=np.uint8)\n\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    scaled = F.smallest_max_size(img, max_size=3, interpolation=cv2.INTER_LINEAR)\n    assert np.array_equal(scaled, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_from_float_unknown_dtype",
                "second_doc": "\"\"\"\nTests that attempting to convert a floating-point image to an integer type with unknown dynamic range raises a RuntimeError, ensuring that explicit user input is required for ambiguous data type conversions.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=np.float32)\n    with pytest.raises(RuntimeError) as exc_info:\n        F.from_float(img, np.dtype(np.int16))\n    assert str(exc_info.value) == (\n        \"Can't infer the maximum value for dtype int16. You need to specify the maximum value manually by passing \"\n        \"the max_value argument\"\n    )"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_resize_default_interpolation",
                "second_doc": "\"\"\"\nTests that the resize operation correctly reduces a 2D input (either image or mask) to the specified dimensions using the default interpolation method. Verifies both the resulting shape and content against the expected output.\n\nArgs:\n    target (str): Specifies whether the input to be resized is an \"image\" or a \"mask\".\n\nReturns:\n    None: The function performs assertions to validate correctness and does not return a value.\n\nWhy:\n    Ensures the integrity of the resize function by confirming it behaves as intended across different input types, which is critical for consistent preprocessing in vision workflows.\n\"\"\"",
                "source_code": "img = np.array([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]], dtype=np.uint8)\n    expected = np.array([[2, 2], [4, 4]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    resized_img = F.resize(img, 2, 2)\n    height, width = resized_img.shape[:2]\n    assert height == 2\n    assert width == 2\n    assert np.array_equal(resized_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_resize_nearest_interpolation",
                "second_doc": "\"\"\"\nTest that the nearest neighbor interpolation resizes a 2D array to the specified dimensions while preserving expected value placement. This ensures the resizing function works consistently for both image and mask formats.\n\nArgs:\n    target (str): The type of data format to use for testing, either \"image\" or \"mask\".\n\nReturns:\n    None. Asserts are used to verify that the resized output matches the expected array shape and values.\n\"\"\"",
                "source_code": "img = np.array([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]], dtype=np.uint8)\n    expected = np.array([[1, 1], [3, 3]], dtype=np.uint8)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    resized_img = F.resize(img, 2, 2, interpolation=cv2.INTER_NEAREST)\n    height, width = resized_img.shape[:2]\n    assert height == 2\n    assert width == 2\n    assert np.array_equal(resized_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_resize_different_height_and_width",
                "second_doc": "\"\"\"\nTest that resizing a 2D image or mask to specific height and width produces the expected output dimensions and format.\n\nArgs:\n    target (str): Specifies the data type to test the resizing on, either \"image\" for image data or \"mask\" for segmentation masks.\n\nReturns:\n    None\n\nWhy:\n    Ensuring images and masks can be resized to arbitrary dimensions without errors is essential for building flexible preprocessing pipelines and guaranteeing compatibility with neural network input requirements.\n\"\"\"",
                "source_code": "img = np.ones((100, 100), dtype=np.uint8)\n    img = convert_2d_to_target_format([img], target=target)\n    resized_img = F.resize(img, height=20, width=30)\n    height, width = resized_img.shape[:2]\n    assert height == 20\n    assert width == 30\n    if target == \"image\":\n        num_channels = resized_img.shape[2]\n        assert num_channels == 3"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_resize_default_interpolation_float",
                "second_doc": "\"\"\"\nTest that resizing a 2D floating-point array using the default interpolation method produces the expected output for both images and masks.\n\nArgs:\n    target (str): Specifies whether the data should be formatted as an \"image\" or a \"mask\".\n\nReturns:\n    None. Asserts that the resized output has the correct shape and values.\n\nThis ensures that the resizing operation preserves values and structure accurately for different image types, contributing to reliable preprocessing and augmentation workflows.\n\"\"\"",
                "source_code": "img = np.array(\n        [[0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2], [0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4]], dtype=np.float32\n    )\n    expected = np.array([[0.15, 0.15], [0.35, 0.35]], dtype=np.float32)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    resized_img = F.resize(img, 2, 2)\n    height, width = resized_img.shape[:2]\n    assert height == 2\n    assert width == 2\n    assert_array_almost_equal_nulp(resized_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_resize_nearest_interpolation_float",
                "second_doc": "\"\"\"\nTest that resizing a 2D float32 array using nearest neighbor interpolation produces the expected output for both images and masks.\n\nArgs:\n    target (str): Specifies whether the input should be treated as an \"image\" or a \"mask\".\n\nReturns:\n    None\n\nThe method verifies the correctness of geometric resizing operations, ensuring that transformation functions maintain data integrity when reducing array dimensions with nearest interpolation. This helps confirm that core augmentation operations do not introduce unintended artifacts, supporting accurate preprocessing in vision pipelines.\n\"\"\"",
                "source_code": "img = np.array(\n        [[0.1, 0.1, 0.1, 0.1], [0.2, 0.2, 0.2, 0.2], [0.3, 0.3, 0.3, 0.3], [0.4, 0.4, 0.4, 0.4]], dtype=np.float32\n    )\n    expected = np.array([[0.1, 0.1], [0.3, 0.3]], dtype=np.float32)\n    img, expected = convert_2d_to_target_format([img, expected], target=target)\n    resized_img = F.resize(img, 2, 2, interpolation=cv2.INTER_NEAREST)\n    height, width = resized_img.shape[:2]\n    assert height == 2\n    assert width == 2\n    assert np.array_equal(resized_img, expected)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_bbox_vflip",
                "second_doc": "\"\"\"\nTests the vertical flipping operation for bounding boxes to ensure coordinate transformation is performed correctly according to image dimensions.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "assert F.bbox_vflip((0.1, 0.2, 0.6, 0.5), 100, 200) == (0.1, 0.5, 0.6, 0.8)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_bbox_hflip",
                "second_doc": "\"\"\"\nTest horizontal flipping of bounding box coordinates to ensure correct transformation.\n\nThis method verifies that applying a horizontal flip to bounding box coordinates produces the expected new coordinates relative to the given image width and height. Ensuring accurate bounding box transformations is critical for object detection tasks where annotations must remain consistent with transformed images.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "assert F.bbox_hflip((0.1, 0.2, 0.6, 0.5), 100, 200) == (0.4, 0.2, 0.9, 0.5)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_bbox_flip",
                "second_doc": "\"\"\"\nTest that the bbox_flip function produces the same output as the explicit vertical, horizontal, or combined flip functions for bounding boxes.\n\nArgs:\n    code (int): An integer indicating the type of flip to perform (0 for vertical, 1 for horizontal, -1 for both).\n    func (Callable): The function that corresponds to the expected flip operation.\n\nReturns:\n    None\n\nWhy:\n    This ensures that the bbox_flip utility reliably applies the intended geometric transformations, which is essential for maintaining consistency between input images and their associated bounding boxes during data augmentation in computer vision tasks.\n\"\"\"",
                "source_code": "rows, cols = 100, 200\n    bbox = [0.1, 0.2, 0.6, 0.5]\n    assert F.bbox_flip(bbox, code, rows, cols) == func(bbox, rows, cols)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_crop_bbox_by_coords",
                "second_doc": "\"\"\"\nTest that bounding box coordinates are correctly recalculated after cropping an image region, ensuring spatial consistency between augmented images and associated labels.\n\nArgs:\n    None\n\nReturns:\n    None. The function asserts that the recalculated bounding box matches the expected output after cropping.\n\"\"\"",
                "source_code": "cropped_bbox = F.crop_bbox_by_coords((0.5, 0.2, 0.9, 0.7), (18, 18, 82, 82), 64, 64, 100, 100)\n    assert cropped_bbox == (0.5, 0.03125, 1.125, 0.8125)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_bbox_center_crop",
                "second_doc": "\"\"\"\nTests whether the center cropping transformation correctly adjusts bounding box coordinates for a given image size.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensuring the accuracy of bounding box adjustments during cropping is essential for maintaining the integrity of ground truth annotations after geometric transformations in computer vision workflows.\n\"\"\"",
                "source_code": "cropped_bbox = F.bbox_center_crop((0.5, 0.2, 0.9, 0.7), 64, 64, 100, 100)\n    assert cropped_bbox == (0.5, 0.03125, 1.125, 0.8125)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_bbox_crop",
                "second_doc": "\"\"\"\nTests the functionality of cropping a bounding box within an image, ensuring the bounding box coordinates are correctly adjusted after a crop operation. This helps maintain the accuracy of associated object annotations following image modifications.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "cropped_bbox = F.bbox_crop([0.5, 0.2, 0.9, 0.7], 24, 24, 64, 64, 100, 100)\n    assert cropped_bbox == (0.65, -0.1, 1.65, 1.15)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_bbox_random_crop",
                "second_doc": "\"\"\"\nTests the functionality of randomly cropping a bounding box within specified constraints to ensure the bounding box transformation operates as expected.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Verifying the behavior of bounding box cropping is important for maintaining data consistency and validity during augmentation procedures, which is crucial for reliable object detection workflows.\n\"\"\"",
                "source_code": "cropped_bbox = F.bbox_random_crop([0.5, 0.2, 0.9, 0.7], 80, 80, 0.2, 0.1, 100, 100)\n    assert cropped_bbox == (0.6, 0.2, 1.1, 0.825)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_bbox_rot90",
                "second_doc": "\"\"\"\nTest that rotating a bounding box by multiples of 90 degrees yields correct coordinates relative to the given image dimensions.\n\nArgs:\n    None\n\nReturns:\n    None\n\nThis test ensures that bounding box coordinates remain accurate and consistent after applying 90-degree rotations, validating the robustness of geometric transformations for object localization tasks.\n\"\"\"",
                "source_code": "assert F.bbox_rot90((0.1, 0.2, 0.3, 0.4), 0, 100, 200) == (0.1, 0.2, 0.3, 0.4)\n    assert F.bbox_rot90((0.1, 0.2, 0.3, 0.4), 1, 100, 200) == (0.2, 0.7, 0.4, 0.9)\n    assert F.bbox_rot90((0.1, 0.2, 0.3, 0.4), 2, 100, 200) == (0.7, 0.6, 0.9, 0.8)\n    assert F.bbox_rot90((0.1, 0.2, 0.3, 0.4), 3, 100, 200) == (0.6, 0.1, 0.8, 0.3)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_bbox_transpose",
                "second_doc": "\"\"\"\nTest the correctness of the bounding box transpose operation by comparing the function outputs to expected values for specific inputs.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This method verifies that transposing bounding boxes across different axes produces accurate results, which is essential for ensuring geometric transformations do not corrupt object location data during image augmentation.\n\"\"\"",
                "source_code": "assert np.allclose(F.bbox_transpose((0.7, 0.1, 0.8, 0.4), 0, 100, 200), (0.1, 0.7, 0.4, 0.8))\n    assert np.allclose(F.bbox_transpose((0.7, 0.1, 0.8, 0.4), 1, 100, 200), (0.6, 0.2, 0.9, 0.3))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_filter_bboxes",
                "second_doc": "\"\"\"\nTests that the filtering of bounding boxes based on minimum area and visibility criteria is performed correctly, ensuring only valid bounding boxes are retained after image transformations. This verification is important to maintain accurate training data for downstream tasks where object localization matters.\n\nArgs:\n    bboxes (list of tuple): List of bounding boxes, each represented as a tuple (x_min, y_min, x_max, y_max).\n    min_area (float): The minimum area required for a bounding box to be retained.\n    min_visibility (float): The minimum visibility percentage required for a bounding box to remain after augmentation.\n    target (list of tuple): The expected list of bounding boxes after filtering.\n\nReturns:\n    None. Asserts that the filtered bounding boxes match the expected output.\n\"\"\"",
                "source_code": "filtered_bboxes = filter_bboxes(bboxes, min_area=min_area, min_visibility=min_visibility, rows=100, cols=100)\n    assert filtered_bboxes == target"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_fun_max_size",
                "second_doc": "\"\"\"\nTests that resizing an image with the smallest_max_size function correctly adjusts its dimensions so the maximum side matches the specified target width. This ensures that images are standardized for further processing in computer vision pipelines.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "target_width = 256\n\n    img = np.empty((330, 49), dtype=np.uint8)\n    out = F.smallest_max_size(img, target_width, interpolation=cv2.INTER_LINEAR)\n\n    assert out.shape == (1724, target_width)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_is_rgb_image",
                "second_doc": "\"\"\"\nTest whether the function correctly distinguishes RGB images from other image types based on their shape.\n\nThis ensures that downstream image processing pipelines can accurately identify input image formats and handle them appropriately.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "image = np.ones((5, 5, 3), dtype=np.uint8)\n    assert F.is_rgb_image(image)\n\n    multispectral_image = np.ones((5, 5, 4), dtype=np.uint8)\n    assert not F.is_rgb_image(multispectral_image)\n\n    gray_image = np.ones((5, 5), dtype=np.uint8)\n    assert not F.is_rgb_image(gray_image)\n\n    gray_image = np.ones((5, 5, 1), dtype=np.uint8)\n    assert not F.is_rgb_image(gray_image)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_is_grayscale_image",
                "second_doc": "\"\"\"\nTests the functionality of the grayscale image detection method by verifying its response to various image array shapes and channel configurations.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This test ensures that the method correctly distinguishes between grayscale and non-grayscale images, which is essential for applying appropriate transformations and augmentations based on image types.\n\"\"\"",
                "source_code": "image = np.ones((5, 5, 3), dtype=np.uint8)\n    assert not F.is_grayscale_image(image)\n\n    multispectral_image = np.ones((5, 5, 4), dtype=np.uint8)\n    assert not F.is_grayscale_image(multispectral_image)\n\n    gray_image = np.ones((5, 5), dtype=np.uint8)\n    assert F.is_grayscale_image(gray_image)\n\n    gray_image = np.ones((5, 5, 1), dtype=np.uint8)\n    assert F.is_grayscale_image(gray_image)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_is_multispectral_image",
                "second_doc": "\"\"\"\nTest function to verify the behavior of is_multispectral_image with images of different channel configurations.  \nThis ensures the correct identification of multispectral images, supporting proper image processing workflows.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "image = np.ones((5, 5, 3), dtype=np.uint8)\n    assert not F.is_multispectral_image(image)\n\n    multispectral_image = np.ones((5, 5, 4), dtype=np.uint8)\n    assert F.is_multispectral_image(multispectral_image)\n\n    gray_image = np.ones((5, 5), dtype=np.uint8)\n    assert not F.is_multispectral_image(gray_image)\n\n    gray_image = np.ones((5, 5, 1), dtype=np.uint8)\n    assert not F.is_multispectral_image(gray_image)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_brightness_contrast",
                "second_doc": "\"\"\"\nTest that brightness and contrast adjustment functions produce consistent and accurate results across different image data types.\n\nThis method validates that, for various unsigned integer and floating-point image formats, the main brightness and contrast adjustment function produces results identical to its specialized internal implementations. It ensures reliability and numerical correctness of image intensity transformations, which is crucial for downstream processing and model training.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the outputs of the tested functions do not match for the given image types.\n\"\"\"",
                "source_code": "dtype = np.uint8\n    min_value = np.iinfo(dtype).min\n    max_value = np.iinfo(dtype).max\n\n    image_uint8 = np.random.randint(min_value, max_value, size=(5, 5, 3), dtype=dtype)\n\n    assert np.array_equal(F.brightness_contrast_adjust(image_uint8), F._brightness_contrast_adjust_uint(image_uint8))\n\n    assert np.array_equal(\n        F._brightness_contrast_adjust_non_uint(image_uint8), F._brightness_contrast_adjust_uint(image_uint8)\n    )\n\n    dtype = np.uint16\n    min_value = np.iinfo(dtype).min\n    max_value = np.iinfo(dtype).max\n\n    image_uint16 = np.random.randint(min_value, max_value, size=(5, 5, 3), dtype=dtype)\n\n    assert np.array_equal(\n        F.brightness_contrast_adjust(image_uint16), F._brightness_contrast_adjust_non_uint(image_uint16)\n    )\n\n    F.brightness_contrast_adjust(image_uint16)\n\n    dtype = np.uint32\n    min_value = np.iinfo(dtype).min\n    max_value = np.iinfo(dtype).max\n\n    image_uint32 = np.random.randint(min_value, max_value, size=(5, 5, 3), dtype=dtype)\n\n    assert np.array_equal(\n        F.brightness_contrast_adjust(image_uint32), F._brightness_contrast_adjust_non_uint(image_uint32)\n    )\n\n    image_float = np.random.random((5, 5, 3))\n\n    assert np.array_equal(\n        F.brightness_contrast_adjust(image_float), F._brightness_contrast_adjust_non_uint(image_float)\n    )"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_swap_tiles_on_image_with_empty_tiles",
                "second_doc": "\"\"\"\nVerifies that calling swap_tiles_on_image with an empty list of tile indices returns the original image unchanged. \n\nThis test ensures the function handles cases where no tiles are provided for swapping, maintaining data integrity in no-operation scenarios.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "img = np.array([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]], dtype=np.uint8)\n\n    result_img = F.swap_tiles_on_image(img, [])\n\n    assert np.array_equal(img, result_img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_swap_tiles_on_image_with_non_empty_tiles",
                "second_doc": "\"\"\"\nTests that specific tiles in an image are correctly swapped according to provided index mapping to ensure transformation functions modify image data as intended.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "img = np.array([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]], dtype=np.uint8)\n\n    tiles = np.array([[0, 0, 2, 2, 2, 2], [2, 2, 0, 0, 2, 2]])\n\n    target = np.array([[3, 3, 1, 1], [4, 4, 2, 2], [3, 3, 1, 1], [4, 4, 2, 2]], dtype=np.uint8)\n\n    result_img = F.swap_tiles_on_image(img, tiles)\n\n    assert np.array_equal(result_img, target)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_solarize",
                "second_doc": "\"\"\"\nTest that the solarize function inverts pixel values in an image above a specified threshold for various data types, ensuring correctness and boundary integrity.\n\nArgs:\n    dtype: The numpy data type of the input image, parameterized across supported types.\n\nReturns:\n    None. Asserts are used to verify the correctness of the solarize function's output.\n    \nWhy:\n    Verifying precise behavior of pixel value manipulation is essential to ensure that image transformations work reliably across datasets with different data types and intensity ranges, helping to maintain data integrity during preprocessing.\n\"\"\"",
                "source_code": "max_value = F.MAX_VALUES_BY_DTYPE[dtype]\n\n    if dtype == np.dtype(\"float32\"):\n        img = np.arange(2 ** 10, dtype=np.float32) / (2 ** 10)\n        img = img.reshape([2 ** 5, 2 ** 5])\n    else:\n        max_count = 1024\n        count = min(max_value + 1, 1024)\n        step = max(1, (max_value + 1) // max_count)\n        shape = [int(np.sqrt(count))] * 2\n        img = np.arange(0, max_value + 1, step, dtype=dtype).reshape(shape)\n\n    for threshold in [0, max_value // 3, max_value // 3 * 2, max_value, max_value + 1]:\n        check_img = img.copy()\n        cond = check_img >= threshold\n        check_img[cond] = max_value - check_img[cond]\n\n        result_img = F.solarize(img, threshold=threshold)\n\n        assert np.all(np.isclose(result_img, check_img))\n        assert np.min(result_img) >= 0\n        assert np.max(result_img) <= max_value"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_posterize_checks",
                "second_doc": "\"\"\"\nTests proper error handling in the posterize function by verifying that it raises assertions for invalid image types or mismatched parameter requirements.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensuring the posterize transformation validates input types and parameter compatibility is crucial for catching user errors early and maintaining robustness in image processing pipelines.\n\"\"\"",
                "source_code": "img = np.random.random([256, 256, 3])\n    with pytest.raises(AssertionError) as exc_info:\n        F.posterize(img, 4)\n    assert str(exc_info.value) == \"Image must have uint8 channel type\"\n\n    img = np.random.randint(0, 256, [256, 256], dtype=np.uint8)\n    with pytest.raises(AssertionError) as exc_info:\n        F.posterize(img, [1, 2, 3])\n    assert str(exc_info.value) == \"If bits is iterable image must be RGB\""
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_equalize_checks",
                "second_doc": "\"\"\"\nTests the error handling behavior of the equalize function by verifying it raises informative exceptions for unsupported modes, incorrect mask shapes, and invalid image data types.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This method ensures robust input validation and clear user feedback when incorrect arguments are passed to the equalization function, helping prevent silent failures and making it easier to debug data processing pipelines.\n\"\"\"",
                "source_code": "img = np.random.randint(0, 255, [256, 256], dtype=np.uint8)\n\n    with pytest.raises(ValueError) as exc_info:\n        F.equalize(img, mode=\"other\")\n    assert str(exc_info.value) == \"Unsupported equalization mode. Supports: ['cv', 'pil']. Got: other\"\n\n    mask = np.random.randint(0, 1, [256, 256, 3], dtype=np.bool)\n    with pytest.raises(ValueError) as exc_info:\n        F.equalize(img, mask=mask)\n    assert str(exc_info.value) == \"Wrong mask shape. Image shape: {}. Mask shape: {}\".format(img.shape, mask.shape)\n\n    img = np.random.randint(0, 255, [256, 256, 3], dtype=np.uint8)\n    with pytest.raises(ValueError) as exc_info:\n        F.equalize(img, mask=mask, by_channels=False)\n    assert str(exc_info.value) == \"When by_channels=False only 1-channel mask supports. \" \"Mask shape: {}\".format(\n        mask.shape\n    )\n\n    img = np.random.random([256, 256, 3])\n    with pytest.raises(AssertionError) as exc_info:\n        F.equalize(img, mask=mask, by_channels=False)\n    assert str(exc_info.value) == \"Image must have uint8 channel type\""
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_equalize_grayscale",
                "second_doc": "\"\"\"\nTests whether the custom image equalization function produces the same result as OpenCV's standard histogram equalization for grayscale images.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures consistency and correctness between the custom implementation and the widely used OpenCV function, which is important for maintaining reliable and predictable preprocessing operations across different components.\n\"\"\"",
                "source_code": "img = np.random.randint(0, 255, [256, 256], dtype=np.uint8)\n    assert np.all(cv2.equalizeHist(img) == F.equalize(img, mode=\"cv\"))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_equalize_rgb",
                "second_doc": "\"\"\"\nTests the correctness of the image equalization function by comparing its results to OpenCV implementations for both per-channel and luminance-based modes. This ensures consistent enhancement of image contrast, which is essential for robust image processing pipelines.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "img = np.random.randint(0, 255, [256, 256, 3], dtype=np.uint8)\n\n    _img = img.copy()\n    for i in range(3):\n        _img[..., i] = cv2.equalizeHist(_img[..., i])\n    assert np.all(_img == F.equalize(img, mode=\"cv\"))\n\n    _img = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n    img_cv = _img.copy()\n    img_cv[..., 0] = cv2.equalizeHist(_img[..., 0])\n    img_cv = cv2.cvtColor(img_cv, cv2.COLOR_YCrCb2RGB)\n    assert np.all(img_cv == F.equalize(img, mode=\"cv\", by_channels=False))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_equalize_grayscale_mask",
                "second_doc": "\"\"\"\nTest that histogram equalization is correctly applied to only the masked region of a grayscale image.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that the equalization transformation operates as expected when a mask is specified, validating correct behavior of selective image processing within the augmentation pipeline.\n\"\"\"",
                "source_code": "img = np.random.randint(0, 255, [256, 256], dtype=np.uint8)\n\n    mask = np.zeros([256, 256], dtype=np.bool)\n    mask[:10, :10] = True\n\n    assert np.all(cv2.equalizeHist(img[:10, :10]) == F.equalize(img, mask=mask, mode=\"cv\")[:10, :10])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_equalize_rgb_mask",
                "second_doc": "\"\"\"\nTests the correctness of RGB image histogram equalization within masked regions, ensuring channel-specific and multi-channel behaviors produce expected outputs.\n\nThis function verifies that the equalization operation is accurately applied to user-defined areas of an image, both when treating channels independently and when operating across channels, thereby guaranteeing the reliability of mask-based augmentation.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "img = np.random.randint(0, 255, [256, 256, 3], dtype=np.uint8)\n\n    mask = np.zeros([256, 256], dtype=np.bool)\n    mask[:10, :10] = True\n\n    _img = img.copy()[:10, :10]\n    for i in range(3):\n        _img[..., i] = cv2.equalizeHist(_img[..., i])\n    assert np.all(_img == F.equalize(img, mask, mode=\"cv\")[:10, :10])\n\n    _img = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n    img_cv = _img.copy()[:10, :10]\n    img_cv[..., 0] = cv2.equalizeHist(img_cv[..., 0])\n    img_cv = cv2.cvtColor(img_cv, cv2.COLOR_YCrCb2RGB)\n    assert np.all(img_cv == F.equalize(img, mask=mask, mode=\"cv\", by_channels=False)[:10, :10])\n\n    mask = np.zeros([256, 256, 3], dtype=np.bool)\n    mask[:10, :10, 0] = True\n    mask[10:20, 10:20, 1] = True\n    mask[20:30, 20:30, 2] = True\n    img_r = img.copy()[:10, :10, 0]\n    img_g = img.copy()[10:20, 10:20, 1]\n    img_b = img.copy()[20:30, 20:30, 2]\n\n    img_r = cv2.equalizeHist(img_r)\n    img_g = cv2.equalizeHist(img_g)\n    img_b = cv2.equalizeHist(img_b)\n\n    result_img = F.equalize(img, mask=mask, mode=\"cv\")\n    assert np.all(img_r == result_img[:10, :10, 0])\n    assert np.all(img_g == result_img[10:20, 10:20, 1])\n    assert np.all(img_b == result_img[20:30, 20:30, 2])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_downscale_ones",
                "second_doc": "\"\"\"\nTest that the downscaling operation preserves the values of an image filled with ones for different data types. This verifies the function's ability to correctly process and maintain homogeneous regions during scaling, thereby ensuring data integrity during image resizing.\n\nArgs:\n    dtype (str): The data type of the input image, tested with values \"float32\" and \"uint8\".\n\nReturns:\n    None. Asserts that the downscaled image is equal to the original.\n\"\"\"",
                "source_code": "img = np.ones((100, 100, 3), dtype=dtype)\n    downscaled = F.downscale(img, scale=0.5)\n    assert np.all(downscaled == img)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_downscale_random",
                "second_doc": "\"\"\"\nTests that the downscale function preserves the original image shape and content when scaling by certain factors, ensuring the consistency and correctness of image transformation operations.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This method verifies that downscaling an image by specific scale factors does not alter its shape or pixel values, which is essential for maintaining integrity during image preprocessing and augmentation workflows.\n\"\"\"",
                "source_code": "img = np.random.rand(100, 100, 3)\n    downscaled = F.downscale(img, scale=0.5)\n    assert downscaled.shape == img.shape\n    downscaled = F.downscale(img, scale=1)\n    assert np.all(img == downscaled)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_maybe_process_in_chunks",
                "second_doc": "\"\"\"\nTests whether processing an image with a rotation transformation preserves the shape when applied to varying numbers of image channels.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensuring that image transformations do not inadvertently alter the dimensionality or shape of image data is crucial for maintaining data integrity throughout processing pipelines.\n\"\"\"",
                "source_code": "image = np.random.randint(0, 256, (100, 100, 6), np.uint8)\n\n    for i in range(1, image.shape[-1] + 1):\n        before = image[:, :, :i]\n        after = F.rotate(before, angle=1)\n        assert before.shape == after.shape"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_imgaug.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "test_imagaug_dual_augmentations_are_deterministic",
                "second_doc": "\"\"\"\nTest that dual augmentations yield deterministic results by ensuring the same transformation is applied to both image and mask data. This is critical for tasks like segmentation where spatial alignment between image and mask must be preserved after augmentation.\n\nArgs:\n    augmentation_cls: An augmentation class to be tested, such as IAAPiecewiseAffine, IAAPerspective, or IAAFliplr.\n\nReturns:\n    None. Asserts that the augmented image and mask are identical for each iteration, raising an AssertionError if this is not the case.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1)\n    mask = np.copy(image)\n    for _i in range(10):\n        data = aug(image=image, mask=mask)\n        assert np.array_equal(data[\"image\"], data[\"mask\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_imagaug_fliplr_transform_bboxes",
                "second_doc": "\"\"\"\nTest the horizontal flip transformation to verify that bounding boxes and masks are accurately updated in sync with the image. This ensures that spatial annotations remain consistent after image augmentations are applied, which is critical for maintaining data integrity during training and evaluation.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the flipped mask does not match the image, or if the transformed bounding boxes do not align with the expected positions.\n\"\"\"",
                "source_code": "aug = IAAFliplr(p=1)\n    mask = np.copy(image)\n    bboxes = [(10, 10, 20, 20), (20, 10, 30, 40)]\n    expect = [(79, 10, 89, 20), (69, 10, 79, 40)]\n    bboxes = convert_bboxes_to_albumentations(bboxes, \"pascal_voc\", rows=image.shape[0], cols=image.shape[1])\n    data = aug(image=image, mask=mask, bboxes=bboxes)\n    actual = convert_bboxes_from_albumentations(data[\"bboxes\"], \"pascal_voc\", rows=image.shape[0], cols=image.shape[1])\n    assert np.array_equal(data[\"image\"], data[\"mask\"])\n    assert np.allclose(actual, expect)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_imagaug_flipud_transform_bboxes",
                "second_doc": "\"\"\"\nTests that vertically flipping an image with bounding boxes correctly updates both the image and the corresponding box coordinates to match the expected post-transform locations.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This ensures that geometric augmentations applied to images are consistently and accurately applied to associated bounding boxes, preserving spatial relationships necessary for reliable model training and evaluation.\n\"\"\"",
                "source_code": "aug = IAAFlipud(p=1)\n    mask = np.copy(image)\n    dummy_class = 1234\n    bboxes = [(10, 10, 20, 20, dummy_class), (20, 10, 30, 40, dummy_class)]\n    expect = [(10, 79, 20, 89, dummy_class), (20, 59, 30, 89, dummy_class)]\n    bboxes = convert_bboxes_to_albumentations(bboxes, \"pascal_voc\", rows=image.shape[0], cols=image.shape[1])\n    data = aug(image=image, mask=mask, bboxes=bboxes)\n    actual = convert_bboxes_from_albumentations(data[\"bboxes\"], \"pascal_voc\", rows=image.shape[0], cols=image.shape[1])\n    assert np.array_equal(data[\"image\"], data[\"mask\"])\n    assert np.allclose(actual, expect)"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_keypoint.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_keypoint_to_albumentations",
                "second_doc": "\"\"\"\nTest the conversion of keypoints from various coordinate and attribute formats into the standardized representation used by the library.\n\nArgs:\n    kp: The keypoint input, which can be a tuple or numpy array consisting of coordinates and optional attributes.\n    source_format: A string specifying the format and order of the input keypoint (e.g., \"xy\", \"yx\", \"xys\", \"xya\", \"xyas\").\n    expected: The expected standardized tuple representation of the keypoint after conversion.\n\nReturns:\n    None\n\nWhy:\n    Ensures that keypoints from different formats and conventions are consistently converted, which is essential for the correct and reliable spatial manipulation of associated annotations during image augmentation.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n\n    converted_keypoint = convert_keypoint_to_albumentations(\n        kp, rows=image.shape[0], cols=image.shape[1], source_format=source_format\n    )\n    assert converted_keypoint == expected"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_keypoint_from_albumentations",
                "second_doc": "\"\"\"\nTests the correctness of converting keypoints from Albumentations format to various target formats.\n\nThis method ensures keypoints are accurately transformed depending on the desired output convention, which is crucial for maintaining consistency in downstream image processing and analysis tasks.\n\nArgs:\n    kp (tuple): The keypoint in Albumentations format to be converted.\n    target_format (str): The desired keypoint coordinate representation (e.g., \"xy\", \"yx\", \"xya\", \"xys\", or \"xyas\").\n    expected (tuple): The expected result after conversion for verification.\n\nReturns:\n    None: Asserts that the converted keypoint matches the expected result.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    converted_keypoint = convert_keypoint_from_albumentations(\n        kp, rows=image.shape[0], cols=image.shape[1], target_format=target_format\n    )\n    assert converted_keypoint == expected"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_keypoint_to_albumentations_and_back",
                "second_doc": "\"\"\"\nTests the round-trip conversion of keypoints to and from the internal representation, ensuring that coordinate transformations are lossless and consistent across supported formats.\n\nArgs:\n    kp (tuple): The original keypoint in a specified format.\n    keypoint_format (str): The format of the keypoint (e.g., \"xy\", \"xyas\", \"xysa\", \"yx\").\n\nReturns:\n    None. Asserts that converting the keypoint to the intermediate format and back preserves the original values.\n\nWhy:\n    Verifying precise and reversible keypoint conversion is crucial for maintaining annotation integrity during image transformations, ensuring downstream tasks receive accurate spatial data.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    converted_kp = convert_keypoint_to_albumentations(\n        kp, rows=image.shape[0], cols=image.shape[1], source_format=keypoint_format\n    )\n    converted_back_kp = convert_keypoint_from_albumentations(\n        converted_kp, rows=image.shape[0], cols=image.shape[1], target_format=keypoint_format\n    )\n    assert converted_back_kp == kp"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_keypoints_to_albumentations",
                "second_doc": "\"\"\"\nTests that converting multiple keypoints to the Albumentations format produces consistent results with individual keypoint conversions.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Verifying the consistency of batch and single keypoint conversions is important to ensure interoperability, correctness, and reliability in processing keypoint annotations for downstream image transformation workflows.\n\"\"\"",
                "source_code": "keypoints = [(20, 30, 40, 50), (30, 40, 50, 60, 99)]\n    image = np.ones((100, 100, 3))\n    converted_keypoints = convert_keypoints_to_albumentations(\n        keypoints, rows=image.shape[0], cols=image.shape[1], source_format=\"xyas\"\n    )\n    converted_keypoint_1 = convert_keypoint_to_albumentations(\n        keypoints[0], rows=image.shape[0], cols=image.shape[1], source_format=\"xyas\"\n    )\n    converted_keypoint_2 = convert_keypoint_to_albumentations(\n        keypoints[1], rows=image.shape[0], cols=image.shape[1], source_format=\"xyas\"\n    )\n    assert converted_keypoints == [converted_keypoint_1, converted_keypoint_2]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_convert_keypoints_from_albumentations",
                "second_doc": "\"\"\"\nTest that converting keypoints from one format to another produces consistent and expected results when working with image data.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    To ensure the correctness and reliability of keypoint conversion utilities, which is important for maintaining compatibility and integrity in data preprocessing workflows for computer vision models.\n\"\"\"",
                "source_code": "keypoints = [(0.2, 0.3, 0.6, 0.8), (0.3, 0.4, 0.7, 0.9, 99)]\n    image = np.ones((100, 100, 3))\n    converted_keypointes = convert_keypoints_from_albumentations(\n        keypoints, rows=image.shape[0], cols=image.shape[1], target_format=\"xyas\"\n    )\n    converted_keypoint_1 = convert_keypoint_from_albumentations(\n        keypoints[0], rows=image.shape[0], cols=image.shape[1], target_format=\"xyas\"\n    )\n    converted_keypoint_2 = convert_keypoint_from_albumentations(\n        keypoints[1], rows=image.shape[0], cols=image.shape[1], target_format=\"xyas\"\n    )\n    assert converted_keypointes == [converted_keypoint_1, converted_keypoint_2]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compose_with_keypoint_noop",
                "second_doc": "\"\"\"\nTest that applying a no-operation transformation pipeline preserves the input image and keypoints without modification, ensuring the augmentation pipeline's handling of keypoints and label fields works as expected.\n\nArgs:\n    keypoints (list of tuples): The coordinates of keypoints to be passed through the augmentation.\n    keypoint_format (str): The format used to interpret keypoint coordinates.\n    labels (list or None): Optional labels associated with the provided keypoints.\n\nReturns:\n    None: Asserts that the image and keypoints remain unchanged after the no-operation transform, and raises an AssertionError if the behavior is incorrect.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    if labels is not None:\n        aug = Compose([NoOp(p=1.0)], keypoint_params={\"format\": keypoint_format, \"label_fields\": [\"labels\"]})\n        transformed = aug(image=image, keypoints=keypoints, labels=labels)\n    else:\n        aug = Compose([NoOp(p=1.0)], keypoint_params={\"format\": keypoint_format})\n        transformed = aug(image=image, keypoints=keypoints)\n    assert np.array_equal(transformed[\"image\"], image)\n    assert transformed[\"keypoints\"] == keypoints"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compose_with_keypoint_noop_error_label_fields",
                "second_doc": "\"\"\"\nTests that an exception is raised when label fields are incorrectly specified for keypoint transformations within an augmentation pipeline.\n\nArgs:\n    keypoints (list): List of keypoints to be transformed.\n    keypoint_format (str): Format of the provided keypoints.\n\nReturns:\n    None\n\nRaises:\n    Exception: If the required label fields for keypoints are not handled correctly.\n    \nThis test ensures that the augmentation pipeline properly validates the relationship between keypoints and their associated label fields, helping prevent unnoticed configuration errors during data augmentation.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    aug = Compose([NoOp(p=1.0)], keypoint_params={\"format\": keypoint_format, \"label_fields\": \"class_id\"})\n    with pytest.raises(Exception):\n        aug(image=image, keypoints=keypoints, cls_id=[0])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compose_with_keypoint_noop_label_outside",
                "second_doc": "\"\"\"\nTest that a no-operation augmentation leaves the image, keypoints, and associated labels unchanged, even when keypoints or label fields are empty or structured in various ways.\n\nArgs:\n    keypoints (list): List of keypoints to process.\n    keypoint_format (str): Format of the input keypoints (e.g., \"xy\").\n    labels (dict): Dictionary containing optional label fields associated with keypoints.\n\nReturns:\n    None\n\nWhy:\n    Verifies the consistency and integrity of data passed through an augmentation pipeline when a no-op transformation is used, ensuring that no unintended modifications occur for images, keypoints, or label data regardless of input structure.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    aug = Compose([NoOp(p=1.0)], keypoint_params={\"format\": keypoint_format, \"label_fields\": list(labels.keys())})\n    transformed = aug(image=image, keypoints=keypoints, **labels)\n    assert np.array_equal(transformed[\"image\"], image)\n    assert transformed[\"keypoints\"] == keypoints\n    for k, v in labels.items():\n        assert transformed[k] == v"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_sized_crop_size",
                "second_doc": "\"\"\"\nTest the RandomSizedCrop augmentation to ensure that it resizes the image to the desired dimensions while preserving the number of keypoints.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This test verifies that after applying a cropping augmentation that randomly selects crop sizes within specified bounds, the output image has the correct target size and all input keypoints are retained. This helps confirm that spatial transformations applied during augmentation do not inadvertently lose or misalign associated data such as keypoints.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    keypoints = [(0.2, 0.3, 0.6, 0.8), (0.3, 0.4, 0.7, 0.9, 99)]\n    aug = RandomSizedCrop(min_max_height=(70, 90), height=50, width=50, p=1.0)\n    transformed = aug(image=image, keypoints=keypoints)\n    assert transformed[\"image\"].shape == (50, 50, 3)\n    assert len(keypoints) == len(transformed[\"keypoints\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_random_resized_crop_size",
                "second_doc": "\"\"\"\nTest that the RandomResizedCrop transform correctly resizes images and maintains the correspondence and count of associated keypoints after augmentation.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that applying a resizing and cropping augmentation not only produces an output image of the expected dimensions but also preserves the integrity and number of keypoints, which is vital for maintaining reliable annotations during pre-processing in computer vision workflows.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    keypoints = [(0.2, 0.3, 0.6, 0.8), (0.3, 0.4, 0.7, 0.9, 99)]\n    aug = RandomResizedCrop(height=50, width=50, p=1.0)\n    transformed = aug(image=image, keypoints=keypoints)\n    assert transformed[\"image\"].shape == (50, 50, 3)\n    assert len(keypoints) == len(transformed[\"keypoints\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_keypoint_flips_transform_3x3",
                "second_doc": "\"\"\"\nTests that keypoint coordinates are correctly transformed when horizontal and vertical flip augmentations are applied to 3x3 images. This ensures that keypoint augmentation preserves spatial consistency, which is critical for maintaining accurate spatial relationships in data augmentation pipelines.\n\nArgs:\n    aug: The augmentation class to apply (e.g., HorizontalFlip or VerticalFlip).\n    keypoints: A list of keypoints (as [x, y] pairs) to be transformed.\n    expected: A list of expected keypoint coordinates after the transformation.\n\nReturns:\n    None. Asserts that the transformed keypoints match the expected results.\n\"\"\"",
                "source_code": "transform = Compose([aug(p=1)], keypoint_params={\"format\": \"xy\"})\n\n    image = np.ones((3, 3, 3))\n    transformed = transform(image=image, keypoints=keypoints, labels=np.ones(len(keypoints)))\n    assert np.allclose(expected, transformed[\"keypoints\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_keypoint_transform_format_xyas",
                "second_doc": "\"\"\"\nTest that keypoint transformations in the 'xyas' format are correctly handled when applying horizontal and vertical flip augmentations.\n\nThis method verifies that geometric transformations applied to keypoints using specific augmentation functions produce the expected results. Ensuring the accuracy of keypoint transformation is essential for reliable augmentation in keypoint-based tasks.\n\nArgs:\n    aug: The augmentation class to apply (e.g., HorizontalFlip, VerticalFlip).\n    keypoints: A list of keypoints in the format [x, y, angle, scale].\n    expected: The expected result of keypoints after the transformation.\n\nReturns:\n    None. Asserts that the transformed keypoints match the expected outputs.\n\"\"\"",
                "source_code": "transform = Compose(\n        [aug(p=1)], keypoint_params={\"format\": \"xyas\", \"angle_in_degrees\": True, \"label_fields\": [\"labels\"]}\n    )\n\n    image = np.ones((100, 100, 3))\n    transformed = transform(image=image, keypoints=keypoints, labels=np.ones(len(keypoints)))\n    assert np.allclose(expected, transformed[\"keypoints\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_keypoint_transform_format_xy",
                "second_doc": "\"\"\"\nTest that keypoint coordinates in 'xy' format are correctly transformed by horizontal and vertical flip augmentations.\n\nArgs:\n    aug: The augmentation class to be applied (e.g., horizontal flip or vertical flip).\n    keypoints: List of input keypoints, each defined as a tuple (x, y, angle, scale).\n    expected: List of expected keypoints after applying the augmentation.\n\nReturns:\n    None\n\nWhy:\n    Ensures that geometric transformations preserve the integrity and expected behavior of keypoint annotations during augmentation, which is vital for consistent downstream model training and evaluation.\n\"\"\"",
                "source_code": "transform = Compose([aug(p=1)], keypoint_params={\"format\": \"xy\", \"label_fields\": [\"labels\"]})\n\n    image = np.ones((100, 100, 3))\n    transformed = transform(image=image, keypoints=keypoints, labels=np.ones(len(keypoints)))\n    assert np.allclose(expected, transformed[\"keypoints\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_iaa_transforms_emit_warning",
                "second_doc": "\"\"\"\nTest that a warning is correctly emitted when a keypoint format incompatible with the specified transformation is used.\n\nArgs:\n    aug: The augmentation transformation class to apply.\n    keypoints: List of keypoints in the given format to be transformed.\n    expected: The expected keypoints after transformation.\n\nReturns:\n    None. Asserts that a UserWarning is raised and matches the expected warning message.\n    \nWhy:\n    This method ensures users are alerted when attempting to use transformations with inappropriate keypoint formats, preventing unintended behavior during data augmentation pipelines.\n\"\"\"",
                "source_code": "with pytest.warns(UserWarning, match=\"IAAFliplr transformation supports only 'xy' keypoints augmentation\"):\n        Compose([aug(p=1)], keypoint_params={\"format\": \"xyas\", \"label_fields\": [\"labels\"]})"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_keypoint_rotate90",
                "second_doc": "\"\"\"\nTests that rotating a keypoint by multiples of 90 degrees results in the expected position and orientation within a specified image size.\n\nArgs:\n    keypoint (tuple): The starting keypoint as (x, y, angle, scale).\n    expected (tuple): The expected keypoint after rotation.\n    factor (int): The number of times to rotate the keypoint by 90 degrees.\n\nReturns:\n    None. Asserts that the transformed keypoint matches the expected result.\n\nWhy:\n    Ensuring correct transformation of keypoints during 90-degree rotations is essential for maintaining spatial and semantic consistency in geometric augmentation workflows, which is critical for tasks such as keypoint detection and tracking.\n\"\"\"",
                "source_code": "actual = F.keypoint_rot90(keypoint, factor, rows=100, cols=200)\n    assert actual == expected"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_keypoint_rotate",
                "second_doc": "\"\"\"\nTest that keypoints are correctly rotated by the specified angle within an image of defined dimensions. This ensures that spatial annotations remain accurate after rotation, which is essential for maintaining the validity of training data in augmentation pipelines.\n\nArgs:\n    keypoint (list): The original keypoint represented as [x, y, angle, scale].\n    expected (list): The expected keypoint values after rotation.\n    angle (float): The rotation angle in degrees.\n\nReturns:\n    None: Asserts that the rotated keypoint matches the expected output within a tolerance.\n\"\"\"",
                "source_code": "actual = F.keypoint_rotate(keypoint, angle, rows=100, cols=100)\n    np.testing.assert_allclose(actual, expected, atol=1e-7)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_keypoint_scale",
                "second_doc": "\"\"\"\nTest that the keypoint scaling function correctly adjusts keypoint scale values during image transformations to maintain spatial consistency between original and augmented data.\n\nArgs:\n    keypoint (list[float]): Input keypoint represented as [x, y, angle, scale].\n    expected (list[float]): The expected output keypoint after scaling.\n    scale (float): The scale factor to apply to the keypoint coordinates and scale value.\n\nReturns:\n    None: Asserts that the scaled keypoint matches the expected output within specified tolerance.\n\"\"\"",
                "source_code": "actual = F.keypoint_scale(keypoint, scale, scale)\n    np.testing.assert_allclose(actual, expected, atol=1e-7)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_keypoint_shift_scale_rotate",
                "second_doc": "\"\"\"\nTests the correctness of the keypoint_shift_scale_rotate function by verifying that a keypoint is accurately transformed according to specified rotation, scaling, and shifting parameters within an image of defined size.\n\nThis ensures that geometric keypoint transformations are performed correctly, which is essential for maintaining spatial consistency and accurate annotation mapping after augmentation.\n\nArgs:\n    keypoint (list): A list representing the original keypoint coordinates and attributes (e.g., [x, y, angle, scale]).\n    expected (list): The expected result of the transformation, for comparison.\n    angle (float): The rotation angle to apply, in degrees.\n    scale (float): The scaling factor to apply.\n    dx (float): The relative horizontal shift to apply.\n    dy (float): The relative vertical shift to apply.\n\nReturns:\n    None. Asserts that the actual output matches the expected result within a relative tolerance.\n\"\"\"",
                "source_code": "actual = F.keypoint_shift_scale_rotate(keypoint, angle, scale, dx, dy, rows=100, cols=200)\n    np.testing.assert_allclose(actual, expected, rtol=1e-4)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_compose_with_additional_targets",
                "second_doc": "\"\"\"\nTests that the augmentation pipeline can correctly apply transformations to both primary and additional keypoint targets within an image, ensuring consistency and correctness of the augmentation output.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    To verify that the augmentation framework accurately handles multiple sets of keypoints when additional targets are specified, maintaining data integrity across all related annotations.\n\"\"\"",
                "source_code": "image = np.ones((100, 100, 3))\n    keypoints = [(10, 10), (50, 50)]\n    kp1 = [(15, 15), (55, 55)]\n    aug = Compose([CenterCrop(50, 50)], keypoint_params={\"format\": \"xy\"}, additional_targets={\"kp1\": \"keypoints\"})\n    transformed = aug(image=image, keypoints=keypoints, kp1=kp1)\n    assert transformed[\"keypoints\"] == [(25, 25)]\n    assert transformed[\"kp1\"] == [(30, 30)]"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_pytorch.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "test_torch_to_tensor_v2_augmentations",
                "second_doc": "\"\"\"\nTests that the ToTensorV2 augmentation correctly converts input images and masks to PyTorch tensors with the expected shapes and data types.\n\nArgs:\n    image (numpy.ndarray): The input image to be converted.\n    mask (numpy.ndarray): The input mask to be converted.\n\nReturns:\n    None\n\nWhy:\n    Verifying the correctness of data type and shape conversions is essential to ensure compatibility and reliability when bridging image preprocessing pipelines with deep learning frameworks such as PyTorch.\n\"\"\"",
                "source_code": "aug = ToTensorV2()\n    data = aug(image=image, mask=mask, force_apply=True)\n    assert isinstance(data[\"image\"], torch.Tensor) and data[\"image\"].shape == image.shape[::-1]\n    assert isinstance(data[\"mask\"], torch.Tensor) and data[\"mask\"].shape == mask.shape\n    assert data[\"image\"].dtype == torch.uint8\n    assert data[\"mask\"].dtype == torch.uint8"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_additional_targets_for_totensorv2",
                "second_doc": "\"\"\"\nTest that the transformation pipeline correctly handles additional image and mask targets and converts them to PyTorch tensors with the expected shape, type, and value equality.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This ensures that augmentation pipelines can seamlessly process multiple related images and masks, maintaining data consistency and compatibility with PyTorch-based workflows.\n\"\"\"",
                "source_code": "aug = A.Compose([ToTensorV2()], additional_targets={\"image2\": \"image\", \"mask2\": \"mask\"})\n    for _i in range(10):\n        image1 = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n        image2 = image1.copy()\n        mask1 = np.random.randint(low=0, high=256, size=(100, 100, 4), dtype=np.uint8)\n        mask2 = mask1.copy()\n        res = aug(image=image1, image2=image2, mask=mask1, mask2=mask2)\n        assert isinstance(res[\"image\"], torch.Tensor) and res[\"image\"].shape == image1.shape[::-1]\n        assert isinstance(res[\"image2\"], torch.Tensor) and res[\"image2\"].shape == image2.shape[::-1]\n        assert isinstance(res[\"mask\"], torch.Tensor) and res[\"mask\"].shape == mask1.shape\n        assert isinstance(res[\"mask2\"], torch.Tensor) and res[\"mask2\"].shape == mask2.shape\n        assert np.array_equal(res[\"image\"], res[\"image2\"])\n        assert np.array_equal(res[\"mask\"], res[\"mask2\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_torch_to_tensor_augmentations",
                "second_doc": "\"\"\"\nTest that the ToTensor augmentation correctly converts the image and mask data types to torch.float32 and emits a deprecation warning.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures compatibility and proper data formatting when integrating augmentation pipelines with deep learning workflows, specifically validating that deprecated transformations still yield expected tensor outputs.\n\"\"\"",
                "source_code": "with pytest.warns(DeprecationWarning):\n        aug = ToTensor()\n    data = aug(image=image, mask=mask, force_apply=True)\n    assert data[\"image\"].dtype == torch.float32\n    assert data[\"mask\"].dtype == torch.float32"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_additional_targets_for_totensor",
                "second_doc": "\"\"\"\nTest that the transformation pipeline can correctly handle additional named image and mask targets, ensuring consistency between source and duplicate data. \n\nThis is done to validate that applying the same transformation to multiple correlated inputs (e.g., multiple images or masks) maintains their correspondence, which is critical for tasks requiring synchronized augmentations across related inputs.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "with pytest.warns(DeprecationWarning):\n        aug = A.Compose([ToTensor(num_classes=4)], additional_targets={\"image2\": \"image\", \"mask2\": \"mask\"})\n    for _i in range(10):\n        image1 = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n        image2 = image1.copy()\n        mask1 = np.random.randint(low=0, high=256, size=(100, 100, 4), dtype=np.uint8)\n        mask2 = mask1.copy()\n        res = aug(image=image1, image2=image2, mask=mask1, mask2=mask2)\n        assert np.array_equal(res[\"image\"], res[\"image2\"])\n        assert np.array_equal(res[\"mask\"], res[\"mask2\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_with_replaycompose",
                "second_doc": "\"\"\"\nTests that applying a deterministic sequence of transformations with replay functionality produces consistent and reproducible results, including data type preservation for images and masks.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensuring reproducible augmentations is important for debugging, experiment tracking, and guaranteeing that identical transformation pipelines yield the same outputs for given inputs.\n\"\"\"",
                "source_code": "aug = A.ReplayCompose([ToTensorV2()])\n    kwargs = {\n        \"image\": np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8),\n        \"mask\": np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8),\n    }\n    res = aug(**kwargs)\n    res2 = A.ReplayCompose.replay(res[\"replay\"], **kwargs)\n    assert np.array_equal(res[\"image\"], res2[\"image\"])\n    assert np.array_equal(res[\"mask\"], res2[\"mask\"])\n    assert res[\"image\"].dtype == torch.uint8\n    assert res[\"mask\"].dtype == torch.uint8\n    assert res2[\"image\"].dtype == torch.uint8\n    assert res2[\"mask\"].dtype == torch.uint8"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_serialization.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "test_augmentations_serialization",
                "second_doc": "\"\"\"\nTests whether serialization and deserialization of image augmentation classes preserve their transformation behavior.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested.\n    params: Dictionary of parameters to initialize the augmentation.\n    p: Probability with which the augmentation is applied.\n    seed: Random seed for deterministic transformation.\n    always_apply: Whether to always apply the augmentation.\n\nReturns:\n    None. Asserts that images and masks produced by the original and deserialized augmentations are identical.\n\nWhy:\n    Ensuring that augmentation configurations can be serialized and deserialized without altering their effect is crucial for reliable experiment reproducibility, pipeline sharing, and deployment in real-world computer vision workflows.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=p, always_apply=always_apply, **params)\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    aug_data = aug(image=image, mask=mask)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, mask=mask)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"mask\"], deserialized_aug_data[\"mask\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_augmentations_serialization_with_custom_parameters",
                "second_doc": "\"\"\"\nTest that image augmentation classes with custom parameters are correctly serialized and deserialized, ensuring that the transformation produces identical results before and after the process.\n\nThis test is important to guarantee that augmentation configurations maintain their functionality and determinism when saved and reloaded, which is essential for reproducible experiments and deployment scenarios.\n\nArgs:\n    augmentation_cls: The image augmentation class to test.\n    params: A dictionary of parameters to initialize the augmentation.\n    p: The probability of applying the augmentation.\n    seed: The random seed for reproducibility.\n    always_apply: Whether the augmentation should always be applied.\n    image: The input image for augmentation (provided from the test context).\n    mask: The corresponding mask for segmentation tasks (provided from the test context).\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the augmented images or masks produced before and after serialization differ.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=p, always_apply=always_apply, **params)\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    aug_data = aug(image=image, mask=mask)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, mask=mask)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"mask\"], deserialized_aug_data[\"mask\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_augmentations_for_bboxes_serialization",
                "second_doc": "\"\"\"\nTest that the serialization and deserialization of augmentation objects preserves deterministic transformation results for images and their bounding boxes.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested.\n    params: Parameters to initialize the augmentation.\n    p: Probability of applying the augmentation.\n    seed: Random seed to ensure determinism.\n    always_apply: Whether the augmentation is always applied.\n\nReturns:\n    None. Asserts that augmentation results are the same before and after serialization.\n\nWhy:\n    Ensures that transforming objects to and from a serializable format does not alter the behavior of augmentations, which is critical for reproducibility and consistency in data processing workflows.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=p, always_apply=always_apply, **params)\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    aug_data = aug(image=image, bboxes=albumentations_bboxes)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, bboxes=albumentations_bboxes)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"bboxes\"], deserialized_aug_data[\"bboxes\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_augmentations_for_keypoints_serialization",
                "second_doc": "\"\"\"\nTest that image augmentations with keypoints are correctly serialized and deserialized, ensuring deterministic outputs for both original and reconstructed augmentation pipelines.\n\nThis method verifies that after converting an augmentation object to a dictionary and back, applying the augmentation with the same random seed produces identical results. This is important to confirm reproducibility and consistency when saving, sharing, or deploying augmentation pipelines.\n\nArgs:\n    augmentation_cls (Type): The augmentation class to be tested.\n    params (dict): Initialization parameters for the augmentation.\n    p (float): The probability with which the augmentation is applied.\n    seed (int): The random seed to ensure deterministic augmentation results.\n    always_apply (bool): If True, the augmentation is always applied.\n    image (np.ndarray): The input image array.\n    keypoints (list): The keypoints associated with the image.\n\nReturns:\n    None. Asserts internally that the augmented images and keypoints remain unchanged after serialization and deserialization.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=p, always_apply=always_apply, **params)\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    aug_data = aug(image=image, keypoints=keypoints)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, keypoints=keypoints)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"keypoints\"], deserialized_aug_data[\"keypoints\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_imgaug_augmentations_serialization",
                "second_doc": "\"\"\"\nTest that serialization and deserialization of various image augmentations preserve the exact outcome of transformations.\n\nThis ensures that augmentation pipelines can be reliably saved and restored, maintaining consistent results.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested for correct serialization.\n    params: Dictionary of parameters to initialize the augmentation class.\n    p: The probability value controlling the application of the augmentation.\n    seed: The random seed used to ensure deterministic output.\n    always_apply: Boolean flag to force augmentation application regardless of probability.\n    image: Input image to be augmented.\n    mask: Corresponding mask to be augmented.\n\nReturns:\n    None. Raises assertion errors if the original and deserialized augmentation transformations do not produce identical images and masks.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=p, always_apply=always_apply, **params)\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    ia.seed(seed)\n    aug_data = aug(image=image, mask=mask)\n    random.seed(seed)\n    ia.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, mask=mask)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"mask\"], deserialized_aug_data[\"mask\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_imgaug_augmentations_for_bboxes_serialization",
                "second_doc": "\"\"\"\nTests that the serialization and deserialization of various imgaug-based augmentations produce consistent and reproducible outputs for both images and bounding boxes. This ensures that after storing and restoring augmentation settings, their deterministic application remains intact. By verifying the equivalence of augmented data before and after the serialization cycle, the method guarantees reliability in storing and transferring augmentation pipelines, which is crucial for reproducibility in data processing workflows.\n\nArgs:\n    augmentation_cls: The augmentation class to test, parameterized via pytest.\n    params: Dictionary of specific parameters for the augmentation class.\n    p: Probability of applying the augmentation (float).\n    seed: Random seed for reproducibility (int).\n    always_apply: Whether to always apply the augmentation (bool).\n\nReturns:\n    None. Asserts internally that the augmentation outputs for images and bounding boxes remain unchanged after serialization and deserialization.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=p, always_apply=always_apply, **params)\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    ia.seed(seed)\n    aug_data = aug(image=image, bboxes=albumentations_bboxes)\n    random.seed(seed)\n    ia.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, bboxes=albumentations_bboxes)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"bboxes\"], deserialized_aug_data[\"bboxes\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_imgaug_augmentations_for_keypoints_serialization",
                "second_doc": "\"\"\"\nTest that imgaug-based keypoint augmentation classes can be serialized and deserialized without altering their effect on images and keypoints. By ensuring consistent results before and after (de)serialization, the test validates the reliability of storing and loading augmentation configurations, which is important for reproducibility and persistence of data processing pipelines.\n\nArgs:\n    augmentation_cls: The augmentation class under test, derived from imgaug-based augmentations.\n    params: Dictionary of parameters to initialize the augmentation.\n    p: Probability for applying the augmentation.\n    seed: Random seed to ensure deterministic behavior.\n    always_apply: Boolean flag indicating whether to always apply the augmentation.\n\nReturns:\n    None. The test asserts that applying the augmentation before and after serialization yields identical results for images and keypoints.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=p, always_apply=always_apply, **params)\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    ia.seed(seed)\n    aug_data = aug(image=image, keypoints=keypoints)\n    random.seed(seed)\n    ia.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, keypoints=keypoints)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"keypoints\"], deserialized_aug_data[\"keypoints\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_image_only_crop_around_bbox_augmentation_serialization",
                "second_doc": "\"\"\"\nTests that the RandomCropNearBBox augmentation produces identical outputs before and after serialization and deserialization, ensuring transform consistency when saving and loading configurations. This confirms the reliability and reproducibility of augmentation pipelines.\n\nArgs:\n    p (float): The probability of applying the augmentation.\n    always_apply (bool): Whether to always apply the augmentation, regardless of probability.\n    seed (int): Random seed used to ensure deterministic results.\n\nReturns:\n    None: The function asserts correctness and does not return a value directly.\n\"\"\"",
                "source_code": "aug = A.RandomCropNearBBox(p=p, always_apply=always_apply, max_part_shift=0.15)\n    annotations = {\"image\": image, \"cropping_bbox\": [-59, 77, 177, 231]}\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    aug_data = aug(**annotations)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(**annotations)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_from_float_serialization",
                "second_doc": "\"\"\"\nTests that an augmentation pipeline remains functionally equivalent after being serialized and deserialized, ensuring consistent image transformation results.\n\nArgs:\n    float_image (np.ndarray): An input image represented as a NumPy array with floating point values.\n\nReturns:\n    None. Asserts internally that the transformed images from the original and deserialized augmentations are identical.\n\"\"\"",
                "source_code": "aug = A.FromFloat(p=1, dtype=\"uint8\")\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    aug_data = aug(image=float_image)\n    deserialized_aug_data = deserialized_aug(image=float_image)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_transform_pipeline_serialization",
                "second_doc": "\"\"\"\nTest that a composed image augmentation pipeline can be accurately serialized to a dictionary and deserialized back, ensuring that the transformation applied to images and masks remains deterministic given the same input and random seed.\n\nArgs:\n    seed (int): The random seed ensuring deterministic behavior in augmentation.\n    \nReturns:\n    None. Asserts that the images and masks transformed by the original and deserialized pipelines are identical.\n\"\"\"",
                "source_code": "aug = A.Compose(\n        [\n            A.OneOrOther(\n                A.Compose(\n                    [\n                        A.Resize(1024, 1024),\n                        A.RandomSizedCrop(min_max_height=(256, 1024), height=512, width=512, p=1),\n                        A.OneOf(\n                            [\n                                A.RandomSizedCrop(min_max_height=(256, 512), height=384, width=384, p=0.5),\n                                A.RandomSizedCrop(min_max_height=(256, 512), height=512, width=512, p=0.5),\n                            ]\n                        ),\n                    ]\n                ),\n                A.Compose(\n                    [\n                        A.Resize(1024, 1024),\n                        A.RandomSizedCrop(min_max_height=(256, 1025), height=256, width=256, p=1),\n                        A.OneOf([A.HueSaturationValue(p=0.5), A.RGBShift(p=0.7)], p=1),\n                    ]\n                ),\n            ),\n            A.HorizontalFlip(p=1),\n            A.RandomBrightnessContrast(p=0.5),\n        ]\n    )\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    aug_data = aug(image=image, mask=mask)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, mask=mask)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"mask\"], deserialized_aug_data[\"mask\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_transform_pipeline_serialization_with_bboxes",
                "second_doc": "\"\"\"\nTests that a composed image augmentation pipeline involving various transformations and bounding box formats can be correctly serialized and deserialized, ensuring that both transformations and the handling of bounding boxes and labels remain consistent after the process. This verification is crucial to guarantee that saving and loading augmentation configurations does not alter transformation results, which is important for reproducibility and deployment.\n\nArgs:\n    bboxes (list): List of bounding boxes in a specific format to be tested.\n    bbox_format (str): Format of the bounding boxes ('coco', 'pascal_voc', 'yolo').\n    labels (list): List of label values associated with each bounding box.\n    seed (int): Random seed to ensure reproducibility of augmentation results.\n\nReturns:\n    None. Asserts that the outputs (images and bounding boxes) from the original and deserialized pipelines are identical.\n\"\"\"",
                "source_code": "aug = A.Compose(\n        [\n            A.OneOrOther(\n                A.Compose([A.RandomRotate90(), A.OneOf([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5)])]),\n                A.Compose([A.Rotate(p=0.5), A.OneOf([A.HueSaturationValue(p=0.5), A.RGBShift(p=0.7)], p=1)]),\n            ),\n            A.HorizontalFlip(p=1),\n            A.RandomBrightnessContrast(p=0.5),\n        ],\n        bbox_params={\"format\": bbox_format, \"label_fields\": [\"labels\"]},\n    )\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    aug_data = aug(image=image, bboxes=bboxes, labels=labels)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, bboxes=bboxes, labels=labels)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"bboxes\"], deserialized_aug_data[\"bboxes\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_transform_pipeline_serialization_with_keypoints",
                "second_doc": "\"\"\"\nTests that the transform pipeline can be serialized and deserialized correctly when handling images with keypoints and associated labels, ensuring deterministic behavior and data consistency after augmentation.\n\nArgs:\n    keypoints (list of tuples): The coordinates of keypoints to be transformed.\n    keypoint_format (str): The format in which keypoints are represented (e.g., \"xy\", \"yx\", etc.).\n    labels (list): The labels corresponding to the keypoints.\n    seed (int): The random seed to control the deterministic output of augmentations.\n\nReturns:\n    None\n\nWhy:\n    Verifying the integrity of the serialization and deserialization process for complex augmentation pipelines is crucial to ensure reproducibility, correct behavior, and compatibility when saving, loading, or sharing transformation configurations.\n\"\"\"",
                "source_code": "aug = A.Compose(\n        [\n            A.OneOrOther(\n                A.Compose([A.RandomRotate90(), A.OneOf([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5)])]),\n                A.Compose([A.Rotate(p=0.5), A.OneOf([A.HueSaturationValue(p=0.5), A.RGBShift(p=0.7)], p=1)]),\n            ),\n            A.HorizontalFlip(p=1),\n            A.RandomBrightnessContrast(p=0.5),\n        ],\n        keypoint_params={\"format\": keypoint_format, \"label_fields\": [\"labels\"]},\n    )\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    aug_data = aug(image=image, keypoints=keypoints, labels=labels)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, keypoints=keypoints, labels=labels)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"keypoints\"], deserialized_aug_data[\"keypoints\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_additional_targets_for_image_only_serialization",
                "second_doc": "\"\"\"\nTest that serialization and deserialization of composed image-only augmentations with additional image targets maintain consistent augmentation results.\n\nArgs:\n    augmentation_cls: The augmentation class to apply (from the tested list).\n    params: Dictionary of parameters for the augmentation.\n    seed: Random seed for ensuring deterministic behavior.\n\nReturns:\n    None. Asserts internally that augmented outputs on both primary and additional image targets are identical before and after pipeline serialization.\n\nWhy:\n    Ensures the reliability and reproducibility of augmentation pipelines by verifying that transformation behavior remains unchanged after saving and loading the augmentation configuration, even when applied to multiple image inputs.\n\"\"\"",
                "source_code": "aug = A.Compose([augmentation_cls(always_apply=True, **params)], additional_targets={\"image2\": \"image\"})\n    image2 = image.copy()\n\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug)\n    random.seed(seed)\n    aug_data = aug(image=image, image2=image2)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, image2=image2)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"image2\"], deserialized_aug_data[\"image2\"])"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_lambda_serialization",
                "second_doc": "\"\"\"\nTests the consistency of a custom lambda-based vertical flip transform after serialization and deserialization. This ensures that custom augmentation logic remains functionally identical following conversion to and from a serializable representation, which is critical for reproducibility and pipeline deployment.\n\nArgs:\n    seed (int): The random seed to control transformation randomness for consistency checks.\n    p (int): The probability of applying the vertical flip transformation.\n\nReturns:\n    None. Asserts internally that the outputs from the original and deserialized transforms are equal for images, masks, bounding boxes, and keypoints.\n\"\"\"",
                "source_code": "def vflip_image(image, **kwargs):\n        return F.vflip(image)\n\n    def vflip_mask(mask, **kwargs):\n        return F.vflip(mask)\n\n    def vflip_bbox(bbox, **kwargs):\n        return F.bbox_vflip(bbox, **kwargs)\n\n    def vflip_keypoint(keypoint, **kwargs):\n        return F.keypoint_vflip(keypoint, **kwargs)\n\n    aug = A.Lambda(name=\"vflip\", image=vflip_image, mask=vflip_mask, bbox=vflip_bbox, keypoint=vflip_keypoint, p=p)\n\n    serialized_aug = A.to_dict(aug)\n    deserialized_aug = A.from_dict(serialized_aug, lambda_transforms={\"vflip\": aug})\n    random.seed(seed)\n    aug_data = aug(image=image, mask=mask, bboxes=albumentations_bboxes, keypoints=keypoints)\n    random.seed(seed)\n    deserialized_aug_data = deserialized_aug(image=image, mask=mask, bboxes=albumentations_bboxes, keypoints=keypoints)\n    assert np.array_equal(aug_data[\"image\"], deserialized_aug_data[\"image\"])\n    assert np.array_equal(aug_data[\"mask\"], deserialized_aug_data[\"mask\"])\n    assert np.array_equal(aug_data[\"bboxes\"], deserialized_aug_data[\"bboxes\"])\n    assert np.array_equal(aug_data[\"keypoints\"], deserialized_aug_data[\"keypoints\"])"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/test_transforms.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "test_transpose_both_image_and_mask",
                "second_doc": "\"\"\"\nTests that applying the Transpose augmentation correctly swaps the spatial dimensions of both the image and its corresponding mask, ensuring spatial consistency and correct shape handling for joint transformations.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Verifies that spatial augmentations applied to both images and masks maintain their alignment and expected dimensionality, which is critical for tasks that require paired data such as semantic segmentation.\n\"\"\"",
                "source_code": "image = np.ones((8, 6, 3))\n    mask = np.ones((8, 6))\n    augmentation = A.Transpose(p=1)\n    augmented = augmentation(image=image, mask=mask)\n    assert augmented[\"image\"].shape == (6, 8, 3)\n    assert augmented[\"mask\"].shape == (6, 8)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_rotate_interpolation",
                "second_doc": "\"\"\"\nTests whether rotating an image and its corresponding mask using different interpolation modes produces correct and consistent results. This ensures that the rotation transformation handles both images and segmentation masks appropriately across supported interpolation strategies.\n\nArgs:\n    interpolation (int): OpenCV interpolation flag specifying the interpolation method to be used for rotating the image.\n\nReturns:\n    None. The function asserts the equality between the output of the pipeline and the expected results, raising an assertion error if a mismatch is found.\n\"\"\"",
                "source_code": "image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    mask = np.random.randint(low=0, high=2, size=(100, 100), dtype=np.uint8)\n    aug = A.Rotate(limit=(45, 45), interpolation=interpolation, p=1)\n    data = aug(image=image, mask=mask)\n    expected_image = F.rotate(image, 45, interpolation=interpolation, border_mode=cv2.BORDER_REFLECT_101)\n    expected_mask = F.rotate(mask, 45, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_REFLECT_101)\n    assert np.array_equal(data[\"image\"], expected_image)\n    assert np.array_equal(data[\"mask\"], expected_mask)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_shift_scale_rotate_interpolation",
                "second_doc": "\"\"\"\nTest that the ShiftScaleRotate augmentation produces the correct transformations for both images and masks with various interpolation methods.\n\nArgs:\n    interpolation (int): Interpolation method to be applied during the transformation. Tested with nearest, linear, and cubic interpolation modes.\n\nReturns:\n    None\n\nWhy:\n    Verifies that geometric augmentations are applied consistently and correctly, ensuring data integrity for both images and segmentation masks when using different interpolation strategies.\n\"\"\"",
                "source_code": "image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    mask = np.random.randint(low=0, high=2, size=(100, 100), dtype=np.uint8)\n    aug = A.ShiftScaleRotate(\n        shift_limit=(0.2, 0.2), scale_limit=(1.1, 1.1), rotate_limit=(45, 45), interpolation=interpolation, p=1\n    )\n    data = aug(image=image, mask=mask)\n    expected_image = F.shift_scale_rotate(\n        image, angle=45, scale=2.1, dx=0.2, dy=0.2, interpolation=interpolation, border_mode=cv2.BORDER_REFLECT_101\n    )\n    expected_mask = F.shift_scale_rotate(\n        mask, angle=45, scale=2.1, dx=0.2, dy=0.2, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_REFLECT_101\n    )\n    assert np.array_equal(data[\"image\"], expected_image)\n    assert np.array_equal(data[\"mask\"], expected_mask)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_optical_distortion_interpolation",
                "second_doc": "\"\"\"\nTest that applying the OpticalDistortion transformation with different interpolation methods produces consistent and correct results for both images and masks.\n\nArgs:\n    interpolation (int): OpenCV interpolation flag specifying the interpolation method to use. One of cv2.INTER_NEAREST, cv2.INTER_LINEAR, or cv2.INTER_CUBIC.\n\nReturns:\n    None\n\nWhy:\n    Ensures that the transformation correctly applies the specified interpolation method to images while handling masks appropriately, maintaining accuracy and reliability in preprocessing pipelines.\n\"\"\"",
                "source_code": "image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    mask = np.random.randint(low=0, high=2, size=(100, 100), dtype=np.uint8)\n    aug = A.OpticalDistortion(distort_limit=(0.05, 0.05), shift_limit=(0, 0), interpolation=interpolation, p=1)\n    data = aug(image=image, mask=mask)\n    expected_image = F.optical_distortion(\n        image, k=0.05, dx=0, dy=0, interpolation=interpolation, border_mode=cv2.BORDER_REFLECT_101\n    )\n    expected_mask = F.optical_distortion(\n        mask, k=0.05, dx=0, dy=0, interpolation=cv2.INTER_NEAREST, border_mode=cv2.BORDER_REFLECT_101\n    )\n    assert np.array_equal(data[\"image\"], expected_image)\n    assert np.array_equal(data[\"mask\"], expected_mask)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_grid_distortion_interpolation",
                "second_doc": "\"\"\"\nTest that grid distortion transformation applies the correct interpolation method and produces consistent results for both images and masks.\n\nArgs:\n    interpolation (int): Interpolation flag specifying the interpolation algorithm to use for resampling (e.g., nearest, linear, cubic).\n\nReturns:\n    None\n\nWhy:\n    Verifies that the grid distortion augmentation produces identical results to the functional implementation, ensuring accurate, predictable, and interchangeable behavior required for reliable preprocessing in image-based experiments.\n\"\"\"",
                "source_code": "image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    mask = np.random.randint(low=0, high=2, size=(100, 100), dtype=np.uint8)\n    aug = A.GridDistortion(num_steps=1, distort_limit=(0.3, 0.3), interpolation=interpolation, p=1)\n    data = aug(image=image, mask=mask)\n    expected_image = F.grid_distortion(\n        image, num_steps=1, xsteps=[1.3], ysteps=[1.3], interpolation=interpolation, border_mode=cv2.BORDER_REFLECT_101\n    )\n    expected_mask = F.grid_distortion(\n        mask,\n        num_steps=1,\n        xsteps=[1.3],\n        ysteps=[1.3],\n        interpolation=cv2.INTER_NEAREST,\n        border_mode=cv2.BORDER_REFLECT_101,\n    )\n    assert np.array_equal(data[\"image\"], expected_image)\n    assert np.array_equal(data[\"mask\"], expected_mask)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_elastic_transform_interpolation",
                "second_doc": "\"\"\"\nTests that the ElasticTransform augmentation produces deterministic and correct results for different interpolation modes by comparing its output to the expected elastic transformation function. This ensures that the transform handles both images and masks consistently, which is critical for reliable preprocessing in vision pipelines.\n\nArgs:\n    interpolation (int): The interpolation method used in the elastic transform (e.g., cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC).\n    monkeypatch: pytest fixture for patching attributes during the test.\n\nReturns:\n    None. Asserts that the outputs match expected results.\n\"\"\"",
                "source_code": "image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    mask = np.random.randint(low=0, high=2, size=(100, 100), dtype=np.uint8)\n    monkeypatch.setattr(\n        \"albumentations.augmentations.transforms.ElasticTransform.get_params\", lambda *_: {\"random_state\": 1111}\n    )\n    aug = A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, interpolation=interpolation, p=1)\n    data = aug(image=image, mask=mask)\n    expected_image = F.elastic_transform(\n        image,\n        alpha=1,\n        sigma=50,\n        alpha_affine=50,\n        interpolation=interpolation,\n        border_mode=cv2.BORDER_REFLECT_101,\n        random_state=np.random.RandomState(1111),\n    )\n    expected_mask = F.elastic_transform(\n        mask,\n        alpha=1,\n        sigma=50,\n        alpha_affine=50,\n        interpolation=cv2.INTER_NEAREST,\n        border_mode=cv2.BORDER_REFLECT_101,\n        random_state=np.random.RandomState(1111),\n    )\n    assert np.array_equal(data[\"image\"], expected_image)\n    assert np.array_equal(data[\"mask\"], expected_mask)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Checks whether transformations based on DualTransform does not introduce a mask interpolation artifacts\"\"\"",
                "first_doc": "\"\"\"\nTests that binary masks remain binary after applying the specified augmentation.\n\nArgs:\n    augmentation_cls: The augmentation class to apply to the image and mask.\n    params: Additional keyword arguments to initialize the augmentation class.\n\nReturns:\n    None: This method asserts that the augmented mask contains only binary values (0 and 1), ensuring correct interpolation during augmentation.\n\"\"\"",
                "method_name": "test_binary_mask_interpolation",
                "second_doc": "\"\"\"\nValidates that applying geometric and spatial augmentations to binary segmentation masks preserves their binary nature.\n\nThis check ensures that the augmentation algorithms do not introduce unintended intermediate values, which is essential for maintaining the integrity of segmentation data during preprocessing and model training.\n\nArgs:\n    augmentation_cls: The augmentation class to apply, which defines a specific transformation.\n    params: Dictionary of keyword arguments used to configure the augmentation class.\n\nReturns:\n    None: The method asserts that unique values in the output mask are only 0 and 1, guaranteeing that the transformed mask remains strictly binary.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n    image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    mask = np.random.randint(low=0, high=2, size=(100, 100), dtype=np.uint8)\n    data = aug(image=image, mask=mask)\n    assert np.array_equal(np.unique(data[\"mask\"]), np.array([0, 1]))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Checks whether transformations based on DualTransform does not introduce a mask interpolation artifacts.\n    Note: IAAAffine, IAAPiecewiseAffine, IAAPerspective does not properly operate if mask has values other than {0;1}\n    \"\"\"",
                "first_doc": "\"\"\"\nTests semantic mask interpolation for various image augmentation classes.\n\nThis method creates a random image and segmentation mask, applies a specified augmentation transformation, and verifies that the unique values in the transformed mask are preserved and correspond to the expected set. The method is designed to ensure that label interpolation from semantic segmentation masks works correctly after augmentation, i.e., only valid class labels remain after transformation.\n\nArgs:\n    augmentation_cls: The augmentation class to be instantiated and tested. This is the class implementing the transformation (e.g., ElasticTransform, GridDistortion).\n    params: A dictionary of parameters to be passed to the augmentation class upon instantiation. These control the behavior of the transformation (e.g., rotation limits, scale limits, output sizes).\n\nReturns:\n    None. The method asserts that the transformed mask contains only the valid label values and does not return a value.\n\"\"\"",
                "method_name": "test_semantic_mask_interpolation",
                "second_doc": "\"\"\"\nValidates that label values in semantic segmentation masks remain unchanged and valid after applying various augmentation transformations.\n\nThis method generates a random synthetic image and corresponding semantic mask, applies a specific augmentation, and verifies that the set of unique mask values remains consistent and valid post-transformation. This safeguards against unwanted class label mixing or interpolation artifacts during augmentation, which is critical for maintaining the integrity of ground truth annotations in segmentation tasks.\n\nArgs:\n    augmentation_cls: The augmentation class to instantiate and apply. Examples include geometric or elastic transformations.\n    params: Dictionary of keyword arguments to configure the augmentation class, specifying transformation parameters such as rotation angles or output sizes.\n\nReturns:\n    None. Asserts internally that the transformed mask contains only valid, predefined label values and raises an error if label integrity is violated.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n    image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    mask = np.random.randint(low=0, high=4, size=(100, 100), dtype=np.uint8) * 64\n\n    data = aug(image=image, mask=mask)\n    assert np.array_equal(np.unique(data[\"mask\"]), np.array([0, 64, 128, 192]))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "__test_multiprocessing_support_proc",
                "second_doc": "\"\"\"\nApplies the specified transformation function to the given image. This method is used to verify that image transformations work correctly when executed in a multiprocessing environment.\n\nArgs:\n    args (tuple): A tuple containing the image and a transformation function.\n\nReturns:\n    dict: The result of applying the transformation function to the image.\n\"\"\"",
                "source_code": "x, transform = args\n    return transform(image=x)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Checks whether we can use augmentations in multi-threaded environments\"\"\"",
                "first_doc": "\"\"\"\nTests multiprocessing support for a given augmentation class by applying the augmentation to multiple images in parallel.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested for multiprocessing support.\n    params: A dictionary of keyword arguments to be passed to the augmentation class constructor.\n\nReturns:\n    None: This method does not return a value. It runs multiprocessing tests to ensure correct augmentation behavior when using multiple processes.\n\"\"\"",
                "method_name": "test_multiprocessing_support",
                "second_doc": "\"\"\"\nEvaluates whether a given image augmentation class can be safely and consistently applied to multiple images in parallel using Python's multiprocessing. This ensures that the augmentation implementation produces correct and reliable results when utilized in concurrent data processing environments, which is important for scaling up training workflows.\n\nArgs:\n    augmentation_cls: The image augmentation class to be tested for multiprocessing compatibility.\n    params: A dictionary containing keyword arguments used to initialize the augmentation class.\n\nReturns:\n    None: This function performs multiprocessing tests but does not return a value.\n\"\"\"",
                "source_code": "aug = augmentation_cls(p=1, **params)\n    image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n\n    pool = Pool(8)\n    pool.map(__test_multiprocessing_support_proc, map(lambda x: (x, aug), [image] * 100))\n    pool.close()\n    pool.join()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Unit test for https://github.com/albu/albumentations/issues/189\n    \"\"\"",
                "first_doc": "\"\"\"\nTests the augmentation pipeline to ensure output image dimensions fall within expected sizes.\n\nThis method constructs and applies a complex data augmentation pipeline to a dummy image. It then verifies that the resulting image dimensions are among the allowed output sizes (256, 384, 512 for both height and width).\n\nReturns:\n    None: This method does not return a value.\n\"\"\"",
                "method_name": "test_force_apply",
                "second_doc": "\"\"\"\nValidates that a composed image augmentation pipeline reliably produces outputs constrained to specific dimensions.\n\nThis method constructs and executes a diverse augmentation sequence on a synthetic image, then checks if the resulting image shape is limited to predetermined valid sizes. This verification ensures that complex or randomized transformation pipelines do not result in unintended output dimensions, which could disrupt subsequent processing steps or model compatibility.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "aug = A.Compose(\n        [\n            A.OneOrOther(\n                A.Compose(\n                    [\n                        A.RandomSizedCrop(min_max_height=(256, 1025), height=512, width=512, p=1),\n                        A.OneOf(\n                            [\n                                A.RandomSizedCrop(min_max_height=(256, 512), height=384, width=384, p=0.5),\n                                A.RandomSizedCrop(min_max_height=(256, 512), height=512, width=512, p=0.5),\n                            ]\n                        ),\n                    ]\n                ),\n                A.Compose(\n                    [\n                        A.RandomSizedCrop(min_max_height=(256, 1025), height=256, width=256, p=1),\n                        A.OneOf([A.HueSaturationValue(p=0.5), A.RGBShift(p=0.7)], p=1),\n                    ]\n                ),\n            ),\n            A.HorizontalFlip(p=1),\n            A.RandomBrightnessContrast(p=0.5),\n        ]\n    )\n\n    res = aug(image=np.zeros((1248, 1248, 3), dtype=np.uint8))\n    assert res[\"image\"].shape[0] in (256, 384, 512)\n    assert res[\"image\"].shape[1] in (256, 384, 512)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_additional_targets_for_image_only",
                "second_doc": "\"\"\"\nTest that single-image-only augmentations are applied identically to all image inputs when specified as additional targets.\n\nArgs:\n    augmentation_cls: The augmentation class to be tested, which operates only on image data.\n    params: Dictionary of parameters for the augmentation class.\n\nReturns:\n    None. Asserts that the output of the augmentation is identical for both primary and additional image inputs.\n\nWhy:\n    Ensures consistent augmentation behavior across multiple image keys, which is critical when applying deterministic transforms to paired image data or modalities.\n\"\"\"",
                "source_code": "aug = A.Compose([augmentation_cls(always_apply=True, **params)], additional_targets={\"image2\": \"image\"})\n    for _i in range(10):\n        image1 = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n        image2 = image1.copy()\n        res = aug(image=image1, image2=image2)\n        aug1 = res[\"image\"]\n        aug2 = res[\"image2\"]\n        assert np.array_equal(aug1, aug2)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_lambda_transform",
                "second_doc": "\"\"\"\nTest the application of custom per-target lambda transformations on multiple types of input data, including images, masks, bounding boxes, and keypoints, ensuring that each transformation correctly alters its respective input.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This method verifies that user-defined per-target transformations can be applied independently and correctly to different data components, supporting flexible and complex augmentation pipelines by allowing distinct operations on images, masks, bounding boxes, and keypoints.\n\"\"\"",
                "source_code": "def negate_image(image, **kwargs):\n        return -image\n\n    def one_hot_mask(mask, num_channels, **kwargs):\n        new_mask = np.eye(num_channels, dtype=np.uint8)[mask]\n        return new_mask\n\n    def vflip_bbox(bbox, **kwargs):\n        return F.bbox_vflip(bbox, **kwargs)\n\n    def vflip_keypoint(keypoint, **kwargs):\n        return F.keypoint_vflip(keypoint, **kwargs)\n\n    aug = A.Lambda(\n        image=negate_image, mask=partial(one_hot_mask, num_channels=16), bbox=vflip_bbox, keypoint=vflip_keypoint, p=1\n    )\n\n    output = aug(\n        image=np.ones((10, 10, 3), dtype=np.float32),\n        mask=np.tile(np.arange(0, 10), (10, 1)),\n        bboxes=[(10, 15, 25, 35)],\n        keypoints=[(20, 30, 40, 50)],\n    )\n    assert (output[\"image\"] < 0).all()\n    assert output[\"mask\"].shape[2] == 16  # num_channels\n    assert output[\"bboxes\"] == [F.bbox_vflip((10, 15, 25, 35), 10, 10)]\n    assert output[\"keypoints\"] == [F.keypoint_vflip((20, 30, 40, 50), 10, 10)]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_channel_droput",
                "second_doc": "\"\"\"\nTests whether the ChannelDropout augmentation correctly removes the specified number of channels from an image, ensuring that only the expected number of channels retain nonzero values.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This method is verifying the correctness of the channel dropout operation to ensure that image augmentation processes behave as expected, helping maintain data integrity during preprocessing.\n\"\"\"",
                "source_code": "img = np.ones((10, 10, 3), dtype=np.float32)\n\n    aug = A.ChannelDropout(channel_drop_range=(1, 1), always_apply=True)  # Drop one channel\n\n    transformed = aug(image=img)[\"image\"]\n\n    assert sum([transformed[:, :, c].max() for c in range(img.shape[2])]) == 2\n\n    aug = A.ChannelDropout(channel_drop_range=(2, 2), always_apply=True)  # Drop two channels\n    transformed = aug(image=img)[\"image\"]\n\n    assert sum([transformed[:, :, c].max() for c in range(img.shape[2])]) == 1"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_equalize",
                "second_doc": "\"\"\"\nTest that the Equalize transformation produces consistent and correct results by comparing its output with the reference implementation, including support for masks and mask functions.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Ensures that the Equalize transformation behaves as expected and is compatible with various mask input formats, thereby verifying the correctness and reliability of the transformation in different scenarios.\n\"\"\"",
                "source_code": "aug = A.Equalize(p=1)\n\n    img = np.random.randint(0, 256, 256 * 256 * 3, np.uint8).reshape((256, 256, 3))\n    a = aug(image=img)[\"image\"]\n    b = F.equalize(img)\n    assert np.all(a == b)\n\n    mask = np.random.randint(0, 2, 256 * 256, np.uint8).reshape((256, 256))\n    aug = A.Equalize(mask=mask, p=1)\n    a = aug(image=img)[\"image\"]\n    b = F.equalize(img, mask=mask)\n    assert np.all(a == b)\n\n    def mask_func(image, test):\n        return mask\n\n    aug = A.Equalize(mask=mask_func, mask_params=[\"test\"], p=1)\n    assert np.all(aug(image=img, test=mask)[\"image\"] == F.equalize(img, mask=mask))"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_crop_non_empty_mask",
                "second_doc": "\"\"\"\nTest cropping operations to ensure that regions containing non-empty mask values are correctly extracted or handled under different scenarios, such as the presence of ignored values or channels, empty masks, and different crop sizes.\n\nThis is necessary to validate the correct behavior of augmentation transformations that operate on images and masks, guaranteeing consistency and integrity during preprocessing for downstream computer vision tasks.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                "source_code": "def _test_crop(mask, crop, aug, n=1):\n        for _ in range(n):\n            augmented = aug(image=mask, mask=mask)\n            np.testing.assert_array_equal(augmented[\"image\"], crop)\n            np.testing.assert_array_equal(augmented[\"mask\"], crop)\n\n    # test general case\n    mask_1 = np.zeros([10, 10])\n    mask_1[0, 0] = 1\n    crop_1 = np.array([[1]])\n    aug_1 = A.CropNonEmptyMaskIfExists(1, 1)\n\n    # test empty mask\n    mask_2 = np.zeros([10, 10])\n    crop_2 = np.array([[0]])\n    aug_2 = A.CropNonEmptyMaskIfExists(1, 1)\n\n    # test ignore values\n    mask_3 = np.ones([2, 2])\n    mask_3[0, 0] = 2\n    crop_3 = np.array([[2]])\n    aug_3 = A.CropNonEmptyMaskIfExists(1, 1, ignore_values=[1])\n\n    # test ignore channels\n    mask_4 = np.zeros([2, 2, 2])\n    mask_4[0, 0, 0] = 1\n    mask_4[1, 1, 1] = 2\n    crop_4 = np.array([[[1, 0]]])\n    aug_4 = A.CropNonEmptyMaskIfExists(1, 1, ignore_channels=[1])\n\n    # test full size crop\n    mask_5 = np.random.random([10, 10, 3])\n    crop_5 = mask_5\n    aug_5 = A.CropNonEmptyMaskIfExists(10, 10)\n\n    mask_6 = np.zeros([10, 10, 3])\n    mask_6[0, 0, 0] = 0\n    crop_6 = mask_6\n    aug_6 = A.CropNonEmptyMaskIfExists(10, 10, ignore_values=[1])\n\n    _test_crop(mask_1, crop_1, aug_1, n=1)\n    _test_crop(mask_2, crop_2, aug_2, n=1)\n    _test_crop(mask_3, crop_3, aug_3, n=5)\n    _test_crop(mask_4, crop_4, aug_4, n=5)\n    _test_crop(mask_5, crop_5, aug_5, n=1)\n    _test_crop(mask_6, crop_6, aug_6, n=10)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_downscale",
                "second_doc": "\"\"\"\nTest that the downscale image augmentation produces consistent results across different interpolation methods for both float and uint8 image types.\n\nArgs:\n    interpolation (int): Interpolation method used for downscaling, parameterized over several OpenCV interpolation flags.\n\nReturns:\n    None\n\nWhy:\n    Ensures that the downscale transformation behaves as expected with different image data types and interpolation strategies, validating the reliability and consistency of the augmentation process.\n\"\"\"",
                "source_code": "img_float = np.random.rand(100, 100, 3)\n    img_uint = (img_float * 255).astype(\"uint8\")\n\n    aug = A.Downscale(scale_min=0.5, scale_max=0.5, interpolation=interpolation, always_apply=True)\n\n    for img in (img_float, img_uint):\n        transformed = aug(image=img)[\"image\"]\n        func_applied = F.downscale(img, scale=0.5, interpolation=interpolation)\n        np.testing.assert_almost_equal(transformed, func_applied)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_crop_keypoints",
                "second_doc": "\"\"\"\nTest that cropping an image correctly updates the associated keypoints, ensuring spatial consistency between the original and cropped coordinates.\n\nArgs:\n    None\n\nReturns:\n    None\n\nThis method verifies that, after applying a crop transformation, keypoints are either preserved or transformed appropriately based on their location relative to the crop area. This check is important to maintain the integrity of keypoint labels after geometric augmentations.\n\"\"\"",
                "source_code": "image = np.random.randint(0, 256, (100, 100), np.uint8)\n    keypoints = [(50, 50, 0, 0)]\n\n    aug = A.Crop(0, 0, 80, 80, p=1)\n    result = aug(image=image, keypoints=keypoints)\n    assert result[\"keypoints\"] == keypoints\n\n    aug = A.Crop(50, 50, 100, 100, p=1)\n    result = aug(image=image, keypoints=keypoints)\n    assert result[\"keypoints\"] == [(0, 0, 0, 0)]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_longest_max_size_keypoints",
                "second_doc": "\"\"\"\nTest that applying the LongestMaxSize transformation correctly rescales keypoints along with the image to ensure spatial consistency after resizing based on the image's longest side.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Verifying that keypoints are accurately transformed during scaling operations helps maintain correct annotations after image augmentation, which is essential for downstream vision tasks requiring precise localization.\n\"\"\"",
                "source_code": "img = np.random.randint(0, 256, [50, 10], np.uint8)\n    keypoints = [(9, 5, 0, 0)]\n\n    aug = A.LongestMaxSize(max_size=100, p=1)\n    result = aug(image=img, keypoints=keypoints)\n    assert result[\"keypoints\"] == [(18, 10, 0, 0)]\n\n    aug = A.LongestMaxSize(max_size=5, p=1)\n    result = aug(image=img, keypoints=keypoints)\n    assert result[\"keypoints\"] == [(0.9, 0.5, 0, 0)]\n\n    aug = A.LongestMaxSize(max_size=50, p=1)\n    result = aug(image=img, keypoints=keypoints)\n    assert result[\"keypoints\"] == [(9, 5, 0, 0)]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_smallest_max_size_keypoints",
                "second_doc": "\"\"\"\nTest that keypoints are correctly scaled and transformed to match changes in image size when applying the SmallestMaxSize augmentation transform.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    This ensures spatial consistency between the transformed image and its keypoints, which is critical for preserving annotation accuracy during data augmentation workflows.\n\"\"\"",
                "source_code": "img = np.random.randint(0, 256, [50, 10], np.uint8)\n    keypoints = [(9, 5, 0, 0)]\n\n    aug = A.SmallestMaxSize(max_size=100, p=1)\n    result = aug(image=img, keypoints=keypoints)\n    assert result[\"keypoints\"] == [(90, 50, 0, 0)]\n\n    aug = A.SmallestMaxSize(max_size=5, p=1)\n    result = aug(image=img, keypoints=keypoints)\n    assert result[\"keypoints\"] == [(4.5, 2.5, 0, 0)]\n\n    aug = A.SmallestMaxSize(max_size=10, p=1)\n    result = aug(image=img, keypoints=keypoints)\n    assert result[\"keypoints\"] == [(9, 5, 0, 0)]"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "test_resize_keypoints",
                "second_doc": "\"\"\"\nTest the correctness of keypoint coordinates after applying resizing transformations to an image, ensuring keypoints are accurately scaled in proportion to the image dimensions.\n\nArgs:\n    None\n\nReturns:\n    None\n\nWhy:\n    Verifying that keypoints are consistently and correctly adjusted when images are resized helps maintain annotation accuracy during augmentation processes required for tasks like object detection and keypoint localization.\n\"\"\"",
                "source_code": "img = np.random.randint(0, 256, [50, 10], np.uint8)\n    keypoints = [(9, 5, 0, 0)]\n\n    aug = A.Resize(height=100, width=5, p=1)\n    result = aug(image=img, keypoints=keypoints)\n    assert result[\"keypoints\"] == [(4.5, 10, 0, 0)]\n\n    aug = A.Resize(height=50, width=10, p=1)\n    result = aug(image=img, keypoints=keypoints)\n    assert result[\"keypoints\"] == [(9, 5, 0, 0)]"
            },
            "type": "function"
        }
    ],
    "albumentations/tests/utils.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "convert_2d_to_3d",
                "second_doc": "\"\"\"\nExpands one or more 2D arrays into 3D arrays by repeating the single channel across the specified number of channels. This conversion standardizes input shapes, ensuring compatibility with image processing operations that expect channel dimensions.\n\nArgs:\n    arrays (tuple of np.ndarray): One or more 2D arrays to be converted.\n    num_channels (int): Number of channels to repeat along the third axis.\n\nReturns:\n    np.ndarray or tuple of np.ndarray: If one array is provided, returns a single 3D array; otherwise, returns a tuple of 3D arrays.\n\"\"\"",
                "source_code": "arrays = tuple(np.repeat(array[:, :, np.newaxis], repeats=num_channels, axis=2) for array in arrays)\n    if len(arrays) == 1:\n        return arrays[0]\n    return arrays"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "convert_2d_to_target_format",
                "second_doc": "\"\"\"\nConverts 2D input arrays to the specified format required for downstream processing, such as mask or image representation. This ensures compatibility with different targets in a data transformation pipeline.\n\nArgs:\n    arrays (list or np.ndarray): Input 2D array(s) to be converted.\n    target (str): Desired output type, one of \"mask\", \"image\", or \"image_4_channels\".\n\nReturns:\n    np.ndarray or list: Converted array in the target format. Returns a single array for masks, a stacked 3-channel array for images, or a 4-channel array for \"image_4_channels\".\n\nRaises:\n    ValueError: If the provided target type is unknown.\n\nWhy:\n    This method unifies various array formats as required by different stages of an image processing workflow, facilitating streamlined integration with functions expecting specific input types (e.g., masks or multi-channel images).\n\"\"\"",
                "source_code": "if target == \"mask\":\n        return arrays[0] if len(arrays) == 1 else arrays\n    elif target == \"image\":\n        return convert_2d_to_3d(arrays, num_channels=3)\n    elif target == \"image_4_channels\":\n        return convert_2d_to_3d(arrays, num_channels=4)\n    else:\n        raise ValueError(\"Unknown target {}\".format(target))"
            },
            "type": "function"
        }
    ],
    "albumentations/tools/make_transforms_docs.py": [
        {
            "methods": [],
            "name": "Targets",
            "type": "class"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "parse_args",
                "second_doc": "\"\"\"\nParses command-line arguments related to managing and checking files, providing a structured way to specify commands and input paths.\n\nArgs:\n    None\n\nReturns:\n    argparse.Namespace: An object containing the parsed command-line arguments.\n\nThis method enables users to interactively specify operations and parameters required for file processing via the command line, ensuring clear and validated input for downstream tasks.\n\"\"\"",
                "source_code": "parser = argparse.ArgumentParser()\n    subparsers = parser.add_subparsers(help=\"Commands\", dest=\"command\")\n    subparsers.add_parser(\"make\")\n    check_parser = subparsers.add_parser(\"check\")\n    check_parser.add_argument(\"filepath\", type=str, help=\"Path to a file that should be checked\")\n    return parser.parse_args()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "make_separator",
                "second_doc": "\"\"\"\nCreate a separator line string, either left-aligned or centered, for organizing visual output such as tables.\n\nArgs:\n    width (int): The total length of the separator line.\n    align_center (bool): If True, center-aligns the separator using colons; otherwise, creates a standard left-aligned line.\n\nReturns:\n    str: The formatted separator line for consistent structure in textual displays.\n    \nWhy:\n    Consistent and flexible separator lines help visually organize structured output, which aids readability and clarity in presenting data (e.g., in augmentation pipeline summaries or logs).\n\"\"\"",
                "source_code": "if align_center:\n        return \":\" + \"-\" * (width - 2) + \":\"\n    return \"-\" * width"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "get_transforms_info",
                "second_doc": "\"\"\"\nCollects available transformation classes in the Albumentations library along with the types of data each transform can be applied to, and constructs related documentation links for further reference.\n\nThis method systematically inspects all transform classes to help users understand which kinds of data (images, masks, bounding boxes, keypoints) are supported by each transform, thereby facilitating safer and more effective pipeline composition.\n\nArgs:\n    None\n\nReturns:\n    dict: A dictionary mapping transformation class names to their properties, including supported targets, a documentation link, and a flag indicating if the transform is for images only.\n\"\"\"",
                "source_code": "transforms_info = {}\n    for name, cls in inspect.getmembers(albumentations):\n        if inspect.isclass(cls) and issubclass(cls, albumentations.BasicTransform) and name not in IGNORED_CLASSES:\n\n            targets = {Targets.IMAGE}\n            if issubclass(cls, albumentations.DualTransform):\n                targets.add(Targets.MASKS)\n\n            if hasattr(cls, \"apply_to_bbox\") and cls.apply_to_bbox is not albumentations.DualTransform.apply_to_bbox:\n                targets.add(Targets.BBOXES)\n\n            if (\n                hasattr(cls, \"apply_to_keypoint\")\n                and cls.apply_to_keypoint is not albumentations.DualTransform.apply_to_keypoint\n            ):\n                targets.add(Targets.KEYPOINTS)\n\n            if issubclass(cls, albumentations.DualIAATransform):\n                targets.update({Targets.BBOXES, Targets.KEYPOINTS})\n\n            if issubclass(cls, albumentations.Lambda):\n                targets.add(Targets.MASKS)\n                targets.add(Targets.BBOXES)\n                targets.add(Targets.KEYPOINTS)\n\n            docs_link = None\n            if cls.__module__ == \"albumentations.augmentations.transforms\":\n                docs_link = TRANSFORM_NAME_WITH_LINK_TEMPLATE.format(name=name)\n            elif cls.__module__ == \"albumentations.imgaug.transforms\":\n                docs_link = IMGAUG_TRANSFORM_NAME_WITH_LINK_TEMPLATE.format(name=name)\n\n            transforms_info[name] = {\n                \"targets\": targets,\n                \"docs_link\": docs_link,\n                \"image_only\": issubclass(cls, albumentations.ImageOnlyTransform),\n            }\n    return transforms_info"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "make_transforms_targets_table",
                "second_doc": "\"\"\"\nGenerates a formatted table indicating which targets are supported by each transformation, improving clarity on feature compatibility within the augmentation framework.\n\nArgs:\n    transforms_info (dict): A dictionary where keys are transform names and values are dictionaries containing transformation metadata, such as supported targets and documentation links.\n    header (list): A list representing the header row of the table.\n    Targets (iterable): An iterable of possible targets to check for support (e.g., image, mask, keypoints).\n    make_separator (callable): A function to create table separators.\n\nReturns:\n    str: A string representation of the formatted table showing which targets are supported by each transformation.\n\"\"\"",
                "source_code": "rows = [header]\n    for transform, info in sorted(transforms_info.items(), key=lambda kv: kv[0]):\n        transform_targets = []\n        for target in Targets:\n            mark = \"\u2713\" if target in info[\"targets\"] else \"\"\n            transform_targets.append(mark)\n        row = [info[\"docs_link\"] or transform] + transform_targets\n        rows.append(row)\n\n    column_widths = [max([len(r) for r in column]) for column in zip(*rows)]\n    lines = [\n        \" | \".join(\n            \"{title: <{width}}\".format(width=width, title=title) for width, title in zip(column_widths, rows[0])\n        ),\n        \" | \".join(\n            make_separator(width, align_center=column_index > 0) for column_index, width in enumerate(column_widths)\n        ),\n    ]\n    for row in rows[1:]:\n        lines.append(\n            \" | \".join(\n                \"{column: <{width}}\".format(width=width, column=column) for width, column in zip(column_widths, row)\n            )\n        )\n    return \"\\n\".join(\"| {line} |\".format(line=line) for line in lines)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "make_transforms_targets_links",
                "second_doc": "\"\"\"\nGenerates a formatted list of documentation links for available image transformations, aiding users in quickly accessing relevant usage information.\n\nArgs:\n    transforms_info (dict): A dictionary where each key is the name of a transformation and each value is a dictionary containing metadata, including a 'docs_link' key for documentation URL.\n\nReturns:\n    str: A string containing each transformation\u2019s documentation link as a separate bullet point.\n\"\"\"",
                "source_code": "return \"\\n\".join(\n        \"- \" + info[\"docs_link\"] for transform, info in sorted(transforms_info.items(), key=lambda kv: kv[0])\n    )"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "check_docs",
                "second_doc": "\"\"\"\nChecks that the documentation for different transform types in the specified file is up to date by comparing it against the current set of available transform links.\n\nThis method verifies that the documentation reflects the latest changes in the codebase, and raises an error if discrepancies are found to prompt synchronization between implementation and documentation. Consistency in documentation ensures users have accurate information about available transformations.\n\nArgs:\n    args: Namespace-like object containing at least the 'filepath' attribute, specifying the path to the documentation file.\n\nRaises:\n    ValueError: If any expected documentation lines for image-only or dual transforms are missing from the file, indicating out-of-date documentation. The error message details the missing entries and provides instructions for updating the docs.\n\"\"\"",
                "source_code": "with open(args.filepath, \"r\", encoding=\"utf8\") as f:\n        text = f.read()\n    outdated_docs = set()\n    image_only_lines_not_in_text = []\n    dual_lines_not_in_text = []\n    for line in image_only_transforms_links.split(\"\\n\"):\n        if line not in text:\n            outdated_docs.update([\"Pixel-level\"])\n            image_only_lines_not_in_text.append(line)\n    for line in dual_transforms_table.split(\"\\n\"):\n        if line not in text:\n            dual_lines_not_in_text.append(line)\n            outdated_docs.update([\"Spatial-level\"])\n    if not outdated_docs:\n        return\n\n    raise ValueError(\n        \"Docs for the following transform types are outdated: {outdated_docs_headers}. \"\n        \"Generate new docs by executing the `python tools/{py_file} make` command \"\n        \"and paste them to {filename}.\\n\"\n        \"# Image only transforms lines not in file:\\n\"\n        \"{image_only_lines}\\n\"\n        \"# Dual transforms lines not in file:\\n\"\n        \"{dual_lines}\".format(\n            outdated_docs_headers=\", \".join(outdated_docs),\n            py_file=os.path.basename(os.path.realpath(__file__)),\n            filename=os.path.basename(filepath),\n            image_only_lines=\"\\n\".join(image_only_lines_not_in_text),\n            dual_lines=\"\\n\".join(dual_lines_not_in_text),\n        )\n    )"
            },
            "type": "function"
        }
    ]
}