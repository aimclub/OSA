{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ccf591",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from dotenv import load_dotenv\n",
    "from rich.logging import RichHandler\n",
    "\n",
    "from src.github_agent import GithubAgent\n",
    "from osa_tool.models.models import ModelHandlerFactory\n",
    "from src.settings import ConfigLoader, GitSettings\n",
    "from src.deepeval_checker import CustomLLM\n",
    "from osa_tool.utils import osa_project_root\n",
    "from struct_to_json import build_tree, tree_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3385de",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\", \"<YOUR_TOKEN>\")\n",
    "\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    datefmt=\"[%X]\",\n",
    "    handlers=[RichHandler()],\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"rich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_github_url(repo_url: str):\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, repo_url)\n",
    "    if match:\n",
    "        return match\n",
    "    else:\n",
    "        logger.error(f\"URL {repo_url} does not match expected format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dacc929",
   "metadata": {},
   "source": [
    "# Preprocess benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a32cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "readme_generator_api = \"openai\" # vsegpt\n",
    "readme_generator_url = \"https://openrouter.ai/api/v1\" \n",
    "# \"https://api.openai.com/v1\"\n",
    "# \"https://api.openai.com/v1\" # \"https://api.openai.com/v1\"  # gpt-3.5-turbo, openai/gpt-3.5-turbo\n",
    "readme_generator_model_name = 'gpt-4.1'\n",
    "# \"google/gemma-3-27b-it\" \n",
    "# \"gpt-4.1\" \"google/gemini-2.5-flash\" # \"deepseek/deepseek-chat-v3-0324\" # \"anthropic/claude-sonnet-4\" #\"gpt-4.1\" #\"anthropic/claude-3.7-sonnet\"\n",
    "\n",
    "# Model to assess readme quality\n",
    "readme_assess_model_name = \"gpt-4.1\"\n",
    "readme_assess_api = \"openai\"\n",
    "readme_assess_url = \"https://openrouter.ai/api/v1\" \n",
    "\n",
    "repo_name_to_url = dict()\n",
    "df = pd.read_csv(\"repos_upd.csv\")\n",
    "dataset_dir = Path(f\"readme_datasets_unlimited_{readme_generator_model_name.split('/')[-1]}\")\n",
    "dataset_dir.mkdir(exist_ok=True, parents=True)\n",
    "git_repo = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57344a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, row in df.iterrows():\n",
    "    repo_url, repo_name_, commit, _ = row\n",
    "    repo_name = repo_name_.split('/')[1]\n",
    "    print(f'{repo_url} {repo_name} {commit}')\n",
    "    params = parse_github_url(repo_url)\n",
    "    if params:\n",
    "        user, repo = params.groups()\n",
    "    else:\n",
    "        continue\n",
    "    full_url = f\"https://api.github.com/repos/{user}/{repo}/git/trees/{commit}?recursive=1\"\n",
    "    # repository_url = repo_url.split(\"/tree/\")[0]\n",
    "    repository_url = repo_url\n",
    "\n",
    "    repo_name_to_url[repo_name] = repository_url\n",
    "    try:\n",
    "        if not Path(dataset_dir, f\"{repo_name}_original_README.md\").exists():\n",
    "            # ORIG readme\n",
    "            github_agent = GithubAgent(repository_url)\n",
    "            github_agent.clone_repository()\n",
    "            if Path(repo_name, \"readme.md\").exists():\n",
    "                orig_readme = Path(repo_name, \"readme.md\")\n",
    "            else:\n",
    "                orig_readme = Path(repo_name, \"README.md\")\n",
    "            shutil.move(\n",
    "                orig_readme, Path(dataset_dir, f\"{repo_name}_original_README.md\")\n",
    "            )\n",
    "         # Generate structure json file\n",
    "        if not Path(dataset_dir, f\"{repo_name}_struct.json\").exists():\n",
    "            headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
    "            if GITHUB_TOKEN:\n",
    "                headers[\"Authorization\"] = f\"token {GITHUB_TOKEN}\"\n",
    "            try:\n",
    "                r = requests.get(full_url, headers=headers)\n",
    "                if r.status_code == 200:\n",
    "                    with open(\n",
    "                        f\"{dataset_dir}/{repo_name}_struct.json\",\n",
    "                        \"w\",\n",
    "                        encoding=\"utf-8\",\n",
    "                    ) as f:\n",
    "                        f.write(r.text)\n",
    "                        logger.info(\n",
    "                            f\"{dataset_dir}/{repo_name}_struct.json saved successfully\"\n",
    "                        )\n",
    "                    with open(\n",
    "                        f\"{dataset_dir}/{repo_name}_struct.json\",\n",
    "                        \"r\",\n",
    "                        encoding=\"utf-8\",\n",
    "                    ) as f:\n",
    "                        f = json.load(f)\n",
    "                    os.remove(f\"{dataset_dir}/{repo_name}_struct.json\")\n",
    "                    stop_words = [\n",
    "                        \"assets\",\n",
    "                        \"results\",\n",
    "                        \"sources\",\n",
    "                        \"packages\",\n",
    "                        \"images\",\n",
    "                        \"data\",\n",
    "                    ]\n",
    "                    paths = [\n",
    "                        entry[\"path\"]\n",
    "                        for entry in f.get(\"tree\", [])\n",
    "                        if not any(\n",
    "                            f\"/{stop}/\" in f\"/{entry['path']}/\"\n",
    "                            or entry[\"path\"].startswith(f\"{stop}/\")\n",
    "                            for stop in stop_words\n",
    "                        )\n",
    "                    ]\n",
    "                    tree = build_tree(paths)\n",
    "                    struct = tree_to_dict(tree)\n",
    "                    with open(\n",
    "                        f\"{dataset_dir}/{repo_name}_struct.json\",\n",
    "                        \"w\",\n",
    "                        encoding=\"utf-8\",\n",
    "                    ) as f:\n",
    "                        json.dump(struct, f, indent=4, ensure_ascii=False)\n",
    "                else:\n",
    "                    logger.info(f\"[{r.status_code}] Error for {full_url}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Request failed for {full_url}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error {e} occured during readme generation for {repo_name}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a04d76",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d525f",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "OPENROUTER_API_KEY = \"\"\n",
    "HF_TOKEN = \"\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENROUTER_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.settings import ArticleConfigLoader\n",
    "\n",
    "\n",
    "def load_configuration(\n",
    "    repo_url: str, api: str, model_name: str, url: str, article: Optional[str]\n",
    ") -> ConfigLoader:\n",
    "    \"\"\"\n",
    "    Loads configuration for osa_tool.\n",
    "\n",
    "    Args:\n",
    "        repo_url (str): URL of the GitHub repository.\n",
    "        api (str): LLM API service provider.\n",
    "        model_name (str): Specific LLM model to use.\n",
    "        article (Optional[str]): Link to the pdf file of the article. Can be None.\n",
    "\n",
    "    Returns:\n",
    "        config_loader: The configuration object which contains settings for osa_tool.\n",
    "    \"\"\"\n",
    "    if article is None:\n",
    "        config_loader = ConfigLoader(\n",
    "            config_dir=os.path.join(\n",
    "                osa_project_root(), \"osa_tool\", \"config\", \"standart\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        config_loader = ArticleConfigLoader(\n",
    "            config_dir=os.path.join(\n",
    "                osa_project_root(), \"osa_tool\", \"config\", \"with_article\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    config_loader.config.git = GitSettings(repository=repo_url)\n",
    "    config_loader.config.llm = config_loader.config.llm.model_copy(\n",
    "        update={\"api\": api, \"model\": model_name, \"url\": url}\n",
    "    )\n",
    "    logger.info(\"Config successfully updated and loaded\")\n",
    "    return config_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = (\n",
    "'''\n",
    "Determine whether the AI-generated Readme file (ACTUAL_OUTPUT)\n",
    "is better than the original one (EXPECTED_OUTPUT).\n",
    "ACTUAL_OUTPUT contains two fields: ’readme’, which contains the generated README itself,\n",
    "and ’repo_structure’ which is json with repository’s structure.\n",
    "Generated README’s content must be consistent with the provided repository structure.\n",
    "The ACTUAL_OUTPUT does not necessary have to be the same as EXPECTED_OUTPUT,\n",
    "Your goal is to determine which text is better, using the provided Evaluations steps.\n",
    "Readme structure does not matter much as long as it passes the evaluation steps.\n",
    "'''\n",
    ")\n",
    "config_loader = ConfigLoader(config_dir=os.path.join(\n",
    "                osa_project_root(), \"osa_tool\", \"config\", \"standart\"\n",
    "            ))\n",
    "config_loader.config.llm = config_loader.config.llm.model_copy(update={\"api\": readme_generator_api, \"model\": readme_generator_model_name, \"url\": readme_generator_url})\n",
    "baseline_readme_generator = ModelHandlerFactory.build(config_loader.config)\n",
    "\n",
    "readme_assess_model = CustomLLM(readme_assess_api, readme_assess_model_name, readme_assess_url)\n",
    "metrics_init_params = {\n",
    "    \"model\": readme_assess_model,\n",
    "    \"verbose_mode\": False,\n",
    "    \"async_mode\": False,\n",
    "}\n",
    "readme_correctness_metric = GEval(\n",
    "    name=\"Readme quality\",\n",
    "    criteria=prompt, #\"Determine quality of AI-generated README file by comparing it to the human-written one\",\n",
    "    evaluation_steps=[\n",
    "        \"Step 1: Does the provided structure of the repository address README content?\",\n",
    "        \"Step 2: Does the README provide a clear and accurate overview of the repository’s purpose?\",\n",
    "        \"Step 3: Are installation and setup instructions included and easy to follow?\",\n",
    "        \"Step 4: Are usage examples provided and do they clearly demonstrate functionality?\",\n",
    "        \"Step 5: Are dependencies or requirements listed appropriately?\",\n",
    "        \"Step 6: Is the README easy to read, well-structured, and free of confusing language?\",\n",
    "    ],\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
    "    ],\n",
    "    **metrics_init_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_FOR_GENERATION = ['gpt-4.1', 'anthropic/claude-sonnet-4', 'google/gemma-3-27b-it']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcee845",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = defaultdict(list)\n",
    "\n",
    "for model in MODELS_FOR_GENERATION:\n",
    "    model_nm = model\n",
    "    model_nm = model.split('/')[1] if '/' in model else model\n",
    "    dataset_dir = f\"readme_datasets_unlimited_{model_nm}\"\n",
    "    \n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        repo_url, repo_name_, commit, _ = row\n",
    "        repo_path = repo_name_.split('/')[1]\n",
    "        print(f'{repo_url} {repo_path} {commit}')\n",
    "        res['model'].append(model)\n",
    "        res['repo_path'].append(repo_path)\n",
    "\n",
    "        output_readmeready = f\"{dataset_dir}/{repo_path}__readmeready-{model_nm}.md\"\n",
    "        output_larch = f\"{dataset_dir}/{repo_path}__larch-{model_nm}.md\"\n",
    "\n",
    "        # try:\n",
    "        if os.path.exists(output_readmeready):\n",
    "            print(f\"Skipping.. {repo_path}\")\n",
    "           \n",
    "            res['readmeready'].append(True)\n",
    "            # continue\n",
    "        else:\n",
    "            print(f\"Not done.. {repo_path}\")\n",
    "            res['readmeready'].append(False)\n",
    "        \n",
    "        if os.path.exists(output_larch):\n",
    "            print(f\"Skipping.. {repo_path}\")\n",
    "\n",
    "            res['larch'].append(True)\n",
    "            # continue\n",
    "        else:\n",
    "            print(f\"Not done.. {repo_path}\")\n",
    "            res['larch'].append(False)\n",
    "        # res['readmeready'][model][repo_path] = 'done'\n",
    "\n",
    "        # LARCH\n",
    "        # try:\n",
    "        # except:\n",
    "            # res['larch'][model][repo_path] = 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc56c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df = pd.DataFrame.from_dict(res)\n",
    "r_df.model.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bd839",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df[r_df['model'] == 'google/gemma-3-27b-it'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f2401",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_urls = [\n",
    "    \"https://github.com/AntonOsika/gpt-engineer\",\n",
    "    \"https://github.com/THUDM/ChatGLM-6B\",\n",
    "    \"https://github.com/OpenEthan/SMSBoom\",\n",
    "    \"https://github.com/lra/mackup\",\n",
    "    \"https://github.com/chenfei-wu/TaskMatrix\",\n",
    "    \"https://github.com/hacksider/Deep-Live-Cam\",\n",
    "    \"https://github.com/google/python-fire\",\n",
    "    \"https://github.com/stitionai/devika\",\n",
    "    \"https://github.com/Pythagora-io/gpt-pilot\",\n",
    "    \"https://github.com/CorentinJ/Real-Time-Voice-Cloning\",\n",
    "    \"https://github.com/encode/httpx\",\n",
    "    \"https://github.com/lss233/kirara-ai\",\n",
    "    \"https://github.com/assafelovic/gpt-researcher\",\n",
    "    \"https://github.com/mkdocs/mkdocs\",\n",
    "    \"https://github.com/ageitgey/face_recognition\",\n",
    "    \"https://github.com/donnemartin/system-design-primer\",\n",
    "    \"https://github.com/chatanywhere/GPT_API_free\",\n",
    "    \"https://github.com/Asabeneh/30-Days-Of-Python\",\n",
    "    \"https://github.com/kaixindelele/ChatPaper\",\n",
    "    \"https://github.com/twintproject/twint\",\n",
    "    \"https://github.com/pallets/flask\",\n",
    "    \"https://github.com/charlax/professional-programming\",\n",
    "    \"https://github.com/ethereum/EIPs\",\n",
    "    \"https://github.com/xai-org/grok-1\",\n",
    "    \"https://github.com/eriklindernoren/PyTorch-GAN\",\n",
    "    \"https://github.com/public-apis/public-apis\",\n",
    "    \"https://github.com/Z4nzu/hackingtool\",\n",
    "    \"https://github.com/gto76/python-cheatsheet\",\n",
    "    \"https://github.com/danielgatis/rembg\",\n",
    "    \"https://github.com/bregman-arie/devops-exercises\",\n",
    "    \"https://github.com/Vision-CAIR/MiniGPT-4\",\n",
    "    \"https://github.com/Jack-Cherish/python-spider\",\n",
    "    \"https://github.com/openai/chatgpt-retrieval-plugin\",\n",
    "    \"https://github.com/black-forest-labs/flux\",\n",
    "    \"https://github.com/sb-ai-lab/EmotiEffLib\",\n",
    "    \"https://github.com/sb-ai-lab/Ride\",\n",
    "    \"https://github.com/hse-cs/delPezzo\",\n",
    "    \"https://github.com/deeppavlov/chatsky\",\n",
    "    \"https://github.com/deeppavlov/dialog2graph\",\n",
    "    \"https://github.com/corl-team/verl-loras\",\n",
    "    \"https://github.com/sb-ai-lab/Eco2AI\",\n",
    "    \"https://github.com/AIRI-Institute/GENA_LM\",\n",
    "    \"https://github.com/AIRI-Institute/eco4cast\",\n",
    "    \"https://github.com/Vishnu-tppr/Camouflage-AI\",\n",
    "    \"https://github.com/tbhvishal/Python-Weather-Info-App\",\n",
    "    \"https://github.com/readytensor/rt-repo-assessment\",\n",
    "    \"https://github.com/DadaNanjesha/TagGenerator\", # renamed\n",
    "    \"https://github.com/stephenombuya/Code-Contribution-Analyzer\",\n",
    "]\n",
    "\n",
    "\n",
    "def generate_latex_list(items, ordered=False, nested_level=0):\n",
    "    \"\"\"\n",
    "    Generates LaTeX list code from a list of items.\n",
    "    Items can be strings or lists of strings for nesting.\n",
    "    \"\"\"\n",
    "    indent = \"  \" * nested_level\n",
    "    list_type = \"enumerate\" if ordered else \"itemize\"\n",
    "    latex_code = f\"{indent}\\\\begin{{{list_type}}}\\n\"\n",
    "\n",
    "    for item in items:\n",
    "        if isinstance(item, list):\n",
    "            # Recursively generate nested list\n",
    "            latex_code += generate_latex_list(item, ordered, nested_level + 1)\n",
    "        else:\n",
    "            latex_code += f\"{indent}  \\\\item {item}\\n\"\n",
    "\n",
    "    latex_code += f\"{indent}\\\\end{{{list_type}}}\\n\"\n",
    "    return latex_code\n",
    "\n",
    "print(generate_latex_list(repo_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df[r_df['readmeready'] == False].groupby(['repo_path', 'model']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a407dc",
   "metadata": {},
   "source": [
    "# README Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_FOR_GENERATION = ['gpt-4.1', 'anthropic/claude-sonnet-4', 'google/gemma-3-27b-it']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc555ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.readme_generator import ReadmeGenerator\n",
    "from huggingface_hub import login\n",
    "import asyncio\n",
    "\n",
    "results = defaultdict(list)\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENROUTER_API_KEY\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"OPENAI_API_BASE\"] = 'https://openrouter.ai/api/v1'\n",
    "\n",
    "login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "# model = 'gpt-4.1'\n",
    "\n",
    "async def generate_readmes():\n",
    "    rr_done = 0\n",
    "    l_done = 0\n",
    "    for model in MODELS_FOR_GENERATION:\n",
    "        model_nm = model\n",
    "        model_nm = model.split('/')[1] if '/' in model else model\n",
    "        dataset_dir = f\"readme_datasets_unlimited_{model_nm}\"\n",
    "        \n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            repo_url, repo_name_, commit, _ = row\n",
    "            repo_path = repo_name_.split('/')[1]\n",
    "            print(f'{repo_url} {repo_path} {commit}')\n",
    "            \n",
    "\n",
    "            output_readmeready = f\"{dataset_dir}/{repo_path}__readmeready-{model_nm}.md\"\n",
    "            output_larch = f\"{dataset_dir}/{repo_path}__larch-{model_nm}.md\"\n",
    "\n",
    "            try:\n",
    "                if os.path.exists(output_readmeready):\n",
    "                    print(f\"Skipping.. {repo_path}\")\n",
    "                    rr_done += 1\n",
    "                    # res['readmeready'].append(True)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Not done.. {repo_path}\")\n",
    "                    # res['readmeready'].append(False)\n",
    "\n",
    "                ready_gen = ReadmeGenerator(\n",
    "                    tool=\"readmeready\",\n",
    "                    model=model,\n",
    "                    output_path=output_readmeready,\n",
    "                    repo_path=repo_path,\n",
    "                    repo_url=repo_url,\n",
    "                    api_key=OPENAI_API_KEY,\n",
    "                )\n",
    "                await asyncio.wait_for(ready_gen.generate(), 600.0)\n",
    "                rr_done += 1\n",
    "\n",
    "                # res['readmeready'][model][repo_path] = 'done'\n",
    "                # shutil.rmtree(f'{repo_path}/output_tmp')\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                continue\n",
    "                # res['readmeready'][model][repo_path] = 'error'\n",
    "\n",
    "\n",
    "            # LARCH\n",
    "            try:\n",
    "                if os.path.exists(output_larch):\n",
    "                    print(f\"Skipping.. {repo_path}\")\n",
    "                    l_done += 1\n",
    "                    # res['readmeready'].append(True)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Not done.. {repo_path}\")\n",
    "                    # res['readmeready'].append(False)\n",
    "\n",
    "                larch_gen = ReadmeGenerator(\n",
    "                    tool=\"larch\",\n",
    "                    model=model, \n",
    "                    output_path=output_larch,\n",
    "                    repo_path=repo_path,\n",
    "                    api_key=OPENAI_API_KEY,\n",
    "                )\n",
    "                larch_gen.generate()\n",
    "                l_done += 1\n",
    "\n",
    "                # res['larch'][model][repo_path] = 'done'\n",
    "            except:\n",
    "                # res['larch'][model][repo_path] = 'error'\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "await generate_readmes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409145e",
   "metadata": {},
   "source": [
    "### Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_in = (pd.read_csv(f'final_eval_res_2025-11-22_{model_nm}.csv'))\n",
    "\n",
    "done = list(r_in[['model', 'proj', 'repo']].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef948c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readme assess\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "dt = datetime.now()\n",
    "\n",
    "larch = 0\n",
    "final = None\n",
    "results = defaultdict(list)\n",
    "\n",
    "for model_nm in ['gpt-4.1', 'gemma-3-27b-it', 'claude-sonnet-4']:\n",
    "    dataset_dir = Path(f\"readme_datasets_unlimited_{model_nm}\")\n",
    "    l = len(list(dataset_dir.glob(\"*.md\")))\n",
    "    print(f'{model_nm} len: {l}')\n",
    "    for repo_readme in dataset_dir.glob(\"*.md\"):\n",
    "        if \"readmeready\" in str(repo_readme) or \"larch\" in str(repo_readme):\n",
    "            continue\n",
    "        repo_name = str(repo_readme).split(\"_original_README.md\")[0].split('/')[1]\n",
    "        # if repo_name != 'TagGenerator':\n",
    "        #     continue\n",
    "        # model = readme_generator_model_name\n",
    "        print(repo_name)\n",
    "        readme_gen = {}\n",
    "        struct_pth = os.path.join(dataset_dir, f\"{repo_name}_struct.json\")\n",
    "        original_readme_pth = os.path.join(dataset_dir, repo_name + \"_original_README.md\")\n",
    "        readme_larch_pth = os.path.join(dataset_dir, f'{repo_name}__larch-{model_nm}.md')\n",
    "        readme_ready_pth = os.path.join(dataset_dir, f'{repo_name}__readmeready-{model_nm}.md')\n",
    "\n",
    "        \n",
    "       \n",
    "        original_readme = open(\n",
    "            str(original_readme_pth), \"r\", encoding=\"utf8\"\n",
    "        ).read()\n",
    "        repo_struct = open(str(struct_pth), \"r\", encoding=\"utf8\").read()\n",
    "\n",
    "        # for proj in ['readme_ready', 'larch']:\n",
    "        proj = 'readmeready'\n",
    "\n",
    "        if (model_nm, proj, repo_name) in done:\n",
    "            print(f'{model_nm} {proj} {repo_name} Already done, skipping')\n",
    "            continue\n",
    "        print(f'{model_nm} {proj} {repo_name} processing..')\n",
    "\n",
    "            # if (model_nm + proj + repo_name) in list(map(lambda x: x[2] + x[3] + x[5],r.to_records())):\n",
    "            #     print(f\"Already done {model_nm + proj + repo_name}, skipping..\")\n",
    "            #     if proj == 'larch':\n",
    "            #         larch += 1\n",
    "            #         print(larch)\n",
    "            #     continue\n",
    "            # print(f'M: {model_nm + proj + repo_name}')\n",
    "            # print(f'LIST: {list(map(lambda x: x[2] + x[3] + x[5],rrr.to_records()))}')\n",
    "            # raise StopIteration()\n",
    "        # try:\n",
    "        #     if proj == 'readme_ready':\n",
    "        #         readme_gen[proj] = open(os.path.join(dataset_dir, f'{repo_name}__readmeready-{model_nm}.md'), \"r\", encoding=\"utf8\").read()\n",
    "        #     else:\n",
    "        #         readme_gen[proj] = open(os.path.join(dataset_dir, f'{repo_name}__larch-{model_nm}.md'), \"r\", encoding=\"utf8\").read()\n",
    "        # except:\n",
    "        #     print(f'Some error, {proj} {repo_name} {model_nm}')\n",
    "        #     continue\n",
    "        try:\n",
    "            readme_gen[proj] = open(os.path.join(dataset_dir, f'{repo_name}__{proj}-{model_nm}.md'), \"r\", encoding=\"utf8\").read()\n",
    "\n",
    "            prompt = {\"readme\": readme_gen[proj], \"repo_structure\": repo_struct}\n",
    "\n",
    "            test_case = LLMTestCase(\n",
    "                input=(\n",
    "                    \"Evaluate the AI-generated README file by comparing it to the human-written one\"\n",
    "                ),\n",
    "                actual_output=json.dumps(prompt),\n",
    "                expected_output=original_readme,\n",
    "            )\n",
    "\n",
    "\n",
    "            try:\n",
    "                print(f\"\\n\\n{'=' * 12} {proj.upper()} METRICS{'=' * 12}\")\n",
    "                score = readme_correctness_metric.measure(test_case)\n",
    "                print(score)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            # score = 0\n",
    "\n",
    "\n",
    "            results[\"model\"].append(model_nm)\n",
    "            results[\"proj\"].append(proj)\n",
    "            results[\"score\"].append(score)\n",
    "            results[\"repo\"].append(repo_name)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "            # print(f\"Model name: {model_nm};\\n{proj}\\nScore: {score}\")\n",
    "    # df = pd.DataFrame.from_dict(results)\n",
    "    # print(f\"Generation model: {readme_generator_model_name}; Checking model: {readme_assess_model_name}\")\n",
    "    # print(f\"OSA mean: {df['score'].mean()}; LLM mean: {df['test_case_gpt'].mean()}\")\n",
    "    # df.to_csv(f\"TESTING_readme_results_gpt_osa_prompt_extended_dataset__{readme_generator_model_name.split('/')[-1]}.csv\")\n",
    "    # print()\n",
    "\n",
    "    eval_res = pd.DataFrame.from_dict(results)\n",
    "    final = eval_res\n",
    "    eval_res.to_csv(f'eval_res_2025-11-22_{model_nm}_{dt}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a40cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ = pd.concat([r_in, final])\n",
    "final_.to_csv(f'final_eval_res_2025-11-22_{model_nm}.csv')\n",
    "final_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfc68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = final_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee774677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_res = pd.DataFrame.from_dict(results)\n",
    "# eval_res.to_csv(f'eval_res_2025-11-22_{readme_generator_model_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_REPOS = [\n",
    "    'labelme',\n",
    "'gpt-engineer',\n",
    "'ChatGLM-6B',\n",
    "'SMSBoom',\n",
    "'mackup',\n",
    "'TaskMatrix',\n",
    "'Deep-Live-Cam',\n",
    "'python-fire',\n",
    "'devika',\n",
    "'gpt-pilot',\n",
    "'Real-Time-Voice-Cloning',\n",
    "'httpx',\n",
    "'kirara-ai',\n",
    "'gpt-researcher',\n",
    "'mkdocs',\n",
    "'face_recognition',\n",
    "'system-design-primer',\n",
    "'GPT_API_free',\n",
    "'30-Days-Of-Python',\n",
    "'ChatPaper',\n",
    "'twint',\n",
    "'flask',\n",
    "'professional-programming',\n",
    "'EIPs',\n",
    "'grok-1',\n",
    "'PyTorch-GAN',\n",
    "'public-apis',\n",
    "'hackingtool',\n",
    "'python-cheatsheet',\n",
    "'rembg',\n",
    "'devops-exercises',\n",
    "'MiniGPT-4',\n",
    "'python-spider',\n",
    "'chatgpt-retrieval-plugin',\n",
    "'flux'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88641431",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['repo_type'] = r['repo'].apply(\n",
    "    lambda r: 'Common' if r in COMMON_REPOS else 'Scientific'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.groupby(['model', 'proj', 'repo_type']).agg(\n",
    "    {\n",
    "        'score': 'mean',\n",
    "        'proj': 'count',\n",
    "    }\n",
    ").rename({\n",
    "    'score': 'mean_score', 'proj': 'count_repos'\n",
    "}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728399a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.groupby('repo').count().sort_values(by='score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
