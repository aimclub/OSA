{
  "implementations/munit/munit.py": [
    {
      "type": "FunctionDef",
      "name": "sample_images",
      "md_content": [],
      "code_start_line": 139,
      "code_end_line": 158,
      "params": [
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_images(batches_done):\n    \"\"\"Saves a generated sample from the validation set\"\"\"\n    imgs = next(iter(val_dataloader))\n    img_samples = None\n    for img1, img2 in zip(imgs[\"A\"], imgs[\"B\"]):\n        # Create copies of image\n        X1 = img1.unsqueeze(0).repeat(opt.style_dim, 1, 1, 1)\n        X1 = Variable(X1.type(Tensor))\n        # Get random style codes\n        s_code = np.random.uniform(-1, 1, (opt.style_dim, opt.style_dim))\n        s_code = Variable(Tensor(s_code))\n        # Generate samples\n        c_code_1, _ = Enc1(X1)\n        X12 = Dec2(c_code_1, s_code)\n        # Concatenate samples horisontally\n        X12 = torch.cat([x for x in X12.data.cpu()], -1)\n        img_sample = torch.cat((img1, X12), -1).unsqueeze(0)\n        # Concatenate with previous samples vertically\n        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n    save_image(img_samples, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/munit/models.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 8,
      "code_end_line": 14,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "LambdaLR",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 25,
      "params": [],
      "have_return": true,
      "code_content": "class LambdaLR:\n    def __init__(self, n_epochs, offset, decay_start_epoch):\n        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch\n\n    def step(self, epoch):\n        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 22,
      "params": [
        "self",
        "n_epochs",
        "offset",
        "decay_start_epoch"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, n_epochs, offset, decay_start_epoch):\n        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "step",
      "md_content": [],
      "code_start_line": 24,
      "code_end_line": 25,
      "params": [
        "self",
        "epoch"
      ],
      "have_return": true,
      "code_content": "    def step(self, epoch):\n        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Encoder",
      "md_content": [],
      "code_start_line": 33,
      "code_end_line": 42,
      "params": [],
      "have_return": true,
      "code_content": "class Encoder(nn.Module):\n    def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2, style_dim=8):\n        super(Encoder, self).__init__()\n        self.content_encoder = ContentEncoder(in_channels, dim, n_residual, n_downsample)\n        self.style_encoder = StyleEncoder(in_channels, dim, n_downsample, style_dim)\n\n    def forward(self, x):\n        content_code = self.content_encoder(x)\n        style_code = self.style_encoder(x)\n        return content_code, style_code\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 34,
      "code_end_line": 37,
      "params": [
        "self",
        "in_channels",
        "dim",
        "n_residual",
        "n_downsample",
        "style_dim"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2, style_dim=8):\n        super(Encoder, self).__init__()\n        self.content_encoder = ContentEncoder(in_channels, dim, n_residual, n_downsample)\n        self.style_encoder = StyleEncoder(in_channels, dim, n_downsample, style_dim)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 42,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        content_code = self.content_encoder(x)\n        style_code = self.style_encoder(x)\n        return content_code, style_code\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Decoder",
      "md_content": [],
      "code_start_line": 50,
      "code_end_line": 105,
      "params": [],
      "have_return": true,
      "code_content": "class Decoder(nn.Module):\n    def __init__(self, out_channels=3, dim=64, n_residual=3, n_upsample=2, style_dim=8):\n        super(Decoder, self).__init__()\n\n        layers = []\n        dim = dim * 2 ** n_upsample\n        # Residual blocks\n        for _ in range(n_residual):\n            layers += [ResidualBlock(dim, norm=\"adain\")]\n\n        # Upsampling\n        for _ in range(n_upsample):\n            layers += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(dim, dim // 2, 5, stride=1, padding=2),\n                LayerNorm(dim // 2),\n                nn.ReLU(inplace=True),\n            ]\n            dim = dim // 2\n\n        # Output layer\n        layers += [nn.ReflectionPad2d(3), nn.Conv2d(dim, out_channels, 7), nn.Tanh()]\n\n        self.model = nn.Sequential(*layers)\n\n        # Initiate mlp (predicts AdaIN parameters)\n        num_adain_params = self.get_num_adain_params()\n        self.mlp = MLP(style_dim, num_adain_params)\n\n    def get_num_adain_params(self):\n        \"\"\"Return the number of AdaIN parameters needed by the model\"\"\"\n        num_adain_params = 0\n        for m in self.modules():\n            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n                num_adain_params += 2 * m.num_features\n        return num_adain_params\n\n    def assign_adain_params(self, adain_params):\n        \"\"\"Assign the adain_params to the AdaIN layers in model\"\"\"\n        for m in self.modules():\n            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n                # Extract mean and std predictions\n                mean = adain_params[:, : m.num_features]\n                std = adain_params[:, m.num_features : 2 * m.num_features]\n                # Update bias and weight\n                m.bias = mean.contiguous().view(-1)\n                m.weight = std.contiguous().view(-1)\n                # Move pointer\n                if adain_params.size(1) > 2 * m.num_features:\n                    adain_params = adain_params[:, 2 * m.num_features :]\n\n    def forward(self, content_code, style_code):\n        # Update AdaIN parameters by MLP prediction based off style code\n        self.assign_adain_params(self.mlp(style_code))\n        img = self.model(content_code)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 51,
      "code_end_line": 77,
      "params": [
        "self",
        "out_channels",
        "dim",
        "n_residual",
        "n_upsample",
        "style_dim"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, out_channels=3, dim=64, n_residual=3, n_upsample=2, style_dim=8):\n        super(Decoder, self).__init__()\n\n        layers = []\n        dim = dim * 2 ** n_upsample\n        # Residual blocks\n        for _ in range(n_residual):\n            layers += [ResidualBlock(dim, norm=\"adain\")]\n\n        # Upsampling\n        for _ in range(n_upsample):\n            layers += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(dim, dim // 2, 5, stride=1, padding=2),\n                LayerNorm(dim // 2),\n                nn.ReLU(inplace=True),\n            ]\n            dim = dim // 2\n\n        # Output layer\n        layers += [nn.ReflectionPad2d(3), nn.Conv2d(dim, out_channels, 7), nn.Tanh()]\n\n        self.model = nn.Sequential(*layers)\n\n        # Initiate mlp (predicts AdaIN parameters)\n        num_adain_params = self.get_num_adain_params()\n        self.mlp = MLP(style_dim, num_adain_params)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_num_adain_params",
      "md_content": [],
      "code_start_line": 79,
      "code_end_line": 85,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_num_adain_params(self):\n        \"\"\"Return the number of AdaIN parameters needed by the model\"\"\"\n        num_adain_params = 0\n        for m in self.modules():\n            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n                num_adain_params += 2 * m.num_features\n        return num_adain_params\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "assign_adain_params",
      "md_content": [],
      "code_start_line": 87,
      "code_end_line": 99,
      "params": [
        "self",
        "adain_params"
      ],
      "have_return": false,
      "code_content": "    def assign_adain_params(self, adain_params):\n        \"\"\"Assign the adain_params to the AdaIN layers in model\"\"\"\n        for m in self.modules():\n            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n                # Extract mean and std predictions\n                mean = adain_params[:, : m.num_features]\n                std = adain_params[:, m.num_features : 2 * m.num_features]\n                # Update bias and weight\n                m.bias = mean.contiguous().view(-1)\n                m.weight = std.contiguous().view(-1)\n                # Move pointer\n                if adain_params.size(1) > 2 * m.num_features:\n                    adain_params = adain_params[:, 2 * m.num_features :]\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 101,
      "code_end_line": 105,
      "params": [
        "self",
        "content_code",
        "style_code"
      ],
      "have_return": true,
      "code_content": "    def forward(self, content_code, style_code):\n        # Update AdaIN parameters by MLP prediction based off style code\n        self.assign_adain_params(self.mlp(style_code))\n        img = self.model(content_code)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ContentEncoder",
      "md_content": [],
      "code_start_line": 113,
      "code_end_line": 141,
      "params": [],
      "have_return": true,
      "code_content": "class ContentEncoder(nn.Module):\n    def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2):\n        super(ContentEncoder, self).__init__()\n\n        # Initial convolution block\n        layers = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels, dim, 7),\n            nn.InstanceNorm2d(dim),\n            nn.ReLU(inplace=True),\n        ]\n\n        # Downsampling\n        for _ in range(n_downsample):\n            layers += [\n                nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1),\n                nn.InstanceNorm2d(dim * 2),\n                nn.ReLU(inplace=True),\n            ]\n            dim *= 2\n\n        # Residual blocks\n        for _ in range(n_residual):\n            layers += [ResidualBlock(dim, norm=\"in\")]\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 114,
      "code_end_line": 138,
      "params": [
        "self",
        "in_channels",
        "dim",
        "n_residual",
        "n_downsample"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_channels=3, dim=64, n_residual=3, n_downsample=2):\n        super(ContentEncoder, self).__init__()\n\n        # Initial convolution block\n        layers = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels, dim, 7),\n            nn.InstanceNorm2d(dim),\n            nn.ReLU(inplace=True),\n        ]\n\n        # Downsampling\n        for _ in range(n_downsample):\n            layers += [\n                nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1),\n                nn.InstanceNorm2d(dim * 2),\n                nn.ReLU(inplace=True),\n            ]\n            dim *= 2\n\n        # Residual blocks\n        for _ in range(n_residual):\n            layers += [ResidualBlock(dim, norm=\"in\")]\n\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 140,
      "code_end_line": 141,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "StyleEncoder",
      "md_content": [],
      "code_start_line": 149,
      "code_end_line": 171,
      "params": [],
      "have_return": true,
      "code_content": "class StyleEncoder(nn.Module):\n    def __init__(self, in_channels=3, dim=64, n_downsample=2, style_dim=8):\n        super(StyleEncoder, self).__init__()\n\n        # Initial conv block\n        layers = [nn.ReflectionPad2d(3), nn.Conv2d(in_channels, dim, 7), nn.ReLU(inplace=True)]\n\n        # Downsampling\n        for _ in range(2):\n            layers += [nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n            dim *= 2\n\n        # Downsampling with constant depth\n        for _ in range(n_downsample - 2):\n            layers += [nn.Conv2d(dim, dim, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n\n        # Average pool and output layer\n        layers += [nn.AdaptiveAvgPool2d(1), nn.Conv2d(dim, style_dim, 1, 1, 0)]\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 150,
      "code_end_line": 168,
      "params": [
        "self",
        "in_channels",
        "dim",
        "n_downsample",
        "style_dim"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_channels=3, dim=64, n_downsample=2, style_dim=8):\n        super(StyleEncoder, self).__init__()\n\n        # Initial conv block\n        layers = [nn.ReflectionPad2d(3), nn.Conv2d(in_channels, dim, 7), nn.ReLU(inplace=True)]\n\n        # Downsampling\n        for _ in range(2):\n            layers += [nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n            dim *= 2\n\n        # Downsampling with constant depth\n        for _ in range(n_downsample - 2):\n            layers += [nn.Conv2d(dim, dim, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n\n        # Average pool and output layer\n        layers += [nn.AdaptiveAvgPool2d(1), nn.Conv2d(dim, style_dim, 1, 1, 0)]\n\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 170,
      "code_end_line": 171,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "MLP",
      "md_content": [],
      "code_start_line": 179,
      "code_end_line": 189,
      "params": [],
      "have_return": true,
      "code_content": "class MLP(nn.Module):\n    def __init__(self, input_dim, output_dim, dim=256, n_blk=3, activ=\"relu\"):\n        super(MLP, self).__init__()\n        layers = [nn.Linear(input_dim, dim), nn.ReLU(inplace=True)]\n        for _ in range(n_blk - 2):\n            layers += [nn.Linear(dim, dim), nn.ReLU(inplace=True)]\n        layers += [nn.Linear(dim, output_dim)]\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 180,
      "code_end_line": 186,
      "params": [
        "self",
        "input_dim",
        "output_dim",
        "dim",
        "n_blk",
        "activ"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, input_dim, output_dim, dim=256, n_blk=3, activ=\"relu\"):\n        super(MLP, self).__init__()\n        layers = [nn.Linear(input_dim, dim), nn.ReLU(inplace=True)]\n        for _ in range(n_blk - 2):\n            layers += [nn.Linear(dim, dim), nn.ReLU(inplace=True)]\n        layers += [nn.Linear(dim, output_dim)]\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 188,
      "code_end_line": 189,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "MultiDiscriminator",
      "md_content": [],
      "code_start_line": 197,
      "code_end_line": 235,
      "params": [],
      "have_return": true,
      "code_content": "class MultiDiscriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(MultiDiscriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        # Extracts three discriminator models\n        self.models = nn.ModuleList()\n        for i in range(3):\n            self.models.add_module(\n                \"disc_%d\" % i,\n                nn.Sequential(\n                    *discriminator_block(in_channels, 64, normalize=False),\n                    *discriminator_block(64, 128),\n                    *discriminator_block(128, 256),\n                    *discriminator_block(256, 512),\n                    nn.Conv2d(512, 1, 3, padding=1)\n                ),\n            )\n\n        self.downsample = nn.AvgPool2d(in_channels, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def compute_loss(self, x, gt):\n        \"\"\"Computes the MSE between model output and scalar gt\"\"\"\n        loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n        return loss\n\n    def forward(self, x):\n        outputs = []\n        for m in self.models:\n            outputs.append(m(x))\n            x = self.downsample(x)\n        return outputs\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 198,
      "code_end_line": 223,
      "params": [
        "self",
        "in_channels"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, in_channels=3):\n        super(MultiDiscriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        # Extracts three discriminator models\n        self.models = nn.ModuleList()\n        for i in range(3):\n            self.models.add_module(\n                \"disc_%d\" % i,\n                nn.Sequential(\n                    *discriminator_block(in_channels, 64, normalize=False),\n                    *discriminator_block(64, 128),\n                    *discriminator_block(128, 256),\n                    *discriminator_block(256, 512),\n                    nn.Conv2d(512, 1, 3, padding=1)\n                ),\n            )\n\n        self.downsample = nn.AvgPool2d(in_channels, stride=2, padding=[1, 1], count_include_pad=False)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 201,
      "code_end_line": 207,
      "params": [
        "in_filters",
        "out_filters",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "compute_loss",
      "md_content": [],
      "code_start_line": 225,
      "code_end_line": 228,
      "params": [
        "self",
        "x",
        "gt"
      ],
      "have_return": true,
      "code_content": "    def compute_loss(self, x, gt):\n        \"\"\"Computes the MSE between model output and scalar gt\"\"\"\n        loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n        return loss\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 230,
      "code_end_line": 235,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        outputs = []\n        for m in self.models:\n            outputs.append(m(x))\n            x = self.downsample(x)\n        return outputs\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ResidualBlock",
      "md_content": [],
      "code_start_line": 243,
      "code_end_line": 260,
      "params": [],
      "have_return": true,
      "code_content": "class ResidualBlock(nn.Module):\n    def __init__(self, features, norm=\"in\"):\n        super(ResidualBlock, self).__init__()\n\n        norm_layer = AdaptiveInstanceNorm2d if norm == \"adain\" else nn.InstanceNorm2d\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            norm_layer(features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            norm_layer(features),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 244,
      "code_end_line": 257,
      "params": [
        "self",
        "features",
        "norm"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, features, norm=\"in\"):\n        super(ResidualBlock, self).__init__()\n\n        norm_layer = AdaptiveInstanceNorm2d if norm == \"adain\" else nn.InstanceNorm2d\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            norm_layer(features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            norm_layer(features),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 259,
      "code_end_line": 260,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return x + self.block(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "AdaptiveInstanceNorm2d",
      "md_content": [],
      "code_start_line": 268,
      "code_end_line": 301,
      "params": [],
      "have_return": true,
      "code_content": "class AdaptiveInstanceNorm2d(nn.Module):\n    \"\"\"Reference: https://github.com/NVlabs/MUNIT/blob/master/networks.py\"\"\"\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(AdaptiveInstanceNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        # weight and bias are dynamically assigned\n        self.weight = None\n        self.bias = None\n        # just dummy buffers, not used\n        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n        self.register_buffer(\"running_var\", torch.ones(num_features))\n\n    def forward(self, x):\n        assert (\n            self.weight is not None and self.bias is not None\n        ), \"Please assign weight and bias before calling AdaIN!\"\n        b, c, h, w = x.size()\n        running_mean = self.running_mean.repeat(b)\n        running_var = self.running_var.repeat(b)\n\n        # Apply instance norm\n        x_reshaped = x.contiguous().view(1, b * c, h, w)\n\n        out = F.batch_norm(\n            x_reshaped, running_mean, running_var, self.weight, self.bias, True, self.momentum, self.eps\n        )\n\n        return out.view(b, c, h, w)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"(\" + str(self.num_features) + \")\"\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 271,
      "code_end_line": 281,
      "params": [
        "self",
        "num_features",
        "eps",
        "momentum"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(AdaptiveInstanceNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        # weight and bias are dynamically assigned\n        self.weight = None\n        self.bias = None\n        # just dummy buffers, not used\n        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 283,
      "code_end_line": 298,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        assert (\n            self.weight is not None and self.bias is not None\n        ), \"Please assign weight and bias before calling AdaIN!\"\n        b, c, h, w = x.size()\n        running_mean = self.running_mean.repeat(b)\n        running_var = self.running_var.repeat(b)\n\n        # Apply instance norm\n        x_reshaped = x.contiguous().view(1, b * c, h, w)\n\n        out = F.batch_norm(\n            x_reshaped, running_mean, running_var, self.weight, self.bias, True, self.momentum, self.eps\n        )\n\n        return out.view(b, c, h, w)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__repr__",
      "md_content": [],
      "code_start_line": 300,
      "code_end_line": 301,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __repr__(self):\n        return self.__class__.__name__ + \"(\" + str(self.num_features) + \")\"\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "LayerNorm",
      "md_content": [],
      "code_start_line": 304,
      "code_end_line": 324,
      "params": [],
      "have_return": true,
      "code_content": "class LayerNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, affine=True):\n        super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))\n\n    def forward(self, x):\n        shape = [-1] + [1] * (x.dim() - 1)\n        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n        std = x.view(x.size(0), -1).std(1).view(*shape)\n        x = (x - mean) / (std + self.eps)\n\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n        return x\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 305,
      "code_end_line": 313,
      "params": [
        "self",
        "num_features",
        "eps",
        "affine"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, num_features, eps=1e-5, affine=True):\n        super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 315,
      "code_end_line": 324,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        shape = [-1] + [1] * (x.dim() - 1)\n        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n        std = x.view(x.size(0), -1).std(1).view(*shape)\n        x = (x - mean) / (std + self.eps)\n\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n        return x\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/munit/datasets.py": [
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 36,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n        if mode == \"train\":\n            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 17,
      "params": [
        "self",
        "root",
        "transforms_",
        "mode"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n        if mode == \"train\":\n            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 33,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 35,
      "code_end_line": 36,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/bgan/bgan.py": [
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 40,
      "code_end_line": 63,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 41,
      "code_end_line": 58,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 44,
      "code_end_line": 49,
      "params": [
        "in_feat",
        "out_feat",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 60,
      "code_end_line": 63,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 66,
      "code_end_line": 82,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 67,
      "code_end_line": 77,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 79,
      "code_end_line": 82,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "boundary_seeking_loss",
      "md_content": [],
      "code_start_line": 85,
      "code_end_line": 90,
      "params": [
        "y_pred",
        "y_true"
      ],
      "have_return": true,
      "code_content": "def boundary_seeking_loss(y_pred, y_true):\n    \"\"\"\n    Boundary seeking loss.\n    Reference: https://wiseodd.github.io/techblog/2017/03/07/boundary-seeking-gan/\n    \"\"\"\n    return 0.5 * torch.mean((torch.log(y_pred) - torch.log(1 - y_pred)) ** 2)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/ebgan/ebgan.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 38,
      "code_end_line": 44,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 47,
      "code_end_line": 71,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, noise):\n        out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 48,
      "code_end_line": 65,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 67,
      "code_end_line": 71,
      "params": [
        "self",
        "noise"
      ],
      "have_return": true,
      "code_content": "    def forward(self, noise):\n        out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 74,
      "code_end_line": 101,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        # Upsampling\n        self.down = nn.Sequential(nn.Conv2d(opt.channels, 64, 3, 2, 1), nn.ReLU())\n        # Fully-connected layers\n        self.down_size = opt.img_size // 2\n        down_dim = 64 * (opt.img_size // 2) ** 2\n\n        self.embedding = nn.Linear(down_dim, 32)\n\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(32, 0.8),\n            nn.ReLU(inplace=True),\n            nn.Linear(32, down_dim),\n            nn.BatchNorm1d(down_dim),\n            nn.ReLU(inplace=True),\n        )\n        # Upsampling\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(64, opt.channels, 3, 1, 1))\n\n    def forward(self, img):\n        out = self.down(img)\n        embedding = self.embedding(out.view(out.size(0), -1))\n        out = self.fc(embedding)\n        out = self.up(out.view(out.size(0), 64, self.down_size, self.down_size))\n        return out, embedding\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 75,
      "code_end_line": 94,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        # Upsampling\n        self.down = nn.Sequential(nn.Conv2d(opt.channels, 64, 3, 2, 1), nn.ReLU())\n        # Fully-connected layers\n        self.down_size = opt.img_size // 2\n        down_dim = 64 * (opt.img_size // 2) ** 2\n\n        self.embedding = nn.Linear(down_dim, 32)\n\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(32, 0.8),\n            nn.ReLU(inplace=True),\n            nn.Linear(32, down_dim),\n            nn.BatchNorm1d(down_dim),\n            nn.ReLU(inplace=True),\n        )\n        # Upsampling\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(64, opt.channels, 3, 1, 1))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 96,
      "code_end_line": 101,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.down(img)\n        embedding = self.embedding(out.view(out.size(0), -1))\n        out = self.fc(embedding)\n        out = self.up(out.view(out.size(0), 64, self.down_size, self.down_size))\n        return out, embedding\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "pullaway_loss",
      "md_content": [],
      "code_start_line": 142,
      "code_end_line": 148,
      "params": [
        "embeddings"
      ],
      "have_return": true,
      "code_content": "def pullaway_loss(embeddings):\n    norm = torch.sqrt(torch.sum(embeddings ** 2, -1, keepdim=True))\n    normalized_emb = embeddings / norm\n    similarity = torch.matmul(normalized_emb, normalized_emb.transpose(1, 0))\n    batch_size = embeddings.size(0)\n    loss_pt = (torch.sum(similarity) - batch_size) / (batch_size * (batch_size - 1))\n    return loss_pt\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/ccgan/ccgan.py": [
    {
      "type": "FunctionDef",
      "name": "apply_random_mask",
      "md_content": [],
      "code_start_line": 84,
      "code_end_line": 92,
      "params": [
        "imgs"
      ],
      "have_return": true,
      "code_content": "def apply_random_mask(imgs):\n    idx = np.random.randint(0, opt.img_size - opt.mask_size, (imgs.shape[0], 2))\n\n    masked_imgs = imgs.clone()\n    for i, (y1, x1) in enumerate(idx):\n        y2, x2 = y1 + opt.mask_size, x1 + opt.mask_size\n        masked_imgs[i, :, y1:y2, x1:x2] = -1\n\n    return masked_imgs\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "save_sample",
      "md_content": [],
      "code_start_line": 95,
      "code_end_line": 100,
      "params": [
        "saved_samples"
      ],
      "have_return": false,
      "code_content": "def save_sample(saved_samples):\n    # Generate inpainted image\n    gen_imgs = generator(saved_samples[\"masked\"], saved_samples[\"lowres\"])\n    # Save sample\n    sample = torch.cat((saved_samples[\"masked\"].data, gen_imgs.data, saved_samples[\"imgs\"].data), -2)\n    save_image(sample, \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/ccgan/models.py": [
    {
      "type": "ClassDef",
      "name": "UNetDown",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 23,
      "params": [],
      "have_return": true,
      "code_content": "class UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        model = [nn.Conv2d(in_size, out_size, 4, stride=2, padding=1, bias=False)]\n        if normalize:\n            model.append(nn.BatchNorm2d(out_size, 0.8))\n        model.append(nn.LeakyReLU(0.2))\n        if dropout:\n            model.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 20,
      "params": [
        "self",
        "in_size",
        "out_size",
        "normalize",
        "dropout"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        model = [nn.Conv2d(in_size, out_size, 4, stride=2, padding=1, bias=False)]\n        if normalize:\n            model.append(nn.BatchNorm2d(out_size, 0.8))\n        model.append(nn.LeakyReLU(0.2))\n        if dropout:\n            model.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*model)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 22,
      "code_end_line": 23,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "UNetUp",
      "md_content": [],
      "code_start_line": 26,
      "code_end_line": 42,
      "params": [],
      "have_return": true,
      "code_content": "class UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        model = [\n            nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_size, 0.8),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            model.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        out = torch.cat((x, skip_input), 1)\n        return out\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 27,
      "code_end_line": 37,
      "params": [
        "self",
        "in_size",
        "out_size",
        "dropout"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        model = [\n            nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_size, 0.8),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            model.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*model)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 42,
      "params": [
        "self",
        "x",
        "skip_input"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x, skip_input):\n        x = self.model(x)\n        out = torch.cat((x, skip_input), 1)\n        return out\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 45,
      "code_end_line": 80,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self, input_shape):\n        super(Generator, self).__init__()\n        channels, _, _ = input_shape\n        self.down1 = UNetDown(channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128 + channels, 256, dropout=0.5)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 256, dropout=0.5)\n        self.up4 = UNetUp(512, 128)\n        self.up5 = UNetUp(256 + channels, 64)\n\n        final = [nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, 1, 1), nn.Tanh()]\n        self.final = nn.Sequential(*final)\n\n    def forward(self, x, x_lr):\n        # U-Net generator with skip connections from encoder to decoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d2 = torch.cat((d2, x_lr), 1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        u1 = self.up1(d6, d5)\n        u2 = self.up2(u1, d4)\n        u3 = self.up3(u2, d3)\n        u4 = self.up4(u3, d2)\n        u5 = self.up5(u4, d1)\n\n        return self.final(u5)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 63,
      "params": [
        "self",
        "input_shape"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, input_shape):\n        super(Generator, self).__init__()\n        channels, _, _ = input_shape\n        self.down1 = UNetDown(channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128 + channels, 256, dropout=0.5)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 256, dropout=0.5)\n        self.up4 = UNetUp(512, 128)\n        self.up5 = UNetUp(256 + channels, 64)\n\n        final = [nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, 1, 1), nn.Tanh()]\n        self.final = nn.Sequential(*final)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 65,
      "code_end_line": 80,
      "params": [
        "self",
        "x",
        "x_lr"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x, x_lr):\n        # U-Net generator with skip connections from encoder to decoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d2 = torch.cat((d2, x_lr), 1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        u1 = self.up1(d6, d5)\n        u2 = self.up2(u1, d4)\n        u3 = self.up3(u2, d3)\n        u4 = self.up4(u3, d2)\n        u5 = self.up5(u4, d1)\n\n        return self.final(u5)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 83,
      "code_end_line": 111,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n        # Calculate output of image discriminator (PatchGAN)\n        patch_h, patch_w = int(height / 2 ** 3), int(width / 2 ** 3)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, stride, normalize):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = channels\n        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 84,
      "code_end_line": 108,
      "params": [
        "self",
        "input_shape"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n        # Calculate output of image discriminator (PatchGAN)\n        patch_h, patch_w = int(height / 2 ** 3), int(width / 2 ** 3)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, stride, normalize):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = channels\n        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 92,
      "code_end_line": 98,
      "params": [
        "in_filters",
        "out_filters",
        "stride",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, stride, normalize):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 110,
      "code_end_line": 111,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/ccgan/datasets.py": [
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 27,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, transforms_x=None, transforms_lr=None, mode='train'):\n        self.transform_x = transforms.Compose(transforms_x)\n        self.transform_lr = transforms.Compose(transforms_lr)\n\n        self.files = sorted(glob.glob('%s/*.*' % root))\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n\n        x = self.transform_x(img)\n        x_lr = self.transform_lr(img)\n\n        return {'x': x, 'x_lr': x_lr}\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 15,
      "params": [
        "self",
        "root",
        "transforms_x",
        "transforms_lr",
        "mode"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, transforms_x=None, transforms_lr=None, mode='train'):\n        self.transform_x = transforms.Compose(transforms_x)\n        self.transform_lr = transforms.Compose(transforms_lr)\n\n        self.files = sorted(glob.glob('%s/*.*' % root))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 24,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n\n        x = self.transform_x(img)\n        x_lr = self.transform_lr(img)\n\n        return {'x': x, 'x_lr': x_lr}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 26,
      "code_end_line": 27,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/srgan/models.py": [
    {
      "type": "ClassDef",
      "name": "FeatureExtractor",
      "md_content": [],
      "code_start_line": 8,
      "code_end_line": 15,
      "params": [],
      "have_return": true,
      "code_content": "class FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n\n    def forward(self, img):\n        return self.feature_extractor(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 9,
      "code_end_line": 12,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 14,
      "code_end_line": 15,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        return self.feature_extractor(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ResidualBlock",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 30,
      "params": [],
      "have_return": true,
      "code_content": "class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n            nn.PReLU(),\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 27,
      "params": [
        "self",
        "in_features"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n            nn.PReLU(),\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 29,
      "code_end_line": 30,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return x + self.conv_block(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "GeneratorResNet",
      "md_content": [],
      "code_start_line": 33,
      "code_end_line": 71,
      "params": [],
      "have_return": true,
      "code_content": "class GeneratorResNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n        super(GeneratorResNet, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n\n        # Residual blocks\n        res_blocks = []\n        for _ in range(n_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n\n        # Upsampling layers\n        upsampling = []\n        for out_features in range(2):\n            upsampling += [\n                # nn.Upsample(scale_factor=2),\n                nn.Conv2d(64, 256, 3, 1, 1),\n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),\n            ]\n        self.upsampling = nn.Sequential(*upsampling)\n\n        # Final output layer\n        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 34,
      "code_end_line": 62,
      "params": [
        "self",
        "in_channels",
        "out_channels",
        "n_residual_blocks"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n        super(GeneratorResNet, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n\n        # Residual blocks\n        res_blocks = []\n        for _ in range(n_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n\n        # Upsampling layers\n        upsampling = []\n        for out_features in range(2):\n            upsampling += [\n                # nn.Upsample(scale_factor=2),\n                nn.Conv2d(64, 256, 3, 1, 1),\n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),\n            ]\n        self.upsampling = nn.Sequential(*upsampling)\n\n        # Final output layer\n        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 64,
      "code_end_line": 71,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 74,
      "code_end_line": 105,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 75,
      "code_end_line": 102,
      "params": [
        "self",
        "input_shape"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 83,
      "code_end_line": 92,
      "params": [
        "in_filters",
        "out_filters",
        "first_block"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 104,
      "code_end_line": 105,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/srgan/datasets.py": [
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 16,
      "code_end_line": 45,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, hr_shape):\n        hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n\n        self.files = sorted(glob.glob(root + \"/*.*\"))\n\n    def __getitem__(self, index):\n        img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 35,
      "params": [
        "self",
        "root",
        "hr_shape"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, hr_shape):\n        hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n\n        self.files = sorted(glob.glob(root + \"/*.*\"))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 42,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n        img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 44,
      "code_end_line": 45,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/srgan/srgan.py": [],
  "implementations/relativistic_gan/relativistic_gan.py": [
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 62,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, z):\n        out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 38,
      "code_end_line": 56,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 58,
      "code_end_line": 62,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 65,
      "code_end_line": 91,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n\n    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 66,
      "code_end_line": 84,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 69,
      "code_end_line": 73,
      "params": [
        "in_filters",
        "out_filters",
        "bn"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 86,
      "code_end_line": 91,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/lsgan/lsgan.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 42,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 45,
      "code_end_line": 69,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, z):\n        out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 63,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 65,
      "code_end_line": 69,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 72,
      "code_end_line": 98,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Linear(128 * ds_size ** 2, 1)\n\n    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 73,
      "code_end_line": 91,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Linear(128 * ds_size ** 2, 1)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 76,
      "code_end_line": 80,
      "params": [
        "in_filters",
        "out_filters",
        "bn"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 93,
      "code_end_line": 98,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/cogan/mnistm.py": [
    {
      "type": "ClassDef",
      "name": "MNISTM",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 146,
      "params": [],
      "have_return": true,
      "code_content": "class MNISTM(data.Dataset):\n    \"\"\"`MNIST-M Dataset.\"\"\"\n\n    url = \"https://github.com/VanushVaswani/keras_mnistm/releases/download/1.0/keras_mnistm.pkl.gz\"\n\n    raw_folder = \"raw\"\n    processed_folder = \"processed\"\n    training_file = \"mnist_m_train.pt\"\n    test_file = \"mnist_m_test.pt\"\n\n    def __init__(self, root, mnist_root=\"data\", train=True, transform=None, target_transform=None, download=False):\n        \"\"\"Init MNIST-M dataset.\"\"\"\n        super(MNISTM, self).__init__()\n        self.root = os.path.expanduser(root)\n        self.mnist_root = os.path.expanduser(mnist_root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train  # training set or test set\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError(\"Dataset not found.\" + \" You can use download=True to download it\")\n\n        if self.train:\n            self.train_data, self.train_labels = torch.load(\n                os.path.join(self.root, self.processed_folder, self.training_file)\n            )\n        else:\n            self.test_data, self.test_labels = torch.load(\n                os.path.join(self.root, self.processed_folder, self.test_file)\n            )\n\n    def __getitem__(self, index):\n        \"\"\"Get images and target for data loader.\n\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"\n        if self.train:\n            img, target = self.train_data[index], self.train_labels[index]\n        else:\n            img, target = self.test_data[index], self.test_labels[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(img.squeeze().numpy(), mode=\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        \"\"\"Return size of dataset.\"\"\"\n        if self.train:\n            return len(self.train_data)\n        else:\n            return len(self.test_data)\n\n    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and os.path.exists(\n            os.path.join(self.root, self.processed_folder, self.test_file)\n        )\n\n    def download(self):\n        \"\"\"Download the MNIST data.\"\"\"\n        # import essential packages\n        from six.moves import urllib\n        import gzip\n        import pickle\n        from torchvision import datasets\n\n        # check if dataset already exists\n        if self._check_exists():\n            return\n\n        # make data dirs\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        # download pkl files\n        print(\"Downloading \" + self.url)\n        filename = self.url.rpartition(\"/\")[2]\n        file_path = os.path.join(self.root, self.raw_folder, filename)\n        if not os.path.exists(file_path.replace(\".gz\", \"\")):\n            data = urllib.request.urlopen(self.url)\n            with open(file_path, \"wb\") as f:\n                f.write(data.read())\n            with open(file_path.replace(\".gz\", \"\"), \"wb\") as out_f, gzip.GzipFile(file_path) as zip_f:\n                out_f.write(zip_f.read())\n            os.unlink(file_path)\n\n        # process and save as torch files\n        print(\"Processing...\")\n\n        # load MNIST-M images from pkl file\n        with open(file_path.replace(\".gz\", \"\"), \"rb\") as f:\n            mnist_m_data = pickle.load(f, encoding=\"bytes\")\n        mnist_m_train_data = torch.ByteTensor(mnist_m_data[b\"train\"])\n        mnist_m_test_data = torch.ByteTensor(mnist_m_data[b\"test\"])\n\n        # get MNIST labels\n        mnist_train_labels = datasets.MNIST(root=self.mnist_root, train=True, download=True).train_labels\n        mnist_test_labels = datasets.MNIST(root=self.mnist_root, train=False, download=True).test_labels\n\n        # save MNIST-M dataset\n        training_set = (mnist_m_train_data, mnist_train_labels)\n        test_set = (mnist_m_test_data, mnist_test_labels)\n        with open(os.path.join(self.root, self.processed_folder, self.training_file), \"wb\") as f:\n            torch.save(training_set, f)\n        with open(os.path.join(self.root, self.processed_folder, self.test_file), \"wb\") as f:\n            torch.save(test_set, f)\n\n        print(\"Done!\")\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 29,
      "code_end_line": 51,
      "params": [
        "self",
        "root",
        "mnist_root",
        "train",
        "transform",
        "target_transform",
        "download"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, mnist_root=\"data\", train=True, transform=None, target_transform=None, download=False):\n        \"\"\"Init MNIST-M dataset.\"\"\"\n        super(MNISTM, self).__init__()\n        self.root = os.path.expanduser(root)\n        self.mnist_root = os.path.expanduser(mnist_root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train  # training set or test set\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError(\"Dataset not found.\" + \" You can use download=True to download it\")\n\n        if self.train:\n            self.train_data, self.train_labels = torch.load(\n                os.path.join(self.root, self.processed_folder, self.training_file)\n            )\n        else:\n            self.test_data, self.test_labels = torch.load(\n                os.path.join(self.root, self.processed_folder, self.test_file)\n            )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 53,
      "code_end_line": 77,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n        \"\"\"Get images and target for data loader.\n\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"\n        if self.train:\n            img, target = self.train_data[index], self.train_labels[index]\n        else:\n            img, target = self.test_data[index], self.test_labels[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(img.squeeze().numpy(), mode=\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 79,
      "code_end_line": 84,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        \"\"\"Return size of dataset.\"\"\"\n        if self.train:\n            return len(self.train_data)\n        else:\n            return len(self.test_data)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_exists",
      "md_content": [],
      "code_start_line": 86,
      "code_end_line": 89,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and os.path.exists(\n            os.path.join(self.root, self.processed_folder, self.test_file)\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "download",
      "md_content": [],
      "code_start_line": 91,
      "code_end_line": 146,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def download(self):\n        \"\"\"Download the MNIST data.\"\"\"\n        # import essential packages\n        from six.moves import urllib\n        import gzip\n        import pickle\n        from torchvision import datasets\n\n        # check if dataset already exists\n        if self._check_exists():\n            return\n\n        # make data dirs\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        # download pkl files\n        print(\"Downloading \" + self.url)\n        filename = self.url.rpartition(\"/\")[2]\n        file_path = os.path.join(self.root, self.raw_folder, filename)\n        if not os.path.exists(file_path.replace(\".gz\", \"\")):\n            data = urllib.request.urlopen(self.url)\n            with open(file_path, \"wb\") as f:\n                f.write(data.read())\n            with open(file_path.replace(\".gz\", \"\"), \"wb\") as out_f, gzip.GzipFile(file_path) as zip_f:\n                out_f.write(zip_f.read())\n            os.unlink(file_path)\n\n        # process and save as torch files\n        print(\"Processing...\")\n\n        # load MNIST-M images from pkl file\n        with open(file_path.replace(\".gz\", \"\"), \"rb\") as f:\n            mnist_m_data = pickle.load(f, encoding=\"bytes\")\n        mnist_m_train_data = torch.ByteTensor(mnist_m_data[b\"train\"])\n        mnist_m_test_data = torch.ByteTensor(mnist_m_data[b\"test\"])\n\n        # get MNIST labels\n        mnist_train_labels = datasets.MNIST(root=self.mnist_root, train=True, download=True).train_labels\n        mnist_test_labels = datasets.MNIST(root=self.mnist_root, train=False, download=True).test_labels\n\n        # save MNIST-M dataset\n        training_set = (mnist_m_train_data, mnist_train_labels)\n        test_set = (mnist_m_test_data, mnist_test_labels)\n        with open(os.path.join(self.root, self.processed_folder, self.training_file), \"wb\") as f:\n            torch.save(training_set, f)\n        with open(os.path.join(self.root, self.processed_folder, self.test_file), \"wb\") as f:\n            torch.save(test_set, f)\n\n        print(\"Done!\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/cogan/cogan.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 48,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Linear\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "CoupledGenerators",
      "md_content": [],
      "code_start_line": 51,
      "code_end_line": 87,
      "params": [],
      "have_return": true,
      "code_content": "class CoupledGenerators(nn.Module):\n    def __init__(self):\n        super(CoupledGenerators, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.fc = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.shared_conv = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n        )\n        self.G1 = nn.Sequential(\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n        self.G2 = nn.Sequential(\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, noise):\n        out = self.fc(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img_emb = self.shared_conv(out)\n        img1 = self.G1(img_emb)\n        img2 = self.G2(img_emb)\n        return img1, img2\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 52,
      "code_end_line": 79,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(CoupledGenerators, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.fc = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.shared_conv = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n        )\n        self.G1 = nn.Sequential(\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n        self.G2 = nn.Sequential(\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 81,
      "code_end_line": 87,
      "params": [
        "self",
        "noise"
      ],
      "have_return": true,
      "code_content": "    def forward(self, noise):\n        out = self.fc(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img_emb = self.shared_conv(out)\n        img1 = self.G1(img_emb)\n        img2 = self.G2(img_emb)\n        return img1, img2\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "CoupledDiscriminators",
      "md_content": [],
      "code_start_line": 90,
      "code_end_line": 122,
      "params": [],
      "have_return": true,
      "code_content": "class CoupledDiscriminators(nn.Module):\n    def __init__(self):\n        super(CoupledDiscriminators, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            block.extend([nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)])\n            return block\n\n        self.shared_conv = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.D1 = nn.Linear(128 * ds_size ** 2, 1)\n        self.D2 = nn.Linear(128 * ds_size ** 2, 1)\n\n    def forward(self, img1, img2):\n        # Determine validity of first image\n        out = self.shared_conv(img1)\n        out = out.view(out.shape[0], -1)\n        validity1 = self.D1(out)\n        # Determine validity of second image\n        out = self.shared_conv(img2)\n        out = out.view(out.shape[0], -1)\n        validity2 = self.D2(out)\n\n        return validity1, validity2\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 91,
      "code_end_line": 110,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(CoupledDiscriminators, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            block.extend([nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)])\n            return block\n\n        self.shared_conv = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.D1 = nn.Linear(128 * ds_size ** 2, 1)\n        self.D2 = nn.Linear(128 * ds_size ** 2, 1)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 94,
      "code_end_line": 99,
      "params": [
        "in_filters",
        "out_filters",
        "bn"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            block.extend([nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)])\n            return block\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 112,
      "code_end_line": 122,
      "params": [
        "self",
        "img1",
        "img2"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img1, img2):\n        # Determine validity of first image\n        out = self.shared_conv(img1)\n        out = out.view(out.shape[0], -1)\n        validity1 = self.D1(out)\n        # Determine validity of second image\n        out = self.shared_conv(img2)\n        out = out.view(out.shape[0], -1)\n        validity2 = self.D2(out)\n\n        return validity1, validity2\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/acgan/acgan.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 43,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 74,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.label_emb = nn.Embedding(opt.n_classes, opt.latent_dim)\n\n        self.init_size = opt.img_size // 4  # Initial size before upsampling\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, noise, labels):\n        gen_input = torch.mul(self.label_emb(labels), noise)\n        out = self.l1(gen_input)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 47,
      "code_end_line": 67,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.label_emb = nn.Embedding(opt.n_classes, opt.latent_dim)\n\n        self.init_size = opt.img_size // 4  # Initial size before upsampling\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 69,
      "code_end_line": 74,
      "params": [
        "self",
        "noise",
        "labels"
      ],
      "have_return": true,
      "code_content": "    def forward(self, noise, labels):\n        gen_input = torch.mul(self.label_emb(labels), noise)\n        out = self.l1(gen_input)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 77,
      "code_end_line": 108,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.conv_blocks = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n\n        # Output layers\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n\n    def forward(self, img):\n        out = self.conv_blocks(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        label = self.aux_layer(out)\n\n        return validity, label\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 78,
      "code_end_line": 100,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.conv_blocks = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n\n        # Output layers\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 81,
      "code_end_line": 86,
      "params": [
        "in_filters",
        "out_filters",
        "bn"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 102,
      "code_end_line": 108,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.conv_blocks(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        label = self.aux_layer(out)\n\n        return validity, label\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "sample_image",
      "md_content": [],
      "code_start_line": 152,
      "code_end_line": 160,
      "params": [
        "n_row",
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_image(n_row, batches_done):\n    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n    # Sample noise\n    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n    # Get labels ranging from 0 to n_classes for n rows\n    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n    labels = Variable(LongTensor(labels))\n    gen_imgs = generator(z, labels)\n    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/dcgan/dcgan.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 42,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 45,
      "code_end_line": 70,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, z):\n        out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 64,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 66,
      "code_end_line": 70,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 73,
      "code_end_line": 99,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n\n    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 74,
      "code_end_line": 92,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 77,
      "code_end_line": 81,
      "params": [
        "in_filters",
        "out_filters",
        "bn"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 94,
      "code_end_line": 99,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/infogan/infogan.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 41,
      "code_end_line": 47,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "to_categorical",
      "md_content": [],
      "code_start_line": 50,
      "code_end_line": 55,
      "params": [
        "y",
        "num_columns"
      ],
      "have_return": true,
      "code_content": "def to_categorical(y, num_columns):\n    \"\"\"Returns one-hot encoded Variable\"\"\"\n    y_cat = np.zeros((y.shape[0], num_columns))\n    y_cat[range(y.shape[0]), y] = 1.0\n\n    return Variable(FloatTensor(y_cat))\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 58,
      "code_end_line": 85,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        input_dim = opt.latent_dim + opt.n_classes + opt.code_dim\n\n        self.init_size = opt.img_size // 4  # Initial size before upsampling\n        self.l1 = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, noise, labels, code):\n        gen_input = torch.cat((noise, labels, code), -1)\n        out = self.l1(gen_input)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 59,
      "code_end_line": 78,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n        input_dim = opt.latent_dim + opt.n_classes + opt.code_dim\n\n        self.init_size = opt.img_size // 4  # Initial size before upsampling\n        self.l1 = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 80,
      "code_end_line": 85,
      "params": [
        "self",
        "noise",
        "labels",
        "code"
      ],
      "have_return": true,
      "code_content": "    def forward(self, noise, labels, code):\n        gen_input = torch.cat((noise, labels, code), -1)\n        out = self.l1(gen_input)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 88,
      "code_end_line": 121,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.conv_blocks = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n\n        # Output layers\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n        self.latent_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.code_dim))\n\n    def forward(self, img):\n        out = self.conv_blocks(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        label = self.aux_layer(out)\n        latent_code = self.latent_layer(out)\n\n        return validity, label, latent_code\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 89,
      "code_end_line": 112,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.conv_blocks = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n\n        # Output layers\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n        self.latent_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.code_dim))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 92,
      "code_end_line": 97,
      "params": [
        "in_filters",
        "out_filters",
        "bn"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 114,
      "code_end_line": 121,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.conv_blocks(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        label = self.aux_layer(out)\n        latent_code = self.latent_layer(out)\n\n        return validity, label, latent_code\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "sample_image",
      "md_content": [],
      "code_start_line": 181,
      "code_end_line": 196,
      "params": [
        "n_row",
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_image(n_row, batches_done):\n    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n    # Static sample\n    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n    static_sample = generator(z, static_label, static_code)\n    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n\n    # Get varied c1 and c2\n    zeros = np.zeros((n_row ** 2, 1))\n    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n    sample1 = generator(static_z, static_label, c1)\n    sample2 = generator(static_z, static_label, c2)\n    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/softmax_gan/softmax_gan.py": [
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 38,
      "code_end_line": 61,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 56,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 47,
      "params": [
        "in_feat",
        "out_feat",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 58,
      "code_end_line": 61,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 64,
      "code_end_line": 80,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.img_size ** 2, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 65,
      "code_end_line": 74,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.img_size ** 2, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 76,
      "code_end_line": 80,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "log",
      "md_content": [],
      "code_start_line": 117,
      "code_end_line": 118,
      "params": [
        "x"
      ],
      "have_return": true,
      "code_content": "def log(x):\n    return torch.log(x + 1e-8)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/bicyclegan/bicyclegan.py": [
    {
      "type": "FunctionDef",
      "name": "sample_images",
      "md_content": [],
      "code_start_line": 102,
      "code_end_line": 122,
      "params": [
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_images(batches_done):\n    \"\"\"Saves a generated sample from the validation set\"\"\"\n    generator.eval()\n    imgs = next(iter(val_dataloader))\n    img_samples = None\n    for img_A, img_B in zip(imgs[\"A\"], imgs[\"B\"]):\n        # Repeat input image by number of desired columns\n        real_A = img_A.view(1, *img_A.shape).repeat(opt.latent_dim, 1, 1, 1)\n        real_A = Variable(real_A.type(Tensor))\n        # Sample latent representations\n        sampled_z = Variable(Tensor(np.random.normal(0, 1, (opt.latent_dim, opt.latent_dim))))\n        # Generate samples\n        fake_B = generator(real_A, sampled_z)\n        # Concatenate samples horisontally\n        fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n        img_sample = torch.cat((img_A, fake_B), -1)\n        img_sample = img_sample.view(1, *img_sample.shape)\n        # Concatenate with previous samples vertically\n        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n    save_image(img_samples, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=8, normalize=True)\n    generator.train()\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "reparameterization",
      "md_content": [],
      "code_start_line": 125,
      "code_end_line": 129,
      "params": [
        "mu",
        "logvar"
      ],
      "have_return": true,
      "code_content": "def reparameterization(mu, logvar):\n    std = torch.exp(logvar / 2)\n    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), opt.latent_dim))))\n    z = sampled_z * std + mu\n    return z\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/bicyclegan/models.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 9,
      "code_end_line": 15,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "UNetDown",
      "md_content": [],
      "code_start_line": 23,
      "code_end_line": 33,
      "params": [],
      "have_return": true,
      "code_content": "class UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size, 0.8))\n        layers.append(nn.LeakyReLU(0.2))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 24,
      "code_end_line": 30,
      "params": [
        "self",
        "in_size",
        "out_size",
        "normalize",
        "dropout"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size, 0.8))\n        layers.append(nn.LeakyReLU(0.2))\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 32,
      "code_end_line": 33,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "UNetUp",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 49,
      "params": [],
      "have_return": true,
      "code_content": "class UNetUp(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(UNetUp, self).__init__()\n        self.model = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_size, out_size, 3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_size, 0.8),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 44,
      "params": [
        "self",
        "in_size",
        "out_size"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size):\n        super(UNetUp, self).__init__()\n        self.model = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_size, out_size, 3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_size, 0.8),\n            nn.ReLU(inplace=True),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 49,
      "params": [
        "self",
        "x",
        "skip_input"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 52,
      "code_end_line": 94,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self, latent_dim, img_shape):\n        super(Generator, self).__init__()\n        channels, self.h, self.w = img_shape\n\n        self.fc = nn.Linear(latent_dim, self.h * self.w)\n\n        self.down1 = UNetDown(channels + 1, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512)\n        self.down5 = UNetDown(512, 512)\n        self.down6 = UNetDown(512, 512)\n        self.down7 = UNetDown(512, 512, normalize=False)\n        self.up1 = UNetUp(512, 512)\n        self.up2 = UNetUp(1024, 512)\n        self.up3 = UNetUp(1024, 512)\n        self.up4 = UNetUp(1024, 256)\n        self.up5 = UNetUp(512, 128)\n        self.up6 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, stride=1, padding=1), nn.Tanh()\n        )\n\n    def forward(self, x, z):\n        # Propogate noise through fc layer and reshape to img shape\n        z = self.fc(z).view(z.size(0), 1, self.h, self.w)\n        d1 = self.down1(torch.cat((x, z), 1))\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        u1 = self.up1(d7, d6)\n        u2 = self.up2(u1, d5)\n        u3 = self.up3(u2, d4)\n        u4 = self.up4(u3, d3)\n        u5 = self.up5(u4, d2)\n        u6 = self.up6(u5, d1)\n\n        return self.final(u6)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 53,
      "code_end_line": 75,
      "params": [
        "self",
        "latent_dim",
        "img_shape"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, latent_dim, img_shape):\n        super(Generator, self).__init__()\n        channels, self.h, self.w = img_shape\n\n        self.fc = nn.Linear(latent_dim, self.h * self.w)\n\n        self.down1 = UNetDown(channels + 1, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512)\n        self.down5 = UNetDown(512, 512)\n        self.down6 = UNetDown(512, 512)\n        self.down7 = UNetDown(512, 512, normalize=False)\n        self.up1 = UNetUp(512, 512)\n        self.up2 = UNetUp(1024, 512)\n        self.up3 = UNetUp(1024, 512)\n        self.up4 = UNetUp(1024, 256)\n        self.up5 = UNetUp(512, 128)\n        self.up6 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, stride=1, padding=1), nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 77,
      "code_end_line": 94,
      "params": [
        "self",
        "x",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x, z):\n        # Propogate noise through fc layer and reshape to img shape\n        z = self.fc(z).view(z.size(0), 1, self.h, self.w)\n        d1 = self.down1(torch.cat((x, z), 1))\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        u1 = self.up1(d7, d6)\n        u2 = self.up2(u1, d5)\n        u3 = self.up3(u2, d4)\n        u4 = self.up4(u3, d3)\n        u5 = self.up5(u4, d2)\n        u6 = self.up6(u5, d1)\n\n        return self.final(u6)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Encoder",
      "md_content": [],
      "code_start_line": 102,
      "code_end_line": 118,
      "params": [],
      "have_return": true,
      "code_content": "class Encoder(nn.Module):\n    def __init__(self, latent_dim, input_shape):\n        super(Encoder, self).__init__()\n        resnet18_model = resnet18(pretrained=False)\n        self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n        self.pooling = nn.AvgPool2d(kernel_size=8, stride=8, padding=0)\n        # Output is mu and log(var) for reparameterization trick used in VAEs\n        self.fc_mu = nn.Linear(256, latent_dim)\n        self.fc_logvar = nn.Linear(256, latent_dim)\n\n    def forward(self, img):\n        out = self.feature_extractor(img)\n        out = self.pooling(out)\n        out = out.view(out.size(0), -1)\n        mu = self.fc_mu(out)\n        logvar = self.fc_logvar(out)\n        return mu, logvar\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 103,
      "code_end_line": 110,
      "params": [
        "self",
        "latent_dim",
        "input_shape"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, latent_dim, input_shape):\n        super(Encoder, self).__init__()\n        resnet18_model = resnet18(pretrained=False)\n        self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n        self.pooling = nn.AvgPool2d(kernel_size=8, stride=8, padding=0)\n        # Output is mu and log(var) for reparameterization trick used in VAEs\n        self.fc_mu = nn.Linear(256, latent_dim)\n        self.fc_logvar = nn.Linear(256, latent_dim)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 112,
      "code_end_line": 118,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.feature_extractor(img)\n        out = self.pooling(out)\n        out = out.view(out.size(0), -1)\n        mu = self.fc_mu(out)\n        logvar = self.fc_logvar(out)\n        return mu, logvar\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "MultiDiscriminator",
      "md_content": [],
      "code_start_line": 126,
      "code_end_line": 165,
      "params": [],
      "have_return": true,
      "code_content": "class MultiDiscriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(MultiDiscriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        channels, _, _ = input_shape\n        # Extracts discriminator models\n        self.models = nn.ModuleList()\n        for i in range(3):\n            self.models.add_module(\n                \"disc_%d\" % i,\n                nn.Sequential(\n                    *discriminator_block(channels, 64, normalize=False),\n                    *discriminator_block(64, 128),\n                    *discriminator_block(128, 256),\n                    *discriminator_block(256, 512),\n                    nn.Conv2d(512, 1, 3, padding=1)\n                ),\n            )\n\n        self.downsample = nn.AvgPool2d(in_channels, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def compute_loss(self, x, gt):\n        \"\"\"Computes the MSE between model output and scalar gt\"\"\"\n        loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n        return loss\n\n    def forward(self, x):\n        outputs = []\n        for m in self.models:\n            outputs.append(m(x))\n            x = self.downsample(x)\n        return outputs\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 127,
      "code_end_line": 153,
      "params": [
        "self",
        "input_shape"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, input_shape):\n        super(MultiDiscriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        channels, _, _ = input_shape\n        # Extracts discriminator models\n        self.models = nn.ModuleList()\n        for i in range(3):\n            self.models.add_module(\n                \"disc_%d\" % i,\n                nn.Sequential(\n                    *discriminator_block(channels, 64, normalize=False),\n                    *discriminator_block(64, 128),\n                    *discriminator_block(128, 256),\n                    *discriminator_block(256, 512),\n                    nn.Conv2d(512, 1, 3, padding=1)\n                ),\n            )\n\n        self.downsample = nn.AvgPool2d(in_channels, stride=2, padding=[1, 1], count_include_pad=False)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 130,
      "code_end_line": 136,
      "params": [
        "in_filters",
        "out_filters",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "compute_loss",
      "md_content": [],
      "code_start_line": 155,
      "code_end_line": 158,
      "params": [
        "self",
        "x",
        "gt"
      ],
      "have_return": true,
      "code_content": "    def compute_loss(self, x, gt):\n        \"\"\"Computes the MSE between model output and scalar gt\"\"\"\n        loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n        return loss\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 160,
      "code_end_line": 165,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        outputs = []\n        for m in self.models:\n            outputs.append(m(x))\n            x = self.downsample(x)\n        return outputs\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/bicyclegan/datasets.py": [
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 40,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, input_shape, mode=\"train\"):\n        self.transform = transforms.Compose(\n            [\n                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n            ]\n        )\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 21,
      "params": [
        "self",
        "root",
        "input_shape",
        "mode"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, input_shape, mode=\"train\"):\n        self.transform = transforms.Compose(\n            [\n                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n            ]\n        )\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 23,
      "code_end_line": 37,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 40,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/sgan/sgan.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 43,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 73,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.label_emb = nn.Embedding(opt.num_classes, opt.latent_dim)\n\n        self.init_size = opt.img_size // 4  # Initial size before upsampling\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, noise):\n        out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 47,
      "code_end_line": 67,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.label_emb = nn.Embedding(opt.num_classes, opt.latent_dim)\n\n        self.init_size = opt.img_size // 4  # Initial size before upsampling\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 69,
      "code_end_line": 73,
      "params": [
        "self",
        "noise"
      ],
      "have_return": true,
      "code_content": "    def forward(self, noise):\n        out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 76,
      "code_end_line": 107,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.conv_blocks = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n\n        # Output layers\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.num_classes + 1), nn.Softmax())\n\n    def forward(self, img):\n        out = self.conv_blocks(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        label = self.aux_layer(out)\n\n        return validity, label\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 77,
      "code_end_line": 99,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.conv_blocks = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n\n        # Output layers\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.num_classes + 1), nn.Softmax())\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 80,
      "code_end_line": 85,
      "params": [
        "in_filters",
        "out_filters",
        "bn"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 101,
      "code_end_line": 107,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.conv_blocks(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        label = self.aux_layer(out)\n\n        return validity, label\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/stargan/stargan.py": [
    {
      "type": "FunctionDef",
      "name": "criterion_cls",
      "md_content": [],
      "code_start_line": 76,
      "code_end_line": 77,
      "params": [
        "logit",
        "target"
      ],
      "have_return": true,
      "code_content": "def criterion_cls(logit, target):\n    return F.binary_cross_entropy_with_logits(logit, target, size_average=False) / logit.size(0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "compute_gradient_penalty",
      "md_content": [],
      "code_start_line": 142,
      "code_end_line": 161,
      "params": [
        "D",
        "real_samples",
        "fake_samples"
      ],
      "have_return": true,
      "code_content": "def compute_gradient_penalty(D, real_samples, fake_samples):\n    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n    # Random weight term for interpolation between real and fake samples\n    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n    # Get random interpolation between real and fake samples\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates, _ = D(interpolates)\n    fake = Variable(Tensor(np.ones(d_interpolates.shape)), requires_grad=False)\n    # Get gradient w.r.t. interpolates\n    gradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "sample_images",
      "md_content": [],
      "code_start_line": 173,
      "code_end_line": 197,
      "params": [
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_images(batches_done):\n    \"\"\"Saves a generated sample of domain translations\"\"\"\n    val_imgs, val_labels = next(iter(val_dataloader))\n    val_imgs = Variable(val_imgs.type(Tensor))\n    val_labels = Variable(val_labels.type(Tensor))\n    img_samples = None\n    for i in range(10):\n        img, label = val_imgs[i], val_labels[i]\n        # Repeat for number of label changes\n        imgs = img.repeat(c_dim, 1, 1, 1)\n        labels = label.repeat(c_dim, 1)\n        # Make changes to labels\n        for sample_i, changes in enumerate(label_changes):\n            for col, val in changes:\n                labels[sample_i, col] = 1 - labels[sample_i, col] if val == -1 else val\n\n        # Generate translations\n        gen_imgs = generator(imgs, labels)\n        # Concatenate images by width\n        gen_imgs = torch.cat([x for x in gen_imgs.data], -1)\n        img_sample = torch.cat((img.data, gen_imgs), -1)\n        # Add as row to generated samples\n        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n\n    save_image(img_samples.view(1, *img_samples.shape), \"images/%s.png\" % batches_done, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/stargan/models.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 9,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ResidualBlock",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 32,
      "params": [],
      "have_return": true,
      "code_content": "class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        conv_block = [\n            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n        ]\n\n        self.conv_block = nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 29,
      "params": [
        "self",
        "in_features"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        conv_block = [\n            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n        ]\n\n        self.conv_block = nn.Sequential(*conv_block)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 31,
      "code_end_line": 32,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return x + self.conv_block(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "GeneratorResNet",
      "md_content": [],
      "code_start_line": 35,
      "code_end_line": 79,
      "params": [],
      "have_return": true,
      "code_content": "class GeneratorResNet(nn.Module):\n    def __init__(self, img_shape=(3, 128, 128), res_blocks=9, c_dim=5):\n        super(GeneratorResNet, self).__init__()\n        channels, img_size, _ = img_shape\n\n        # Initial convolution block\n        model = [\n            nn.Conv2d(channels + c_dim, 64, 7, stride=1, padding=3, bias=False),\n            nn.InstanceNorm2d(64, affine=True, track_running_stats=True),\n            nn.ReLU(inplace=True),\n        ]\n\n        # Downsampling\n        curr_dim = 64\n        for _ in range(2):\n            model += [\n                nn.Conv2d(curr_dim, curr_dim * 2, 4, stride=2, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim * 2, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            ]\n            curr_dim *= 2\n\n        # Residual blocks\n        for _ in range(res_blocks):\n            model += [ResidualBlock(curr_dim)]\n\n        # Upsampling\n        for _ in range(2):\n            model += [\n                nn.ConvTranspose2d(curr_dim, curr_dim // 2, 4, stride=2, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim // 2, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            ]\n            curr_dim = curr_dim // 2\n\n        # Output layer\n        model += [nn.Conv2d(curr_dim, channels, 7, stride=1, padding=3), nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x, c):\n        c = c.view(c.size(0), c.size(1), 1, 1)\n        c = c.repeat(1, 1, x.size(2), x.size(3))\n        x = torch.cat((x, c), 1)\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 73,
      "params": [
        "self",
        "img_shape",
        "res_blocks",
        "c_dim"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, img_shape=(3, 128, 128), res_blocks=9, c_dim=5):\n        super(GeneratorResNet, self).__init__()\n        channels, img_size, _ = img_shape\n\n        # Initial convolution block\n        model = [\n            nn.Conv2d(channels + c_dim, 64, 7, stride=1, padding=3, bias=False),\n            nn.InstanceNorm2d(64, affine=True, track_running_stats=True),\n            nn.ReLU(inplace=True),\n        ]\n\n        # Downsampling\n        curr_dim = 64\n        for _ in range(2):\n            model += [\n                nn.Conv2d(curr_dim, curr_dim * 2, 4, stride=2, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim * 2, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            ]\n            curr_dim *= 2\n\n        # Residual blocks\n        for _ in range(res_blocks):\n            model += [ResidualBlock(curr_dim)]\n\n        # Upsampling\n        for _ in range(2):\n            model += [\n                nn.ConvTranspose2d(curr_dim, curr_dim // 2, 4, stride=2, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim // 2, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            ]\n            curr_dim = curr_dim // 2\n\n        # Output layer\n        model += [nn.Conv2d(curr_dim, channels, 7, stride=1, padding=3), nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 75,
      "code_end_line": 79,
      "params": [
        "self",
        "x",
        "c"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x, c):\n        c = c.view(c.size(0), c.size(1), 1, 1)\n        c = c.repeat(1, 1, x.size(2), x.size(3))\n        x = torch.cat((x, c), 1)\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 87,
      "code_end_line": 115,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, img_shape=(3, 128, 128), c_dim=5, n_strided=6):\n        super(Discriminator, self).__init__()\n        channels, img_size, _ = img_shape\n\n        def discriminator_block(in_filters, out_filters):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), nn.LeakyReLU(0.01)]\n            return layers\n\n        layers = discriminator_block(channels, 64)\n        curr_dim = 64\n        for _ in range(n_strided - 1):\n            layers.extend(discriminator_block(curr_dim, curr_dim * 2))\n            curr_dim *= 2\n\n        self.model = nn.Sequential(*layers)\n\n        # Output 1: PatchGAN\n        self.out1 = nn.Conv2d(curr_dim, 1, 3, padding=1, bias=False)\n        # Output 2: Class prediction\n        kernel_size = img_size // 2 ** n_strided\n        self.out2 = nn.Conv2d(curr_dim, c_dim, kernel_size, bias=False)\n\n    def forward(self, img):\n        feature_repr = self.model(img)\n        out_adv = self.out1(feature_repr)\n        out_cls = self.out2(feature_repr)\n        return out_adv, out_cls.view(out_cls.size(0), -1)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 88,
      "code_end_line": 109,
      "params": [
        "self",
        "img_shape",
        "c_dim",
        "n_strided"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, img_shape=(3, 128, 128), c_dim=5, n_strided=6):\n        super(Discriminator, self).__init__()\n        channels, img_size, _ = img_shape\n\n        def discriminator_block(in_filters, out_filters):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), nn.LeakyReLU(0.01)]\n            return layers\n\n        layers = discriminator_block(channels, 64)\n        curr_dim = 64\n        for _ in range(n_strided - 1):\n            layers.extend(discriminator_block(curr_dim, curr_dim * 2))\n            curr_dim *= 2\n\n        self.model = nn.Sequential(*layers)\n\n        # Output 1: PatchGAN\n        self.out1 = nn.Conv2d(curr_dim, 1, 3, padding=1, bias=False)\n        # Output 2: Class prediction\n        kernel_size = img_size // 2 ** n_strided\n        self.out2 = nn.Conv2d(curr_dim, c_dim, kernel_size, bias=False)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 92,
      "code_end_line": 95,
      "params": [
        "in_filters",
        "out_filters"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), nn.LeakyReLU(0.01)]\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 111,
      "code_end_line": 115,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        feature_repr = self.model(img)\n        out_adv = self.out1(feature_repr)\n        out_cls = self.out2(feature_repr)\n        return out_adv, out_cls.view(out_cls.size(0), -1)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/stargan/datasets.py": [
    {
      "type": "ClassDef",
      "name": "CelebADataset",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 46,
      "params": [],
      "have_return": true,
      "code_content": "class CelebADataset(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\", attributes=None):\n        self.transform = transforms.Compose(transforms_)\n\n        self.selected_attrs = attributes\n        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n        self.files = self.files[:-2000] if mode == \"train\" else self.files[-2000:]\n        self.label_path = glob.glob(\"%s/*.txt\" % root)[0]\n        self.annotations = self.get_annotations()\n\n    def get_annotations(self):\n        \"\"\"Extracts annotations for CelebA\"\"\"\n        annotations = {}\n        lines = [line.rstrip() for line in open(self.label_path, \"r\")]\n        self.label_names = lines[1].split()\n        for _, line in enumerate(lines[2:]):\n            filename, *values = line.split()\n            labels = []\n            for attr in self.selected_attrs:\n                idx = self.label_names.index(attr)\n                labels.append(1 * (values[idx] == \"1\"))\n            annotations[filename] = labels\n        return annotations\n\n    def __getitem__(self, index):\n        filepath = self.files[index % len(self.files)]\n        filename = filepath.split(\"/\")[-1]\n        img = self.transform(Image.open(filepath))\n        label = self.annotations[filename]\n        label = torch.FloatTensor(np.array(label))\n\n        return img, label\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 13,
      "code_end_line": 20,
      "params": [
        "self",
        "root",
        "transforms_",
        "mode",
        "attributes"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, transforms_=None, mode=\"train\", attributes=None):\n        self.transform = transforms.Compose(transforms_)\n\n        self.selected_attrs = attributes\n        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n        self.files = self.files[:-2000] if mode == \"train\" else self.files[-2000:]\n        self.label_path = glob.glob(\"%s/*.txt\" % root)[0]\n        self.annotations = self.get_annotations()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_annotations",
      "md_content": [],
      "code_start_line": 22,
      "code_end_line": 34,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_annotations(self):\n        \"\"\"Extracts annotations for CelebA\"\"\"\n        annotations = {}\n        lines = [line.rstrip() for line in open(self.label_path, \"r\")]\n        self.label_names = lines[1].split()\n        for _, line in enumerate(lines[2:]):\n            filename, *values = line.split()\n            labels = []\n            for attr in self.selected_attrs:\n                idx = self.label_names.index(attr)\n                labels.append(1 * (values[idx] == \"1\"))\n            annotations[filename] = labels\n        return annotations\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 43,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n        filepath = self.files[index % len(self.files)]\n        filename = filepath.split(\"/\")[-1]\n        img = self.transform(Image.open(filepath))\n        label = self.annotations[filename]\n        label = torch.FloatTensor(np.array(label))\n\n        return img, label\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 45,
      "code_end_line": 46,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/aae/aae.py": [
    {
      "type": "FunctionDef",
      "name": "reparameterization",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 43,
      "params": [
        "mu",
        "logvar"
      ],
      "have_return": true,
      "code_content": "def reparameterization(mu, logvar):\n    std = torch.exp(logvar / 2)\n    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), opt.latent_dim))))\n    z = sampled_z * std + mu\n    return z\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Encoder",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 67,
      "params": [],
      "have_return": true,
      "code_content": "class Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.mu = nn.Linear(512, opt.latent_dim)\n        self.logvar = nn.Linear(512, opt.latent_dim)\n\n    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        x = self.model(img_flat)\n        mu = self.mu(x)\n        logvar = self.logvar(x)\n        z = reparameterization(mu, logvar)\n        return z\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 47,
      "code_end_line": 59,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Encoder, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.mu = nn.Linear(512, opt.latent_dim)\n        self.logvar = nn.Linear(512, opt.latent_dim)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 61,
      "code_end_line": 67,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        x = self.model(img_flat)\n        mu = self.mu(x)\n        logvar = self.logvar(x)\n        z = reparameterization(mu, logvar)\n        return z\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Decoder",
      "md_content": [],
      "code_start_line": 70,
      "code_end_line": 87,
      "params": [],
      "have_return": true,
      "code_content": "class Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.latent_dim, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, int(np.prod(img_shape))),\n            nn.Tanh(),\n        )\n\n    def forward(self, z):\n        img_flat = self.model(z)\n        img = img_flat.view(img_flat.shape[0], *img_shape)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 71,
      "code_end_line": 82,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Decoder, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.latent_dim, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, int(np.prod(img_shape))),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 84,
      "code_end_line": 87,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        img_flat = self.model(z)\n        img = img_flat.view(img_flat.shape[0], *img_shape)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 90,
      "code_end_line": 105,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.latent_dim, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, z):\n        validity = self.model(z)\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 91,
      "code_end_line": 101,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.latent_dim, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 103,
      "code_end_line": 105,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        validity = self.model(z)\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "sample_image",
      "md_content": [],
      "code_start_line": 148,
      "code_end_line": 153,
      "params": [
        "n_row",
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_image(n_row, batches_done):\n    \"\"\"Saves a grid of generated digits\"\"\"\n    # Sample noise\n    z = Variable(Tensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n    gen_imgs = decoder(z)\n    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/pixelda/pixelda.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 45,
      "code_end_line": 51,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ResidualBlock",
      "md_content": [],
      "code_start_line": 54,
      "code_end_line": 67,
      "params": [],
      "have_return": true,
      "code_content": "class ResidualBlock(nn.Module):\n    def __init__(self, in_features=64, out_features=64):\n        super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, 3, 1, 1),\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, in_features, 3, 1, 1),\n            nn.BatchNorm2d(in_features),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 55,
      "code_end_line": 64,
      "params": [
        "self",
        "in_features",
        "out_features"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_features=64, out_features=64):\n        super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, 3, 1, 1),\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, in_features, 3, 1, 1),\n            nn.BatchNorm2d(in_features),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 66,
      "code_end_line": 67,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return x + self.block(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 70,
      "code_end_line": 92,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        # Fully-connected layer which constructs image channel shaped output from noise\n        self.fc = nn.Linear(opt.latent_dim, opt.channels * opt.img_size ** 2)\n\n        self.l1 = nn.Sequential(nn.Conv2d(opt.channels * 2, 64, 3, 1, 1), nn.ReLU(inplace=True))\n\n        resblocks = []\n        for _ in range(opt.n_residual_blocks):\n            resblocks.append(ResidualBlock())\n        self.resblocks = nn.Sequential(*resblocks)\n\n        self.l2 = nn.Sequential(nn.Conv2d(64, opt.channels, 3, 1, 1), nn.Tanh())\n\n    def forward(self, img, z):\n        gen_input = torch.cat((img, self.fc(z).view(*img.shape)), 1)\n        out = self.l1(gen_input)\n        out = self.resblocks(out)\n        img_ = self.l2(out)\n\n        return img_\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 71,
      "code_end_line": 84,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        # Fully-connected layer which constructs image channel shaped output from noise\n        self.fc = nn.Linear(opt.latent_dim, opt.channels * opt.img_size ** 2)\n\n        self.l1 = nn.Sequential(nn.Conv2d(opt.channels * 2, 64, 3, 1, 1), nn.ReLU(inplace=True))\n\n        resblocks = []\n        for _ in range(opt.n_residual_blocks):\n            resblocks.append(ResidualBlock())\n        self.resblocks = nn.Sequential(*resblocks)\n\n        self.l2 = nn.Sequential(nn.Conv2d(64, opt.channels, 3, 1, 1), nn.Tanh())\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 86,
      "code_end_line": 92,
      "params": [
        "self",
        "img",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img, z):\n        gen_input = torch.cat((img, self.fc(z).view(*img.shape)), 1)\n        out = self.l1(gen_input)\n        out = self.resblocks(out)\n        img_ = self.l2(out)\n\n        return img_\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 95,
      "code_end_line": 117,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def block(in_features, out_features, normalization=True):\n            \"\"\"Discriminator block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_features))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.channels, 64, normalization=False),\n            *block(64, 128),\n            *block(128, 256),\n            *block(256, 512),\n            nn.Conv2d(512, 1, 3, 1, 1)\n        )\n\n    def forward(self, img):\n        validity = self.model(img)\n\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 96,
      "code_end_line": 112,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def block(in_features, out_features, normalization=True):\n            \"\"\"Discriminator block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_features))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.channels, 64, normalization=False),\n            *block(64, 128),\n            *block(128, 256),\n            *block(256, 512),\n            nn.Conv2d(512, 1, 3, 1, 1)\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 99,
      "code_end_line": 104,
      "params": [
        "in_features",
        "out_features",
        "normalization"
      ],
      "have_return": true,
      "code_content": "        def block(in_features, out_features, normalization=True):\n            \"\"\"Discriminator block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_features))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 114,
      "code_end_line": 117,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        validity = self.model(img)\n\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Classifier",
      "md_content": [],
      "code_start_line": 120,
      "code_end_line": 142,
      "params": [],
      "have_return": true,
      "code_content": "class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n\n        def block(in_features, out_features, normalization=True):\n            \"\"\"Classifier block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_features))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.channels, 64, normalization=False), *block(64, 128), *block(128, 256), *block(256, 512)\n        )\n\n        input_size = opt.img_size // 2 ** 4\n        self.output_layer = nn.Sequential(nn.Linear(512 * input_size ** 2, opt.n_classes), nn.Softmax())\n\n    def forward(self, img):\n        feature_repr = self.model(img)\n        feature_repr = feature_repr.view(feature_repr.size(0), -1)\n        label = self.output_layer(feature_repr)\n        return label\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 121,
      "code_end_line": 136,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Classifier, self).__init__()\n\n        def block(in_features, out_features, normalization=True):\n            \"\"\"Classifier block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_features))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.channels, 64, normalization=False), *block(64, 128), *block(128, 256), *block(256, 512)\n        )\n\n        input_size = opt.img_size // 2 ** 4\n        self.output_layer = nn.Sequential(nn.Linear(512 * input_size ** 2, opt.n_classes), nn.Softmax())\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 124,
      "code_end_line": 129,
      "params": [
        "in_features",
        "out_features",
        "normalization"
      ],
      "have_return": true,
      "code_content": "        def block(in_features, out_features, normalization=True):\n            \"\"\"Classifier block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_features))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 138,
      "code_end_line": 142,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        feature_repr = self.model(img)\n        feature_repr = feature_repr.view(feature_repr.size(0), -1)\n        label = self.output_layer(feature_repr)\n        return label\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/pixelda/mnistm.py": [
    {
      "type": "ClassDef",
      "name": "MNISTM",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 165,
      "params": [],
      "have_return": true,
      "code_content": "class MNISTM(data.Dataset):\n    \"\"\"`MNIST-M Dataset.\"\"\"\n\n    url = \"https://github.com/VanushVaswani/keras_mnistm/releases/download/1.0/keras_mnistm.pkl.gz\"\n\n    raw_folder = 'raw'\n    processed_folder = 'processed'\n    training_file = 'mnist_m_train.pt'\n    test_file = 'mnist_m_test.pt'\n\n    def __init__(self,\n                 root, mnist_root=\"data\",\n                 train=True,\n                 transform=None, target_transform=None,\n                 download=False):\n        \"\"\"Init MNIST-M dataset.\"\"\"\n        super(MNISTM, self).__init__()\n        self.root = os.path.expanduser(root)\n        self.mnist_root = os.path.expanduser(mnist_root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train  # training set or test set\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError('Dataset not found.' +\n                               ' You can use download=True to download it')\n\n        if self.train:\n            self.train_data, self.train_labels = \\\n                torch.load(os.path.join(self.root,\n                                        self.processed_folder,\n                                        self.training_file))\n        else:\n            self.test_data, self.test_labels = \\\n                torch.load(os.path.join(self.root,\n                                        self.processed_folder,\n                                        self.test_file))\n\n    def __getitem__(self, index):\n        \"\"\"Get images and target for data loader.\n\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"\n        if self.train:\n            img, target = self.train_data[index], self.train_labels[index]\n        else:\n            img, target = self.test_data[index], self.test_labels[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(img.squeeze().numpy(), mode='RGB')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        \"\"\"Return size of dataset.\"\"\"\n        if self.train:\n            return len(self.train_data)\n        else:\n            return len(self.test_data)\n\n    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root,\n                                           self.processed_folder,\n                                           self.training_file)) and \\\n            os.path.exists(os.path.join(self.root,\n                                        self.processed_folder,\n                                        self.test_file))\n\n    def download(self):\n        \"\"\"Download the MNIST data.\"\"\"\n        # import essential packages\n        from six.moves import urllib\n        import gzip\n        import pickle\n        from torchvision import datasets\n\n        # check if dataset already exists\n        if self._check_exists():\n            return\n\n        # make data dirs\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        # download pkl files\n        print('Downloading ' + self.url)\n        filename = self.url.rpartition('/')[2]\n        file_path = os.path.join(self.root, self.raw_folder, filename)\n        if not os.path.exists(file_path.replace('.gz', '')):\n            data = urllib.request.urlopen(self.url)\n            with open(file_path, 'wb') as f:\n                f.write(data.read())\n            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n                    gzip.GzipFile(file_path) as zip_f:\n                out_f.write(zip_f.read())\n            os.unlink(file_path)\n\n        # process and save as torch files\n        print('Processing...')\n\n        # load MNIST-M images from pkl file\n        with open(file_path.replace('.gz', ''), \"rb\") as f:\n            mnist_m_data = pickle.load(f, encoding='bytes')\n        mnist_m_train_data = torch.ByteTensor(mnist_m_data[b'train'])\n        mnist_m_test_data = torch.ByteTensor(mnist_m_data[b'test'])\n\n        # get MNIST labels\n        mnist_train_labels = datasets.MNIST(root=self.mnist_root,\n                                            train=True,\n                                            download=True).train_labels\n        mnist_test_labels = datasets.MNIST(root=self.mnist_root,\n                                           train=False,\n                                           download=True).test_labels\n\n        # save MNIST-M dataset\n        training_set = (mnist_m_train_data, mnist_train_labels)\n        test_set = (mnist_m_test_data, mnist_test_labels)\n        with open(os.path.join(self.root,\n                               self.processed_folder,\n                               self.training_file), 'wb') as f:\n            torch.save(training_set, f)\n        with open(os.path.join(self.root,\n                               self.processed_folder,\n                               self.test_file), 'wb') as f:\n            torch.save(test_set, f)\n\n        print('Done!')\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 29,
      "code_end_line": 58,
      "params": [
        "self",
        "root",
        "mnist_root",
        "train",
        "transform",
        "target_transform",
        "download"
      ],
      "have_return": false,
      "code_content": "    def __init__(self,\n                 root, mnist_root=\"data\",\n                 train=True,\n                 transform=None, target_transform=None,\n                 download=False):\n        \"\"\"Init MNIST-M dataset.\"\"\"\n        super(MNISTM, self).__init__()\n        self.root = os.path.expanduser(root)\n        self.mnist_root = os.path.expanduser(mnist_root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train  # training set or test set\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError('Dataset not found.' +\n                               ' You can use download=True to download it')\n\n        if self.train:\n            self.train_data, self.train_labels = \\\n                torch.load(os.path.join(self.root,\n                                        self.processed_folder,\n                                        self.training_file))\n        else:\n            self.test_data, self.test_labels = \\\n                torch.load(os.path.join(self.root,\n                                        self.processed_folder,\n                                        self.test_file))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 60,
      "code_end_line": 84,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n        \"\"\"Get images and target for data loader.\n\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"\n        if self.train:\n            img, target = self.train_data[index], self.train_labels[index]\n        else:\n            img, target = self.test_data[index], self.test_labels[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(img.squeeze().numpy(), mode='RGB')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 86,
      "code_end_line": 91,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        \"\"\"Return size of dataset.\"\"\"\n        if self.train:\n            return len(self.train_data)\n        else:\n            return len(self.test_data)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "_check_exists",
      "md_content": [],
      "code_start_line": 93,
      "code_end_line": 99,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root,\n                                           self.processed_folder,\n                                           self.training_file)) and \\\n            os.path.exists(os.path.join(self.root,\n                                        self.processed_folder,\n                                        self.test_file))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "download",
      "md_content": [],
      "code_start_line": 101,
      "code_end_line": 165,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def download(self):\n        \"\"\"Download the MNIST data.\"\"\"\n        # import essential packages\n        from six.moves import urllib\n        import gzip\n        import pickle\n        from torchvision import datasets\n\n        # check if dataset already exists\n        if self._check_exists():\n            return\n\n        # make data dirs\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        # download pkl files\n        print('Downloading ' + self.url)\n        filename = self.url.rpartition('/')[2]\n        file_path = os.path.join(self.root, self.raw_folder, filename)\n        if not os.path.exists(file_path.replace('.gz', '')):\n            data = urllib.request.urlopen(self.url)\n            with open(file_path, 'wb') as f:\n                f.write(data.read())\n            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n                    gzip.GzipFile(file_path) as zip_f:\n                out_f.write(zip_f.read())\n            os.unlink(file_path)\n\n        # process and save as torch files\n        print('Processing...')\n\n        # load MNIST-M images from pkl file\n        with open(file_path.replace('.gz', ''), \"rb\") as f:\n            mnist_m_data = pickle.load(f, encoding='bytes')\n        mnist_m_train_data = torch.ByteTensor(mnist_m_data[b'train'])\n        mnist_m_test_data = torch.ByteTensor(mnist_m_data[b'test'])\n\n        # get MNIST labels\n        mnist_train_labels = datasets.MNIST(root=self.mnist_root,\n                                            train=True,\n                                            download=True).train_labels\n        mnist_test_labels = datasets.MNIST(root=self.mnist_root,\n                                           train=False,\n                                           download=True).test_labels\n\n        # save MNIST-M dataset\n        training_set = (mnist_m_train_data, mnist_train_labels)\n        test_set = (mnist_m_test_data, mnist_test_labels)\n        with open(os.path.join(self.root,\n                               self.processed_folder,\n                               self.training_file), 'wb') as f:\n            torch.save(training_set, f)\n        with open(os.path.join(self.root,\n                               self.processed_folder,\n                               self.test_file), 'wb') as f:\n            torch.save(test_set, f)\n\n        print('Done!')\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/wgan/wgan.py": [
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 62,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 40,
      "code_end_line": 57,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 43,
      "code_end_line": 48,
      "params": [
        "in_feat",
        "out_feat",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 59,
      "code_end_line": 62,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 65,
      "code_end_line": 80,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 66,
      "code_end_line": 75,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 77,
      "code_end_line": 80,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/pix2pix/pix2pix.py": [
    {
      "type": "FunctionDef",
      "name": "sample_images",
      "md_content": [],
      "code_start_line": 107,
      "code_end_line": 114,
      "params": [
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_images(batches_done):\n    \"\"\"Saves a generated sample from the validation set\"\"\"\n    imgs = next(iter(val_dataloader))\n    real_A = Variable(imgs[\"B\"].type(Tensor))\n    real_B = Variable(imgs[\"A\"].type(Tensor))\n    fake_B = generator(real_A)\n    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/pix2pix/models.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 12,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "UNetDown",
      "md_content": [],
      "code_start_line": 20,
      "code_end_line": 32,
      "params": [],
      "have_return": true,
      "code_content": "class UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 21,
      "code_end_line": 29,
      "params": [
        "self",
        "in_size",
        "out_size",
        "normalize",
        "dropout"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 31,
      "code_end_line": 32,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "UNetUp",
      "md_content": [],
      "code_start_line": 35,
      "code_end_line": 52,
      "params": [],
      "have_return": true,
      "code_content": "class UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n            nn.InstanceNorm2d(out_size),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 46,
      "params": [
        "self",
        "in_size",
        "out_size",
        "dropout"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n            nn.InstanceNorm2d(out_size),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 48,
      "code_end_line": 52,
      "params": [
        "self",
        "x",
        "skip_input"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "GeneratorUNet",
      "md_content": [],
      "code_start_line": 55,
      "code_end_line": 101,
      "params": [],
      "have_return": true,
      "code_content": "class GeneratorUNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3):\n        super(GeneratorUNet, self).__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(128, out_channels, 4, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        # U-Net generator with skip connections from encoder to decoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        return self.final(u7)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 56,
      "code_end_line": 81,
      "params": [
        "self",
        "in_channels",
        "out_channels"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_channels=3, out_channels=3):\n        super(GeneratorUNet, self).__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(128, out_channels, 4, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 83,
      "code_end_line": 101,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        # U-Net generator with skip connections from encoder to decoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        return self.final(u7)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 109,
      "code_end_line": 133,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels * 2, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n        )\n\n    def forward(self, img_A, img_B):\n        # Concatenate image and condition image by channels to produce input\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 110,
      "code_end_line": 128,
      "params": [
        "self",
        "in_channels"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels * 2, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 113,
      "code_end_line": 119,
      "params": [
        "in_filters",
        "out_filters",
        "normalization"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 130,
      "code_end_line": 133,
      "params": [
        "self",
        "img_A",
        "img_B"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img_A, img_B):\n        # Concatenate image and condition image by channels to produce input\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/pix2pix/datasets.py": [
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 36,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n        if mode == \"train\":\n            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 17,
      "params": [
        "self",
        "root",
        "transforms_",
        "mode"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n        if mode == \"train\":\n            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 33,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 35,
      "code_end_line": 36,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/discogan/discogan.py": [
    {
      "type": "FunctionDef",
      "name": "sample_images",
      "md_content": [],
      "code_start_line": 112,
      "code_end_line": 122,
      "params": [
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_images(batches_done):\n    \"\"\"Saves a generated sample from the validation set\"\"\"\n    imgs = next(iter(val_dataloader))\n    G_AB.eval()\n    G_BA.eval()\n    real_A = Variable(imgs[\"A\"].type(Tensor))\n    fake_B = G_AB(real_A)\n    real_B = Variable(imgs[\"B\"].type(Tensor))\n    fake_A = G_BA(real_B)\n    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data, fake_A.data), 0)\n    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=8, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/discogan/models.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 12,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "UNetDown",
      "md_content": [],
      "code_start_line": 20,
      "code_end_line": 32,
      "params": [],
      "have_return": true,
      "code_content": "class UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 21,
      "code_end_line": 29,
      "params": [
        "self",
        "in_size",
        "out_size",
        "normalize",
        "dropout"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 31,
      "code_end_line": 32,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "UNetUp",
      "md_content": [],
      "code_start_line": 35,
      "code_end_line": 48,
      "params": [],
      "have_return": true,
      "code_content": "class UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [nn.ConvTranspose2d(in_size, out_size, 4, 2, 1), nn.InstanceNorm2d(out_size), nn.ReLU(inplace=True)]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 42,
      "params": [
        "self",
        "in_size",
        "out_size",
        "dropout"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [nn.ConvTranspose2d(in_size, out_size, 4, 2, 1), nn.InstanceNorm2d(out_size), nn.ReLU(inplace=True)]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 44,
      "code_end_line": 48,
      "params": [
        "self",
        "x",
        "skip_input"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "GeneratorUNet",
      "md_content": [],
      "code_start_line": 51,
      "code_end_line": 86,
      "params": [],
      "have_return": true,
      "code_content": "class GeneratorUNet(nn.Module):\n    def __init__(self, input_shape):\n        super(GeneratorUNet, self).__init__()\n        channels, _, _ = input_shape\n        self.down1 = UNetDown(channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256, dropout=0.5)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5, normalize=False)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 256, dropout=0.5)\n        self.up4 = UNetUp(512, 128)\n        self.up5 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2), nn.ZeroPad2d((1, 0, 1, 0)), nn.Conv2d(128, channels, 4, padding=1), nn.Tanh()\n        )\n\n    def forward(self, x):\n        # U-Net generator with skip connections from encoder to decoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        u1 = self.up1(d6, d5)\n        u2 = self.up2(u1, d4)\n        u3 = self.up3(u2, d3)\n        u4 = self.up4(u3, d2)\n        u5 = self.up5(u4, d1)\n\n        return self.final(u5)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 52,
      "code_end_line": 70,
      "params": [
        "self",
        "input_shape"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, input_shape):\n        super(GeneratorUNet, self).__init__()\n        channels, _, _ = input_shape\n        self.down1 = UNetDown(channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256, dropout=0.5)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5, normalize=False)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 256, dropout=0.5)\n        self.up4 = UNetUp(512, 128)\n        self.up5 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2), nn.ZeroPad2d((1, 0, 1, 0)), nn.Conv2d(128, channels, 4, padding=1), nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 72,
      "code_end_line": 86,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        # U-Net generator with skip connections from encoder to decoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        u1 = self.up1(d6, d5)\n        u2 = self.up2(u1, d4)\n        u3 = self.up3(u2, d3)\n        u4 = self.up4(u3, d2)\n        u5 = self.up5(u4, d1)\n\n        return self.final(u5)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 94,
      "code_end_line": 120,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n        # Calculate output of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 3, width // 2 ** 3)\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(256, 1, 4, padding=1)\n        )\n\n    def forward(self, img):\n        # Concatenate image and condition image by channels to produce input\n        return self.model(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 95,
      "code_end_line": 116,
      "params": [
        "self",
        "input_shape"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n        # Calculate output of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 3, width // 2 ** 3)\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(256, 1, 4, padding=1)\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 102,
      "code_end_line": 108,
      "params": [
        "in_filters",
        "out_filters",
        "normalization"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 118,
      "code_end_line": 120,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        # Concatenate image and condition image by channels to produce input\n        return self.model(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/discogan/datasets.py": [
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 34,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, mode='train'):\n        self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + '/*.*'))\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w/2, h))\n        img_B = img.crop((w/2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], 'RGB')\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], 'RGB')\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {'A': img_A, 'B': img_B}\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 15,
      "params": [
        "self",
        "root",
        "transforms_",
        "mode"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, transforms_=None, mode='train'):\n        self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + '/*.*'))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 31,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w/2, h))\n        img_B = img.crop((w/2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], 'RGB')\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], 'RGB')\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {'A': img_A, 'B': img_B}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 33,
      "code_end_line": 34,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/unit/unit.py": [
    {
      "type": "FunctionDef",
      "name": "sample_images",
      "md_content": [],
      "code_start_line": 150,
      "code_end_line": 160,
      "params": [
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_images(batches_done):\n    \"\"\"Saves a generated sample from the test set\"\"\"\n    imgs = next(iter(val_dataloader))\n    X1 = Variable(imgs[\"A\"].type(Tensor))\n    X2 = Variable(imgs[\"B\"].type(Tensor))\n    _, Z1 = E1(X1)\n    _, Z2 = E2(X2)\n    fake_X1 = G1(Z2)\n    fake_X2 = G2(Z1)\n    img_sample = torch.cat((X1.data, fake_X2.data, X2.data, fake_X1.data), 0)\n    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "compute_kl",
      "md_content": [],
      "code_start_line": 163,
      "code_end_line": 166,
      "params": [
        "mu"
      ],
      "have_return": true,
      "code_content": "def compute_kl(mu):\n    mu_2 = torch.pow(mu, 2)\n    loss = torch.mean(mu_2)\n    return loss\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/unit/models.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 8,
      "code_end_line": 14,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "LambdaLR",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 25,
      "params": [],
      "have_return": true,
      "code_content": "class LambdaLR:\n    def __init__(self, n_epochs, offset, decay_start_epoch):\n        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch\n\n    def step(self, epoch):\n        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 22,
      "params": [
        "self",
        "n_epochs",
        "offset",
        "decay_start_epoch"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, n_epochs, offset, decay_start_epoch):\n        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "step",
      "md_content": [],
      "code_start_line": 24,
      "code_end_line": 25,
      "params": [
        "self",
        "epoch"
      ],
      "have_return": true,
      "code_content": "    def step(self, epoch):\n        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ResidualBlock",
      "md_content": [],
      "code_start_line": 33,
      "code_end_line": 50,
      "params": [],
      "have_return": true,
      "code_content": "class ResidualBlock(nn.Module):\n    def __init__(self, features):\n        super(ResidualBlock, self).__init__()\n\n        conv_block = [\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            nn.InstanceNorm2d(features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            nn.InstanceNorm2d(features),\n        ]\n\n        self.conv_block = nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 34,
      "code_end_line": 47,
      "params": [
        "self",
        "features"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, features):\n        super(ResidualBlock, self).__init__()\n\n        conv_block = [\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            nn.InstanceNorm2d(features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            nn.InstanceNorm2d(features),\n        ]\n\n        self.conv_block = nn.Sequential(*conv_block)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 49,
      "code_end_line": 50,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return x + self.conv_block(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Encoder",
      "md_content": [],
      "code_start_line": 53,
      "code_end_line": 90,
      "params": [],
      "have_return": true,
      "code_content": "class Encoder(nn.Module):\n    def __init__(self, in_channels=3, dim=64, n_downsample=2, shared_block=None):\n        super(Encoder, self).__init__()\n\n        # Initial convolution block\n        layers = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels, dim, 7),\n            nn.InstanceNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n        ]\n\n        # Downsampling\n        for _ in range(n_downsample):\n            layers += [\n                nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1),\n                nn.InstanceNorm2d(dim * 2),\n                nn.ReLU(inplace=True),\n            ]\n            dim *= 2\n\n        # Residual blocks\n        for _ in range(3):\n            layers += [ResidualBlock(dim)]\n\n        self.model_blocks = nn.Sequential(*layers)\n        self.shared_block = shared_block\n\n    def reparameterization(self, mu):\n        Tensor = torch.cuda.FloatTensor if mu.is_cuda else torch.FloatTensor\n        z = Variable(Tensor(np.random.normal(0, 1, mu.shape)))\n        return z + mu\n\n    def forward(self, x):\n        x = self.model_blocks(x)\n        mu = self.shared_block(x)\n        z = self.reparameterization(mu)\n        return mu, z\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 54,
      "code_end_line": 79,
      "params": [
        "self",
        "in_channels",
        "dim",
        "n_downsample",
        "shared_block"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_channels=3, dim=64, n_downsample=2, shared_block=None):\n        super(Encoder, self).__init__()\n\n        # Initial convolution block\n        layers = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels, dim, 7),\n            nn.InstanceNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n        ]\n\n        # Downsampling\n        for _ in range(n_downsample):\n            layers += [\n                nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1),\n                nn.InstanceNorm2d(dim * 2),\n                nn.ReLU(inplace=True),\n            ]\n            dim *= 2\n\n        # Residual blocks\n        for _ in range(3):\n            layers += [ResidualBlock(dim)]\n\n        self.model_blocks = nn.Sequential(*layers)\n        self.shared_block = shared_block\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "reparameterization",
      "md_content": [],
      "code_start_line": 81,
      "code_end_line": 84,
      "params": [
        "self",
        "mu"
      ],
      "have_return": true,
      "code_content": "    def reparameterization(self, mu):\n        Tensor = torch.cuda.FloatTensor if mu.is_cuda else torch.FloatTensor\n        z = Variable(Tensor(np.random.normal(0, 1, mu.shape)))\n        return z + mu\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 86,
      "code_end_line": 90,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        x = self.model_blocks(x)\n        mu = self.shared_block(x)\n        z = self.reparameterization(mu)\n        return mu, z\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 93,
      "code_end_line": 122,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self, out_channels=3, dim=64, n_upsample=2, shared_block=None):\n        super(Generator, self).__init__()\n\n        self.shared_block = shared_block\n\n        layers = []\n        dim = dim * 2 ** n_upsample\n        # Residual blocks\n        for _ in range(3):\n            layers += [ResidualBlock(dim)]\n\n        # Upsampling\n        for _ in range(n_upsample):\n            layers += [\n                nn.ConvTranspose2d(dim, dim // 2, 4, stride=2, padding=1),\n                nn.InstanceNorm2d(dim // 2),\n                nn.LeakyReLU(0.2, inplace=True),\n            ]\n            dim = dim // 2\n\n        # Output layer\n        layers += [nn.ReflectionPad2d(3), nn.Conv2d(dim, out_channels, 7), nn.Tanh()]\n\n        self.model_blocks = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.shared_block(x)\n        x = self.model_blocks(x)\n        return x\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 94,
      "code_end_line": 117,
      "params": [
        "self",
        "out_channels",
        "dim",
        "n_upsample",
        "shared_block"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, out_channels=3, dim=64, n_upsample=2, shared_block=None):\n        super(Generator, self).__init__()\n\n        self.shared_block = shared_block\n\n        layers = []\n        dim = dim * 2 ** n_upsample\n        # Residual blocks\n        for _ in range(3):\n            layers += [ResidualBlock(dim)]\n\n        # Upsampling\n        for _ in range(n_upsample):\n            layers += [\n                nn.ConvTranspose2d(dim, dim // 2, 4, stride=2, padding=1),\n                nn.InstanceNorm2d(dim // 2),\n                nn.LeakyReLU(0.2, inplace=True),\n            ]\n            dim = dim // 2\n\n        # Output layer\n        layers += [nn.ReflectionPad2d(3), nn.Conv2d(dim, out_channels, 7), nn.Tanh()]\n\n        self.model_blocks = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 119,
      "code_end_line": 122,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        x = self.shared_block(x)\n        x = self.model_blocks(x)\n        return x\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 130,
      "code_end_line": 154,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n        channels, height, width = input_shape\n        # Calculate output of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.Conv2d(512, 1, 3, padding=1)\n        )\n\n    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 131,
      "code_end_line": 151,
      "params": [
        "self",
        "input_shape"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n        channels, height, width = input_shape\n        # Calculate output of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.Conv2d(512, 1, 3, padding=1)\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 137,
      "code_end_line": 143,
      "params": [
        "in_filters",
        "out_filters",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 153,
      "code_end_line": 154,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/unit/datasets.py": [
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 29,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.unaligned = unaligned\n\n        self.files_A = sorted(glob.glob(os.path.join(root, \"%s/A\" % mode) + \"/*.*\"))\n        self.files_B = sorted(glob.glob(os.path.join(root, \"%s/B\" % mode) + \"/*.*\"))\n\n    def __getitem__(self, index):\n        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n\n        if self.unaligned:\n            item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n        else:\n            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n\n        return {\"A\": item_A, \"B\": item_B}\n\n    def __len__(self):\n        return max(len(self.files_A), len(self.files_B))\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 16,
      "params": [
        "self",
        "root",
        "transforms_",
        "unaligned",
        "mode"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.unaligned = unaligned\n\n        self.files_A = sorted(glob.glob(os.path.join(root, \"%s/A\" % mode) + \"/*.*\"))\n        self.files_B = sorted(glob.glob(os.path.join(root, \"%s/B\" % mode) + \"/*.*\"))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 26,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n\n        if self.unaligned:\n            item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n        else:\n            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n\n        return {\"A\": item_A, \"B\": item_B}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 28,
      "code_end_line": 29,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return max(len(self.files_A), len(self.files_B))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/began/began.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 38,
      "code_end_line": 44,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 47,
      "code_end_line": 72,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, noise):\n        out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 48,
      "code_end_line": 66,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 68,
      "code_end_line": 72,
      "params": [
        "self",
        "noise"
      ],
      "have_return": true,
      "code_content": "    def forward(self, noise):\n        out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 75,
      "code_end_line": 99,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        # Upsampling\n        self.down = nn.Sequential(nn.Conv2d(opt.channels, 64, 3, 2, 1), nn.ReLU())\n        # Fully-connected layers\n        self.down_size = opt.img_size // 2\n        down_dim = 64 * (opt.img_size // 2) ** 2\n        self.fc = nn.Sequential(\n            nn.Linear(down_dim, 32),\n            nn.BatchNorm1d(32, 0.8),\n            nn.ReLU(inplace=True),\n            nn.Linear(32, down_dim),\n            nn.BatchNorm1d(down_dim),\n            nn.ReLU(inplace=True),\n        )\n        # Upsampling\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(64, opt.channels, 3, 1, 1))\n\n    def forward(self, img):\n        out = self.down(img)\n        out = self.fc(out.view(out.size(0), -1))\n        out = self.up(out.view(out.size(0), 64, self.down_size, self.down_size))\n        return out\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 76,
      "code_end_line": 93,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        # Upsampling\n        self.down = nn.Sequential(nn.Conv2d(opt.channels, 64, 3, 2, 1), nn.ReLU())\n        # Fully-connected layers\n        self.down_size = opt.img_size // 2\n        down_dim = 64 * (opt.img_size // 2) ** 2\n        self.fc = nn.Sequential(\n            nn.Linear(down_dim, 32),\n            nn.BatchNorm1d(32, 0.8),\n            nn.ReLU(inplace=True),\n            nn.Linear(32, down_dim),\n            nn.BatchNorm1d(down_dim),\n            nn.ReLU(inplace=True),\n        )\n        # Upsampling\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(64, opt.channels, 3, 1, 1))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 95,
      "code_end_line": 99,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.down(img)\n        out = self.fc(out.view(out.size(0), -1))\n        out = self.up(out.view(out.size(0), 64, self.down_size, self.down_size))\n        return out\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/esrgan/esrgan.py": [],
  "implementations/esrgan/test_on_image.py": [],
  "implementations/esrgan/models.py": [
    {
      "type": "ClassDef",
      "name": "FeatureExtractor",
      "md_content": [],
      "code_start_line": 8,
      "code_end_line": 15,
      "params": [],
      "have_return": true,
      "code_content": "class FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.vgg19_54 = nn.Sequential(*list(vgg19_model.features.children())[:35])\n\n    def forward(self, img):\n        return self.vgg19_54(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 9,
      "code_end_line": 12,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.vgg19_54 = nn.Sequential(*list(vgg19_model.features.children())[:35])\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 14,
      "code_end_line": 15,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        return self.vgg19_54(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "DenseResidualBlock",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 45,
      "params": [],
      "have_return": true,
      "code_content": "class DenseResidualBlock(nn.Module):\n    \"\"\"\n    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n    \"\"\"\n\n    def __init__(self, filters, res_scale=0.2):\n        super(DenseResidualBlock, self).__init__()\n        self.res_scale = res_scale\n\n        def block(in_features, non_linearity=True):\n            layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias=True)]\n            if non_linearity:\n                layers += [nn.LeakyReLU()]\n            return nn.Sequential(*layers)\n\n        self.b1 = block(in_features=1 * filters)\n        self.b2 = block(in_features=2 * filters)\n        self.b3 = block(in_features=3 * filters)\n        self.b4 = block(in_features=4 * filters)\n        self.b5 = block(in_features=5 * filters, non_linearity=False)\n        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n\n    def forward(self, x):\n        inputs = x\n        for block in self.blocks:\n            out = block(inputs)\n            inputs = torch.cat([inputs, out], 1)\n        return out.mul(self.res_scale) + x\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 23,
      "code_end_line": 38,
      "params": [
        "self",
        "filters",
        "res_scale"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, filters, res_scale=0.2):\n        super(DenseResidualBlock, self).__init__()\n        self.res_scale = res_scale\n\n        def block(in_features, non_linearity=True):\n            layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias=True)]\n            if non_linearity:\n                layers += [nn.LeakyReLU()]\n            return nn.Sequential(*layers)\n\n        self.b1 = block(in_features=1 * filters)\n        self.b2 = block(in_features=2 * filters)\n        self.b3 = block(in_features=3 * filters)\n        self.b4 = block(in_features=4 * filters)\n        self.b5 = block(in_features=5 * filters, non_linearity=False)\n        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 27,
      "code_end_line": 31,
      "params": [
        "in_features",
        "non_linearity"
      ],
      "have_return": true,
      "code_content": "        def block(in_features, non_linearity=True):\n            layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias=True)]\n            if non_linearity:\n                layers += [nn.LeakyReLU()]\n            return nn.Sequential(*layers)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 40,
      "code_end_line": 45,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        inputs = x\n        for block in self.blocks:\n            out = block(inputs)\n            inputs = torch.cat([inputs, out], 1)\n        return out.mul(self.res_scale) + x\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ResidualInResidualDenseBlock",
      "md_content": [],
      "code_start_line": 48,
      "code_end_line": 57,
      "params": [],
      "have_return": true,
      "code_content": "class ResidualInResidualDenseBlock(nn.Module):\n    def __init__(self, filters, res_scale=0.2):\n        super(ResidualInResidualDenseBlock, self).__init__()\n        self.res_scale = res_scale\n        self.dense_blocks = nn.Sequential(\n            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)\n        )\n\n    def forward(self, x):\n        return self.dense_blocks(x).mul(self.res_scale) + x\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 49,
      "code_end_line": 54,
      "params": [
        "self",
        "filters",
        "res_scale"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, filters, res_scale=0.2):\n        super(ResidualInResidualDenseBlock, self).__init__()\n        self.res_scale = res_scale\n        self.dense_blocks = nn.Sequential(\n            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 56,
      "code_end_line": 57,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.dense_blocks(x).mul(self.res_scale) + x\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "GeneratorRRDB",
      "md_content": [],
      "code_start_line": 60,
      "code_end_line": 93,
      "params": [],
      "have_return": true,
      "code_content": "class GeneratorRRDB(nn.Module):\n    def __init__(self, channels, filters=64, num_res_blocks=16, num_upsample=2):\n        super(GeneratorRRDB, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Conv2d(channels, filters, kernel_size=3, stride=1, padding=1)\n        # Residual blocks\n        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n        # Upsampling layers\n        upsample_layers = []\n        for _ in range(num_upsample):\n            upsample_layers += [\n                nn.Conv2d(filters, filters * 4, kernel_size=3, stride=1, padding=1),\n                nn.LeakyReLU(),\n                nn.PixelShuffle(upscale_factor=2),\n            ]\n        self.upsampling = nn.Sequential(*upsample_layers)\n        # Final output block\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(filters, channels, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 61,
      "code_end_line": 84,
      "params": [
        "self",
        "channels",
        "filters",
        "num_res_blocks",
        "num_upsample"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, channels, filters=64, num_res_blocks=16, num_upsample=2):\n        super(GeneratorRRDB, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Conv2d(channels, filters, kernel_size=3, stride=1, padding=1)\n        # Residual blocks\n        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n        # Upsampling layers\n        upsample_layers = []\n        for _ in range(num_upsample):\n            upsample_layers += [\n                nn.Conv2d(filters, filters * 4, kernel_size=3, stride=1, padding=1),\n                nn.LeakyReLU(),\n                nn.PixelShuffle(upscale_factor=2),\n            ]\n        self.upsampling = nn.Sequential(*upsample_layers)\n        # Final output block\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(filters, channels, kernel_size=3, stride=1, padding=1),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 86,
      "code_end_line": 93,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 96,
      "code_end_line": 127,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 97,
      "code_end_line": 124,
      "params": [
        "self",
        "input_shape"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 105,
      "code_end_line": 114,
      "params": [
        "in_filters",
        "out_filters",
        "first_block"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 126,
      "code_end_line": 127,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/esrgan/datasets.py": [
    {
      "type": "FunctionDef",
      "name": "denormalize",
      "md_content": [],
      "code_start_line": 16,
      "code_end_line": 20,
      "params": [
        "tensors"
      ],
      "have_return": true,
      "code_content": "def denormalize(tensors):\n    \"\"\" Denormalizes image tensors using mean and std \"\"\"\n    for c in range(3):\n        tensors[:, c].mul_(std[c]).add_(mean[c])\n    return torch.clamp(tensors, 0, 255)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 23,
      "code_end_line": 52,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, hr_shape):\n        hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n\n        self.files = sorted(glob.glob(root + \"/*.*\"))\n\n    def __getitem__(self, index):\n        img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 24,
      "code_end_line": 42,
      "params": [
        "self",
        "root",
        "hr_shape"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, hr_shape):\n        hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n\n        self.files = sorted(glob.glob(root + \"/*.*\"))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 44,
      "code_end_line": 49,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n        img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 51,
      "code_end_line": 52,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/context_encoder/context_encoder.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 56,
      "code_end_line": 62,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "save_sample",
      "md_content": [],
      "code_start_line": 109,
      "code_end_line": 120,
      "params": [
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def save_sample(batches_done):\n    samples, masked_samples, i = next(iter(test_dataloader))\n    samples = Variable(samples.type(Tensor))\n    masked_samples = Variable(masked_samples.type(Tensor))\n    i = i[0].item()  # Upper-left coordinate of mask\n    # Generate inpainted image\n    gen_mask = generator(masked_samples)\n    filled_samples = masked_samples.clone()\n    filled_samples[:, :, i : i + opt.mask_size, i : i + opt.mask_size] = gen_mask\n    # Save sample\n    sample = torch.cat((masked_samples.data, filled_samples.data, samples.data), -2)\n    save_image(sample, \"images/%d.png\" % batches_done, nrow=6, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/context_encoder/models.py": [
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 40,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self, channels=3):\n        super(Generator, self).__init__()\n\n        def downsample(in_feat, out_feat, normalize=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        def upsample(in_feat, out_feat, normalize=True):\n            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.ReLU())\n            return layers\n\n        self.model = nn.Sequential(\n            *downsample(channels, 64, normalize=False),\n            *downsample(64, 64),\n            *downsample(64, 128),\n            *downsample(128, 256),\n            *downsample(256, 512),\n            nn.Conv2d(512, 4000, 1),\n            *upsample(4000, 512),\n            *upsample(512, 256),\n            *upsample(256, 128),\n            *upsample(128, 64),\n            nn.Conv2d(64, channels, 3, 1, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 7,
      "code_end_line": 37,
      "params": [
        "self",
        "channels"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, channels=3):\n        super(Generator, self).__init__()\n\n        def downsample(in_feat, out_feat, normalize=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        def upsample(in_feat, out_feat, normalize=True):\n            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.ReLU())\n            return layers\n\n        self.model = nn.Sequential(\n            *downsample(channels, 64, normalize=False),\n            *downsample(64, 64),\n            *downsample(64, 128),\n            *downsample(128, 256),\n            *downsample(256, 512),\n            nn.Conv2d(512, 4000, 1),\n            *upsample(4000, 512),\n            *upsample(512, 256),\n            *upsample(256, 128),\n            *upsample(128, 64),\n            nn.Conv2d(64, channels, 3, 1, 1),\n            nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "downsample",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 15,
      "params": [
        "in_feat",
        "out_feat",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def downsample(in_feat, out_feat, normalize=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "upsample",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 22,
      "params": [
        "in_feat",
        "out_feat",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def upsample(in_feat, out_feat, normalize=True):\n            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.ReLU())\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 40,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 43,
      "code_end_line": 66,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, stride, normalize):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = channels\n        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 44,
      "code_end_line": 63,
      "params": [
        "self",
        "channels"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, stride, normalize):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = channels\n        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 47,
      "code_end_line": 53,
      "params": [
        "in_filters",
        "out_filters",
        "stride",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, stride, normalize):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 65,
      "code_end_line": 66,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/context_encoder/datasets.py": [
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 53,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, img_size=128, mask_size=64, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.img_size = img_size\n        self.mask_size = mask_size\n        self.mode = mode\n        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n        self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]\n\n    def apply_random_mask(self, img):\n        \"\"\"Randomly masks image\"\"\"\n        y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n        masked_part = img[:, y1:y2, x1:x2]\n        masked_img = img.clone()\n        masked_img[:, y1:y2, x1:x2] = 1\n\n        return masked_img, masked_part\n\n    def apply_center_mask(self, img):\n        \"\"\"Mask center part of image\"\"\"\n        # Get upper-left pixel coordinate\n        i = (self.img_size - self.mask_size) // 2\n        masked_img = img.clone()\n        masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n\n        return masked_img, i\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        img = self.transform(img)\n        if self.mode == \"train\":\n            # For training data perform random mask\n            masked_img, aux = self.apply_random_mask(img)\n        else:\n            # For test data mask the center of the image\n            masked_img, aux = self.apply_center_mask(img)\n\n        return img, masked_img, aux\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 18,
      "params": [
        "self",
        "root",
        "transforms_",
        "img_size",
        "mask_size",
        "mode"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, transforms_=None, img_size=128, mask_size=64, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.img_size = img_size\n        self.mask_size = mask_size\n        self.mode = mode\n        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n        self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "apply_random_mask",
      "md_content": [],
      "code_start_line": 20,
      "code_end_line": 28,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def apply_random_mask(self, img):\n        \"\"\"Randomly masks image\"\"\"\n        y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n        masked_part = img[:, y1:y2, x1:x2]\n        masked_img = img.clone()\n        masked_img[:, y1:y2, x1:x2] = 1\n\n        return masked_img, masked_part\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "apply_center_mask",
      "md_content": [],
      "code_start_line": 30,
      "code_end_line": 37,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def apply_center_mask(self, img):\n        \"\"\"Mask center part of image\"\"\"\n        # Get upper-left pixel coordinate\n        i = (self.img_size - self.mask_size) // 2\n        masked_img = img.clone()\n        masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n\n        return masked_img, i\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 50,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        img = self.transform(img)\n        if self.mode == \"train\":\n            # For training data perform random mask\n            masked_img, aux = self.apply_random_mask(img)\n        else:\n            # For test data mask the center of the image\n            masked_img, aux = self.apply_center_mask(img)\n\n        return img, masked_img, aux\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 52,
      "code_end_line": 53,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/cluster_gan/clustergan.py": [
    {
      "type": "FunctionDef",
      "name": "sample_z",
      "md_content": [],
      "code_start_line": 41,
      "code_end_line": 68,
      "params": [
        "shape",
        "latent_dim",
        "n_c",
        "fix_class",
        "req_grad"
      ],
      "have_return": true,
      "code_content": "def sample_z(shape=64, latent_dim=10, n_c=10, fix_class=-1, req_grad=False):\n\n    assert (fix_class == -1 or (fix_class >= 0 and fix_class < n_c) ), \"Requested class %i outside bounds.\"%fix_class\n\n    Tensor = torch.cuda.FloatTensor\n    \n    # Sample noise as generator input, zn\n    zn = Variable(Tensor(0.75*np.random.normal(0, 1, (shape, latent_dim))), requires_grad=req_grad)\n\n    ######### zc, zc_idx variables with grads, and zc to one-hot vector\n    # Pure one-hot vector generation\n    zc_FT = Tensor(shape, n_c).fill_(0)\n    zc_idx = torch.empty(shape, dtype=torch.long)\n\n    if (fix_class == -1):\n        zc_idx = zc_idx.random_(n_c).cuda()\n        zc_FT = zc_FT.scatter_(1, zc_idx.unsqueeze(1), 1.)\n    else:\n        zc_idx[:] = fix_class\n        zc_FT[:, fix_class] = 1\n\n        zc_idx = zc_idx.cuda()\n        zc_FT = zc_FT.cuda()\n\n    zc = Variable(zc_FT, requires_grad=req_grad)\n\n    # Return components of latent space variable\n    return zn, zc, zc_idx\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "calc_gradient_penalty",
      "md_content": [],
      "code_start_line": 70,
      "code_end_line": 102,
      "params": [
        "netD",
        "real_data",
        "generated_data"
      ],
      "have_return": true,
      "code_content": "def calc_gradient_penalty(netD, real_data, generated_data):\n    # GP strength\n    LAMBDA = 10\n\n    b_size = real_data.size()[0]\n\n    # Calculate interpolation\n    alpha = torch.rand(b_size, 1, 1, 1)\n    alpha = alpha.expand_as(real_data)\n    alpha = alpha.cuda()\n    \n    interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n    interpolated = Variable(interpolated, requires_grad=True)\n    interpolated = interpolated.cuda()\n\n    # Calculate probability of interpolated examples\n    prob_interpolated = netD(interpolated)\n\n    # Calculate gradients of probabilities with respect to examples\n    gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n                           grad_outputs=torch.ones(prob_interpolated.size()).cuda(),\n                           create_graph=True, retain_graph=True)[0]\n\n    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n    # so flatten to easily take norm per example in batch\n    gradients = gradients.view(b_size, -1)\n\n    # Derivatives of the gradient close to 0 can cause problems because of\n    # the square root, so manually calculate norm and add epsilon\n    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n\n    # Return gradient penalty\n    return LAMBDA * ((gradients_norm - 1) ** 2).mean()\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "initialize_weights",
      "md_content": [],
      "code_start_line": 106,
      "code_end_line": 116,
      "params": [
        "net"
      ],
      "have_return": false,
      "code_content": "def initialize_weights(net):\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.ConvTranspose2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "softmax",
      "md_content": [],
      "code_start_line": 120,
      "code_end_line": 121,
      "params": [
        "x"
      ],
      "have_return": true,
      "code_content": "def softmax(x):\n    return F.softmax(x, dim=1)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Reshape",
      "md_content": [],
      "code_start_line": 124,
      "code_end_line": 140,
      "params": [],
      "have_return": true,
      "code_content": "class Reshape(nn.Module):\n    \"\"\"\n    Class for performing a reshape as a layer in a sequential model.\n    \"\"\"\n    def __init__(self, shape=[]):\n        super(Reshape, self).__init__()\n        self.shape = shape\n\n    def forward(self, x):\n        return x.view(x.size(0), *self.shape)\n    \n    def extra_repr(self):\n            # (Optional)Set the extra information about this module. You can test\n            # it by printing an object of this class.\n            return 'shape={}'.format(\n                self.shape\n            )\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 128,
      "code_end_line": 130,
      "params": [
        "self",
        "shape"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, shape=[]):\n        super(Reshape, self).__init__()\n        self.shape = shape\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 132,
      "code_end_line": 133,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return x.view(x.size(0), *self.shape)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "extra_repr",
      "md_content": [],
      "code_start_line": 135,
      "code_end_line": 140,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def extra_repr(self):\n            # (Optional)Set the extra information about this module. You can test\n            # it by printing an object of this class.\n            return 'shape={}'.format(\n                self.shape\n            )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator_CNN",
      "md_content": [],
      "code_start_line": 143,
      "code_end_line": 193,
      "params": [],
      "have_return": true,
      "code_content": "class Generator_CNN(nn.Module):\n    \"\"\"\n    CNN to model the generator of a ClusterGAN\n    Input is a vector from representation space of dimension z_dim\n    output is a vector from image space of dimension X_dim\n    \"\"\"\n    # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n    def __init__(self, latent_dim, n_c, x_shape, verbose=False):\n        super(Generator_CNN, self).__init__()\n\n        self.name = 'generator'\n        self.latent_dim = latent_dim\n        self.n_c = n_c\n        self.x_shape = x_shape\n        self.ishape = (128, 7, 7)\n        self.iels = int(np.prod(self.ishape))\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            # Fully connected layers\n            torch.nn.Linear(self.latent_dim + self.n_c, 1024),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, self.iels),\n            nn.BatchNorm1d(self.iels),\n            nn.LeakyReLU(0.2, inplace=True),\n        \n            # Reshape to 128 x (7x7)\n            Reshape(self.ishape),\n\n            # Upconvolution layers\n            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=True),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=True),\n            nn.Sigmoid()\n        )\n\n        initialize_weights(self)\n\n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)\n    \n    def forward(self, zn, zc):\n        z = torch.cat((zn, zc), 1)\n        x_gen = self.model(z)\n        # Reshape for output\n        x_gen = x_gen.view(x_gen.size(0), *self.x_shape)\n        return x_gen\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 150,
      "code_end_line": 186,
      "params": [
        "self",
        "latent_dim",
        "n_c",
        "x_shape",
        "verbose"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, latent_dim, n_c, x_shape, verbose=False):\n        super(Generator_CNN, self).__init__()\n\n        self.name = 'generator'\n        self.latent_dim = latent_dim\n        self.n_c = n_c\n        self.x_shape = x_shape\n        self.ishape = (128, 7, 7)\n        self.iels = int(np.prod(self.ishape))\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            # Fully connected layers\n            torch.nn.Linear(self.latent_dim + self.n_c, 1024),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, self.iels),\n            nn.BatchNorm1d(self.iels),\n            nn.LeakyReLU(0.2, inplace=True),\n        \n            # Reshape to 128 x (7x7)\n            Reshape(self.ishape),\n\n            # Upconvolution layers\n            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=True),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=True),\n            nn.Sigmoid()\n        )\n\n        initialize_weights(self)\n\n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 188,
      "code_end_line": 193,
      "params": [
        "self",
        "zn",
        "zc"
      ],
      "have_return": true,
      "code_content": "    def forward(self, zn, zc):\n        z = torch.cat((zn, zc), 1)\n        x_gen = self.model(z)\n        # Reshape for output\n        x_gen = x_gen.view(x_gen.size(0), *self.x_shape)\n        return x_gen\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Encoder_CNN",
      "md_content": [],
      "code_start_line": 196,
      "code_end_line": 245,
      "params": [],
      "have_return": true,
      "code_content": "class Encoder_CNN(nn.Module):\n    \"\"\"\n    CNN to model the encoder of a ClusterGAN\n    Input is vector X from image space if dimension X_dim\n    Output is vector z from representation space of dimension z_dim\n    \"\"\"\n    def __init__(self, latent_dim, n_c, verbose=False):\n        super(Encoder_CNN, self).__init__()\n\n        self.name = 'encoder'\n        self.channels = 1\n        self.latent_dim = latent_dim\n        self.n_c = n_c\n        self.cshape = (128, 5, 5)\n        self.iels = int(np.prod(self.cshape))\n        self.lshape = (self.iels,)\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            # Convolutional layers\n            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Flatten\n            Reshape(self.lshape),\n            \n            # Fully connected layers\n            torch.nn.Linear(self.iels, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, latent_dim + n_c)\n        )\n\n        initialize_weights(self)\n        \n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)\n\n    def forward(self, in_feat):\n        z_img = self.model(in_feat)\n        # Reshape for output\n        z = z_img.view(z_img.shape[0], -1)\n        # Separate continuous and one-hot components\n        zn = z[:, 0:self.latent_dim]\n        zc_logits = z[:, self.latent_dim:]\n        # Softmax on zc component\n        zc = softmax(zc_logits)\n        return zn, zc, zc_logits\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 202,
      "code_end_line": 234,
      "params": [
        "self",
        "latent_dim",
        "n_c",
        "verbose"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, latent_dim, n_c, verbose=False):\n        super(Encoder_CNN, self).__init__()\n\n        self.name = 'encoder'\n        self.channels = 1\n        self.latent_dim = latent_dim\n        self.n_c = n_c\n        self.cshape = (128, 5, 5)\n        self.iels = int(np.prod(self.cshape))\n        self.lshape = (self.iels,)\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            # Convolutional layers\n            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Flatten\n            Reshape(self.lshape),\n            \n            # Fully connected layers\n            torch.nn.Linear(self.iels, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, latent_dim + n_c)\n        )\n\n        initialize_weights(self)\n        \n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 236,
      "code_end_line": 245,
      "params": [
        "self",
        "in_feat"
      ],
      "have_return": true,
      "code_content": "    def forward(self, in_feat):\n        z_img = self.model(in_feat)\n        # Reshape for output\n        z = z_img.view(z_img.shape[0], -1)\n        # Separate continuous and one-hot components\n        zn = z[:, 0:self.latent_dim]\n        zc_logits = z[:, self.latent_dim:]\n        # Softmax on zc component\n        zc = softmax(zc_logits)\n        return zn, zc, zc_logits\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator_CNN",
      "md_content": [],
      "code_start_line": 248,
      "code_end_line": 297,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator_CNN(nn.Module):\n    \"\"\"\n    CNN to model the discriminator of a ClusterGAN\n    Input is tuple (X,z) of an image vector and its corresponding\n    representation z vector. For example, if X comes from the dataset, corresponding\n    z is Encoder(X), and if z is sampled from representation space, X is Generator(z)\n    Output is a 1-dimensional value\n    \"\"\"            \n    # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n    def __init__(self, wass_metric=False, verbose=False):\n        super(Discriminator_CNN, self).__init__()\n        \n        self.name = 'discriminator'\n        self.channels = 1\n        self.cshape = (128, 5, 5)\n        self.iels = int(np.prod(self.cshape))\n        self.lshape = (self.iels,)\n        self.wass = wass_metric\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            # Convolutional layers\n            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Flatten\n            Reshape(self.lshape),\n            \n            # Fully connected layers\n            torch.nn.Linear(self.iels, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, 1),\n        )\n        \n        # If NOT using Wasserstein metric, final Sigmoid\n        if (not self.wass):\n            self.model = nn.Sequential(self.model, torch.nn.Sigmoid())\n\n        initialize_weights(self)\n\n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)\n\n    def forward(self, img):\n        # Get output\n        validity = self.model(img)\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 257,
      "code_end_line": 292,
      "params": [
        "self",
        "wass_metric",
        "verbose"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, wass_metric=False, verbose=False):\n        super(Discriminator_CNN, self).__init__()\n        \n        self.name = 'discriminator'\n        self.channels = 1\n        self.cshape = (128, 5, 5)\n        self.iels = int(np.prod(self.cshape))\n        self.lshape = (self.iels,)\n        self.wass = wass_metric\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            # Convolutional layers\n            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Flatten\n            Reshape(self.lshape),\n            \n            # Fully connected layers\n            torch.nn.Linear(self.iels, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, 1),\n        )\n        \n        # If NOT using Wasserstein metric, final Sigmoid\n        if (not self.wass):\n            self.model = nn.Sequential(self.model, torch.nn.Sigmoid())\n\n        initialize_weights(self)\n\n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 294,
      "code_end_line": 297,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        # Get output\n        validity = self.model(img)\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/wgan_gp/wgan_gp.py": [
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 65,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 43,
      "code_end_line": 60,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 51,
      "params": [
        "in_feat",
        "out_feat",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 62,
      "code_end_line": 65,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 68,
      "code_end_line": 83,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 69,
      "code_end_line": 78,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 80,
      "code_end_line": 83,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "compute_gradient_penalty",
      "md_content": [],
      "code_start_line": 119,
      "code_end_line": 138,
      "params": [
        "D",
        "real_samples",
        "fake_samples"
      ],
      "have_return": true,
      "code_content": "def compute_gradient_penalty(D, real_samples, fake_samples):\n    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n    # Random weight term for interpolation between real and fake samples\n    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n    # Get random interpolation between real and fake samples\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = D(interpolates)\n    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n    # Get gradient w.r.t. interpolates\n    gradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/cyclegan/utils.py": [
    {
      "type": "ClassDef",
      "name": "ReplayBuffer",
      "md_content": [],
      "code_start_line": 13,
      "code_end_line": 33,
      "params": [],
      "have_return": true,
      "code_content": "class ReplayBuffer:\n    def __init__(self, max_size=50):\n        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return Variable(torch.cat(to_return))\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 14,
      "code_end_line": 17,
      "params": [
        "self",
        "max_size"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, max_size=50):\n        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n        self.max_size = max_size\n        self.data = []\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "push_and_pop",
      "md_content": [],
      "code_start_line": 19,
      "code_end_line": 33,
      "params": [
        "self",
        "data"
      ],
      "have_return": true,
      "code_content": "    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return Variable(torch.cat(to_return))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "LambdaLR",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 44,
      "params": [],
      "have_return": true,
      "code_content": "class LambdaLR:\n    def __init__(self, n_epochs, offset, decay_start_epoch):\n        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch\n\n    def step(self, epoch):\n        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 41,
      "params": [
        "self",
        "n_epochs",
        "offset",
        "decay_start_epoch"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, n_epochs, offset, decay_start_epoch):\n        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "step",
      "md_content": [],
      "code_start_line": 43,
      "code_end_line": 44,
      "params": [
        "self",
        "epoch"
      ],
      "have_return": true,
      "code_content": "    def step(self, epoch):\n        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/cyclegan/cyclegan.py": [
    {
      "type": "FunctionDef",
      "name": "sample_images",
      "md_content": [],
      "code_start_line": 135,
      "code_end_line": 151,
      "params": [
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_images(batches_done):\n    \"\"\"Saves a generated sample from the test set\"\"\"\n    imgs = next(iter(val_dataloader))\n    G_AB.eval()\n    G_BA.eval()\n    real_A = Variable(imgs[\"A\"].type(Tensor))\n    fake_B = G_AB(real_A)\n    real_B = Variable(imgs[\"B\"].type(Tensor))\n    fake_A = G_BA(real_B)\n    # Arange images along x-axis\n    real_A = make_grid(real_A, nrow=5, normalize=True)\n    real_B = make_grid(real_B, nrow=5, normalize=True)\n    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n    # Arange images along y-axis\n    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n    save_image(image_grid, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), normalize=False)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/cyclegan/models.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 14,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        if hasattr(m, \"bias\") and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0.0)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ResidualBlock",
      "md_content": [],
      "code_start_line": 22,
      "code_end_line": 37,
      "params": [],
      "have_return": true,
      "code_content": "class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 23,
      "code_end_line": 34,
      "params": [
        "self",
        "in_features"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 36,
      "code_end_line": 37,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return x + self.block(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "GeneratorResNet",
      "md_content": [],
      "code_start_line": 40,
      "code_end_line": 87,
      "params": [],
      "have_return": true,
      "code_content": "class GeneratorResNet(nn.Module):\n    def __init__(self, input_shape, num_residual_blocks):\n        super(GeneratorResNet, self).__init__()\n\n        channels = input_shape[0]\n\n        # Initial convolution block\n        out_features = 64\n        model = [\n            nn.ReflectionPad2d(channels),\n            nn.Conv2d(channels, out_features, 7),\n            nn.InstanceNorm2d(out_features),\n            nn.ReLU(inplace=True),\n        ]\n        in_features = out_features\n\n        # Downsampling\n        for _ in range(2):\n            out_features *= 2\n            model += [\n                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Residual blocks\n        for _ in range(num_residual_blocks):\n            model += [ResidualBlock(out_features)]\n\n        # Upsampling\n        for _ in range(2):\n            out_features //= 2\n            model += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Output layer\n        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 41,
      "code_end_line": 84,
      "params": [
        "self",
        "input_shape",
        "num_residual_blocks"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, input_shape, num_residual_blocks):\n        super(GeneratorResNet, self).__init__()\n\n        channels = input_shape[0]\n\n        # Initial convolution block\n        out_features = 64\n        model = [\n            nn.ReflectionPad2d(channels),\n            nn.Conv2d(channels, out_features, 7),\n            nn.InstanceNorm2d(out_features),\n            nn.ReLU(inplace=True),\n        ]\n        in_features = out_features\n\n        # Downsampling\n        for _ in range(2):\n            out_features *= 2\n            model += [\n                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Residual blocks\n        for _ in range(num_residual_blocks):\n            model += [ResidualBlock(out_features)]\n\n        # Upsampling\n        for _ in range(2):\n            out_features //= 2\n            model += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Output layer\n        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 86,
      "code_end_line": 87,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 95,
      "code_end_line": 122,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n\n        # Calculate output shape of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1)\n        )\n\n    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 96,
      "code_end_line": 119,
      "params": [
        "self",
        "input_shape"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n\n        # Calculate output shape of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1)\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 104,
      "code_end_line": 110,
      "params": [
        "in_filters",
        "out_filters",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 121,
      "code_end_line": 122,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/cyclegan/datasets.py": [
    {
      "type": "FunctionDef",
      "name": "to_rgb",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 13,
      "params": [
        "image"
      ],
      "have_return": true,
      "code_content": "def to_rgb(image):\n    rgb_image = Image.new(\"RGB\", image.size)\n    rgb_image.paste(image)\n    return rgb_image\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 16,
      "code_end_line": 43,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.unaligned = unaligned\n\n        self.files_A = sorted(glob.glob(os.path.join(root, \"%s/A\" % mode) + \"/*.*\"))\n        self.files_B = sorted(glob.glob(os.path.join(root, \"%s/B\" % mode) + \"/*.*\"))\n\n    def __getitem__(self, index):\n        image_A = Image.open(self.files_A[index % len(self.files_A)])\n\n        if self.unaligned:\n            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n        else:\n            image_B = Image.open(self.files_B[index % len(self.files_B)])\n\n        # Convert grayscale images to rgb\n        if image_A.mode != \"RGB\":\n            image_A = to_rgb(image_A)\n        if image_B.mode != \"RGB\":\n            image_B = to_rgb(image_B)\n\n        item_A = self.transform(image_A)\n        item_B = self.transform(image_B)\n        return {\"A\": item_A, \"B\": item_B}\n\n    def __len__(self):\n        return max(len(self.files_A), len(self.files_B))\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 22,
      "params": [
        "self",
        "root",
        "transforms_",
        "unaligned",
        "mode"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.unaligned = unaligned\n\n        self.files_A = sorted(glob.glob(os.path.join(root, \"%s/A\" % mode) + \"/*.*\"))\n        self.files_B = sorted(glob.glob(os.path.join(root, \"%s/B\" % mode) + \"/*.*\"))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 24,
      "code_end_line": 40,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n        image_A = Image.open(self.files_A[index % len(self.files_A)])\n\n        if self.unaligned:\n            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n        else:\n            image_B = Image.open(self.files_B[index % len(self.files_B)])\n\n        # Convert grayscale images to rgb\n        if image_A.mode != \"RGB\":\n            image_A = to_rgb(image_A)\n        if image_B.mode != \"RGB\":\n            image_B = to_rgb(image_B)\n\n        item_A = self.transform(image_A)\n        item_B = self.transform(image_B)\n        return {\"A\": item_A, \"B\": item_B}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 43,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return max(len(self.files_A), len(self.files_B))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/dragan/dragan.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 43,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 71,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, noise):\n        out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 47,
      "code_end_line": 65,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 67,
      "code_end_line": 71,
      "params": [
        "self",
        "noise"
      ],
      "have_return": true,
      "code_content": "    def forward(self, noise):\n        out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 74,
      "code_end_line": 100,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n\n    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 75,
      "code_end_line": 93,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discriminator_block",
      "md_content": [],
      "code_start_line": 78,
      "code_end_line": 82,
      "params": [
        "in_filters",
        "out_filters",
        "bn"
      ],
      "have_return": true,
      "code_content": "        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 95,
      "code_end_line": 100,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "compute_gradient_penalty",
      "md_content": [],
      "code_start_line": 144,
      "code_end_line": 167,
      "params": [
        "D",
        "X"
      ],
      "have_return": true,
      "code_content": "def compute_gradient_penalty(D, X):\n    \"\"\"Calculates the gradient penalty loss for DRAGAN\"\"\"\n    # Random weight term for interpolation\n    alpha = Tensor(np.random.random(size=X.shape))\n\n    interpolates = alpha * X + ((1 - alpha) * (X + 0.5 * X.std() * torch.rand(X.size())))\n    interpolates = Variable(interpolates, requires_grad=True)\n\n    d_interpolates = D(interpolates)\n\n    fake = Variable(Tensor(X.shape[0], 1).fill_(1.0), requires_grad=False)\n\n    # Get gradient w.r.t. interpolates\n    gradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n\n    gradient_penalty = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/cgan/cgan.py": [
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 66,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, noise, labels):\n        # Concatenate label embedding and image to produce input\n        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n        img = self.model(gen_input)\n        img = img.view(img.size(0), *img_shape)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 40,
      "code_end_line": 59,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 45,
      "code_end_line": 50,
      "params": [
        "in_feat",
        "out_feat",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 61,
      "code_end_line": 66,
      "params": [
        "self",
        "noise",
        "labels"
      ],
      "have_return": true,
      "code_content": "    def forward(self, noise, labels):\n        # Concatenate label embedding and image to produce input\n        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n        img = self.model(gen_input)\n        img = img.view(img.size(0), *img_shape)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 69,
      "code_end_line": 91,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, img, labels):\n        # Concatenate label embedding and image to produce input\n        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n        validity = self.model(d_in)\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 70,
      "code_end_line": 85,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 1),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 87,
      "code_end_line": 91,
      "params": [
        "self",
        "img",
        "labels"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img, labels):\n        # Concatenate label embedding and image to produce input\n        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n        validity = self.model(d_in)\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "sample_image",
      "md_content": [],
      "code_start_line": 129,
      "code_end_line": 137,
      "params": [
        "n_row",
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_image(n_row, batches_done):\n    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n    # Sample noise\n    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n    # Get labels ranging from 0 to n_classes for n rows\n    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n    labels = Variable(LongTensor(labels))\n    gen_imgs = generator(z, labels)\n    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/wgan_div/wgan_div.py": [
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 65,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 43,
      "code_end_line": 60,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 46,
      "code_end_line": 51,
      "params": [
        "in_feat",
        "out_feat",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 62,
      "code_end_line": 65,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 68,
      "code_end_line": 83,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 69,
      "code_end_line": 78,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 80,
      "code_end_line": 83,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/gan/gan.py": [
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 38,
      "code_end_line": 61,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), *img_shape)\n        return img\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 56,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "block",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 47,
      "params": [
        "in_feat",
        "out_feat",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 58,
      "code_end_line": 61,
      "params": [
        "self",
        "z"
      ],
      "have_return": true,
      "code_content": "    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), *img_shape)\n        return img\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 64,
      "code_end_line": 81,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n\n        return validity\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 65,
      "code_end_line": 75,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 77,
      "code_end_line": 81,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n\n        return validity\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/dualgan/dualgan.py": [
    {
      "type": "FunctionDef",
      "name": "compute_gradient_penalty",
      "md_content": [],
      "code_start_line": 116,
      "code_end_line": 135,
      "params": [
        "D",
        "real_samples",
        "fake_samples"
      ],
      "have_return": true,
      "code_content": "def compute_gradient_penalty(D, real_samples, fake_samples):\n    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n    # Random weight term for interpolation between real and fake samples\n    alpha = FloatTensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n    # Get random interpolation between real and fake samples\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    validity = D(interpolates)\n    fake = Variable(FloatTensor(np.ones(validity.shape)), requires_grad=False)\n    # Get gradient w.r.t. interpolates\n    gradients = autograd.grad(\n        outputs=validity,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "sample_images",
      "md_content": [],
      "code_start_line": 138,
      "code_end_line": 148,
      "params": [
        "batches_done"
      ],
      "have_return": false,
      "code_content": "def sample_images(batches_done):\n    \"\"\"Saves a generated sample from the test set\"\"\"\n    imgs = next(iter(val_dataloader))\n    real_A = Variable(imgs[\"A\"].type(FloatTensor))\n    fake_B = G_AB(real_A)\n    AB = torch.cat((real_A.data, fake_B.data), -2)\n    real_B = Variable(imgs[\"B\"].type(FloatTensor))\n    fake_A = G_BA(real_B)\n    BA = torch.cat((real_B.data, fake_A.data), -2)\n    img_sample = torch.cat((AB, BA), 0)\n    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=8, normalize=True)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/dualgan/models.py": [
    {
      "type": "FunctionDef",
      "name": "weights_init_normal",
      "md_content": [],
      "code_start_line": 8,
      "code_end_line": 14,
      "params": [
        "m"
      ],
      "have_return": false,
      "code_content": "def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "UNetDown",
      "md_content": [],
      "code_start_line": 22,
      "code_end_line": 34,
      "params": [],
      "have_return": true,
      "code_content": "class UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size, affine=True))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 23,
      "code_end_line": 31,
      "params": [
        "self",
        "in_size",
        "out_size",
        "normalize",
        "dropout"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size, affine=True))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 33,
      "code_end_line": 34,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        return self.model(x)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "UNetUp",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 54,
      "params": [],
      "have_return": true,
      "code_content": "class UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, bias=False),\n            nn.InstanceNorm2d(out_size, affine=True),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 38,
      "code_end_line": 48,
      "params": [
        "self",
        "in_size",
        "out_size",
        "dropout"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, bias=False),\n            nn.InstanceNorm2d(out_size, affine=True),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 50,
      "code_end_line": 54,
      "params": [
        "self",
        "x",
        "skip_input"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Generator",
      "md_content": [],
      "code_start_line": 57,
      "code_end_line": 94,
      "params": [],
      "have_return": true,
      "code_content": "class Generator(nn.Module):\n    def __init__(self, channels=3):\n        super(Generator, self).__init__()\n\n        self.down1 = UNetDown(channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5, normalize=False)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 256)\n        self.up5 = UNetUp(512, 128)\n        self.up6 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(nn.ConvTranspose2d(128, channels, 4, stride=2, padding=1), nn.Tanh())\n\n    def forward(self, x):\n        # Propogate noise through fc layer and reshape to img shape\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        u1 = self.up1(d7, d6)\n        u2 = self.up2(u1, d5)\n        u3 = self.up3(u2, d4)\n        u4 = self.up4(u3, d3)\n        u5 = self.up5(u4, d2)\n        u6 = self.up6(u5, d1)\n\n        return self.final(u6)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 58,
      "code_end_line": 76,
      "params": [
        "self",
        "channels"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, channels=3):\n        super(Generator, self).__init__()\n\n        self.down1 = UNetDown(channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5, normalize=False)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 256)\n        self.up5 = UNetUp(512, 128)\n        self.up6 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(nn.ConvTranspose2d(128, channels, 4, stride=2, padding=1), nn.Tanh())\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 78,
      "code_end_line": 94,
      "params": [
        "self",
        "x"
      ],
      "have_return": true,
      "code_content": "    def forward(self, x):\n        # Propogate noise through fc layer and reshape to img shape\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        u1 = self.up1(d7, d6)\n        u2 = self.up2(u1, d5)\n        u3 = self.up3(u2, d4)\n        u4 = self.up4(u3, d3)\n        u5 = self.up5(u4, d2)\n        u6 = self.up6(u5, d1)\n\n        return self.final(u6)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Discriminator",
      "md_content": [],
      "code_start_line": 102,
      "code_end_line": 123,
      "params": [],
      "have_return": true,
      "code_content": "class Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n\n        def discrimintor_block(in_features, out_features, normalize=True):\n            \"\"\"Discriminator block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_features, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discrimintor_block(in_channels, 64, normalize=False),\n            *discrimintor_block(64, 128),\n            *discrimintor_block(128, 256),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(256, 1, kernel_size=4)\n        )\n\n    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 103,
      "code_end_line": 120,
      "params": [
        "self",
        "in_channels"
      ],
      "have_return": true,
      "code_content": "    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n\n        def discrimintor_block(in_features, out_features, normalize=True):\n            \"\"\"Discriminator block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_features, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discrimintor_block(in_channels, 64, normalize=False),\n            *discrimintor_block(64, 128),\n            *discrimintor_block(128, 256),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(256, 1, kernel_size=4)\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "discrimintor_block",
      "md_content": [],
      "code_start_line": 106,
      "code_end_line": 112,
      "params": [
        "in_features",
        "out_features",
        "normalize"
      ],
      "have_return": true,
      "code_content": "        def discrimintor_block(in_features, out_features, normalize=True):\n            \"\"\"Discriminator block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_features, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "forward",
      "md_content": [],
      "code_start_line": 122,
      "code_end_line": 123,
      "params": [
        "self",
        "img"
      ],
      "have_return": true,
      "code_content": "    def forward(self, img):\n        return self.model(img)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "implementations/dualgan/datasets.py": [
    {
      "type": "ClassDef",
      "name": "ImageDataset",
      "md_content": [],
      "code_start_line": 11,
      "code_end_line": 34,
      "params": [],
      "have_return": true,
      "code_content": "class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 15,
      "params": [
        "self",
        "root",
        "transforms_",
        "mode"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__getitem__",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 31,
      "params": [
        "self",
        "index"
      ],
      "have_return": true,
      "code_content": "    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__len__",
      "md_content": [],
      "code_start_line": 33,
      "code_end_line": 34,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def __len__(self):\n        return len(self.files)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ]
}