{
    "PyTorch-GAN/implementations/aae/aae.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "reparameterization",
                "second_doc": "\"\"\"\nPerforms the reparameterization trick to sample a latent variable z from a Gaussian distribution parameterized by mean (mu) and log-variance (logvar). Sampling in this manner enables gradient-based optimization by making the process differentiable.\n\nArgs:\n    mu (Tensor): The mean of the latent Gaussian.\n    logvar (Tensor): The logarithm of the variance of the latent Gaussian.\n\nReturns:\n    Tensor: A sample z, reparameterized to allow gradients to flow during training.\n    \nWhy:\n    This method enables stochastic sampling during training while maintaining differentiability, which is essential for optimizing models with latent variables using backpropagation.\n\"\"\"",
                "source_code": "std = torch.exp(logvar / 2)\n    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), opt.latent_dim))))\n    z = sampled_z * std + mu\n    return z"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the encoder network architecture, constructing the layers necessary to map input images into compact latent representations characterized by mean and variance vectors.\n\nArgs:\n    img_shape (tuple): The shape of the input image, used to determine input dimensions of the first linear layer.\n    opt (object): Configuration object containing model hyperparameters, such as the latent vector dimension (latent_dim).\n\nReturns:\n    None\n\nWhy:\n    This setup allows the encoder to transform high-dimensional image data into a statistical summary in latent space, facilitating subsequent generative or inference processes.\n\"\"\"",
                    "source_code": "super(Encoder, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.mu = nn.Linear(512, opt.latent_dim)\n        self.logvar = nn.Linear(512, opt.latent_dim)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses an input image tensor to generate a latent representation by encoding it through fully connected layers and sampling from the resulting approximate posterior distribution. This enables meaningful sampling and compression of images into a latent space.\n\nArgs:\n    img (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n\nReturns:\n    torch.Tensor: Latent variable tensor sampled from the encoded distribution, typically of shape (batch_size, latent_dim).\n\"\"\"",
                    "source_code": "img_flat = img.view(img.shape[0], -1)\n        x = self.model(img_flat)\n        mu = self.mu(x)\n        logvar = self.logvar(x)\n        z = reparameterization(mu, logvar)\n        return z"
                }
            ],
            "name": "Encoder",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Decoder module by constructing a neural network that transforms latent representations into an output matching the target image shape. This setup enables the model to learn mappings from abstract features to visually meaningful outputs, supporting tasks that require image reconstruction or generation.\n\nArgs:\n    opt: An options object containing model configuration parameters, including the latent dimension (opt.latent_dim) and the target image shape (img_shape).\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Decoder, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.latent_dim, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, int(np.prod(img_shape))),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nTransforms a latent vector into a generated image by passing it through the decoder model and reshaping the output to the desired image dimensions. This enables the synthesis of new images from sampled latent representations.\n\nArgs:\n    z (Tensor): The input latent vector typically sampled from a prior distribution.\n\nReturns:\n    Tensor: The generated image tensor with the specified shape.\n\"\"\"",
                    "source_code": "img_flat = self.model(z)\n        img = img_flat.view(img_flat.shape[0], *img_shape)\n        return img"
                }
            ],
            "name": "Decoder",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers of the discriminator neural network, defining a sequence of fully connected layers and non-linear activations to process input latent representations and output a probability score.\n\nArgs:\n    opt: An object containing configuration parameters, specifically including 'latent_dim' which defines the dimension of the input latent vector.\n\nReturns:\n    None\n\nWhy:\n    This setup enables the network to distinguish between real and generated data by transforming latent representations into a single probability value, thereby facilitating effective adversarial training.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.latent_dim, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nComputes the discriminator's assessment of the input tensor by passing it through its internal model.\n\nArgs:\n    z (torch.Tensor): The input tensor to be evaluated, typically representing a generated or real sample.\n\nReturns:\n    torch.Tensor: The output tensor representing the discriminator's judgment of the input's authenticity.\n\nWhy:\n    This allows the model to differentiate between real and generated data by producing a score that guides the adversarial training dynamics.\n\"\"\"",
                    "source_code": "validity = self.model(z)\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"Saves a grid of generated digits\"\"\"",
                "first_doc": "\"\"\"\nGenerates and saves a grid of sampled images using the decoder network with random noise input.\n\nArgs:\n    n_row: The number of images per row and column in the output grid, producing an n_row x n_row image grid.\n    batches_done: The number representing how many batches have been processed, used to name the saved image file.\n\nReturns:\n    None: This method performs a side effect by saving the generated image grid to disk.\n\"\"\"",
                "method_name": "sample_image",
                "second_doc": "\"\"\"\nCreates and saves a grid of images generated by the decoder network from random latent vectors, allowing visualization of the model's generative abilities at specific training stages.\n\nArgs:\n    n_row (int): Number of images per row and column for the output grid (n_row x n_row).\n    batches_done (int): The current number of batches processed, used to name the saved image file.\n\nReturns:\n    None: The function saves the image grid to disk and does not return a value.\n\"\"\"",
                "source_code": "# Sample noise\n    z = Variable(Tensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n    gen_imgs = decoder(z)\n    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/acgan/acgan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of layers within a neural network using a normal distribution suited to their type to promote stable training dynamics.\n\nArgs:\n    m (torch.nn.Module): A module whose weights (and biases for batch normalization) will be initialized.\n\nReturns:\n    None\n\nThis method assigns random weights to convolutional and batch normalization layers in accordance with recommended practices, which helps to ensure effective gradient flow and model convergence at the start of training.\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the generator network by constructing embedding, linear, and convolutional layers needed to transform random noise and class labels into a synthetic image.\n\nArgs:\n    opt: Configuration options containing image size, latent dimensionality, number of classes, and number of image channels.\n\nReturns:\n    None\n\nWhy:\n    Creates the required architecture for learning to map noise vectors and class information into images with realistic structure and conditional characteristics.\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.label_emb = nn.Embedding(opt.n_classes, opt.latent_dim)\n\n        self.init_size = opt.img_size // 4  # Initial size before upsampling\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the generator network, transforming random noise and label embeddings into a generated image. By integrating label information into the input, the method facilitates conditioned image synthesis, allowing for more controlled and targeted outputs.\n\nArgs:\n    noise (torch.Tensor): Random noise vector input for image generation, typically sampled from a standard normal distribution.\n    labels (torch.Tensor): Class labels to provide conditional information for the generation process.\n\nReturns:\n    torch.Tensor: The generated image tensor, representing the synthetic output conditioned on the input noise and labels.\n\"\"\"",
                    "source_code": "gen_input = torch.mul(self.label_emb(labels), noise)\n        out = self.l1(gen_input)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers of the Discriminator neural network, which is responsible for distinguishing real data from generated data and predicting class labels. This process enables the model to evaluate authenticity and provide auxiliary classification signals, important for training stability and improved feature learning.\n\nArgs:\n    opt: Namespace or configuration object containing settings such as \n        - channels (int): Number of input image channels,\n        - img_size (int): Spatial dimensions of the input image,\n        - n_classes (int): Number of auxiliary classes.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.conv_blocks = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n\n        # Output layers\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPasses the input image through convolutional and linear layers to compute both its authenticity score and predicted class label.\n\nArgs:\n    img (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n\nReturns:\n    tuple: (validity, label)\n        - validity (torch.Tensor): Discriminator output indicating real or fake prediction for each image in the batch.\n        - label (torch.Tensor): Auxiliary classifier output predicting the class label for each input image.\n        \nThe method performs these operations to evaluate input images, enabling adversarial training and class-conditioned generation tasks.\n\"\"\"",
                    "source_code": "out = self.conv_blocks(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        label = self.aux_layer(out)\n\n        return validity, label"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"",
                "first_doc": "\"\"\"\nGenerates and saves a grid of sample images from the generator model.\n\nArgs:\n    n_row: The number of rows (and columns) of images to sample, determining the grid size and the range of class labels used for conditioning.\n    batches_done: The number of batches processed so far, used to name the output image file.\n\nReturns:\n    None: This method saves the generated images to disk and does not return any value.\n\"\"\"",
                "method_name": "sample_image",
                "second_doc": "\"\"\"\nGenerates a grid of images by sampling random noise and a set of class labels, passing them through the generator, and saving the output to disk.\n\nThis method demonstrates the current capability of the generator model by visualizing its outputs over a set of labels and random noise, making it useful for monitoring training progress and evaluating the generator's conditional image synthesis performance.\n\nArgs:\n    n_row (int): Number of rows (and columns) for the image grid, also determining the diversity of class labels used for conditioning.\n    batches_done (int): Current batch count, used to uniquely name the output image file.\n\nReturns:\n    None: The method saves the generated image grid to disk and does not return a value.\n\"\"\"",
                "source_code": "# Sample noise\n    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n    # Get labels ranging from 0 to n_classes for n rows\n    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n    labels = Variable(LongTensor(labels))\n    gen_imgs = generator(z, labels)\n    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/began/began.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of neural network layers to appropriate normal distributions depending on the layer type, ensuring stable and effective model training.\n\nArgs:\n    m (torch.nn.Module): The neural network layer (module) to initialize.\n\nReturns:\n    None\nWhy: Properly initializing network weights, tailored to the specific layer type (convolutional or batch normalization), helps promote stable convergence and prevents issues such as vanishing or exploding gradients during training.\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator model by constructing the neural network architecture that transforms random latent vectors into realistic synthetic images through a series of learned upsampling and convolutional operations. This process enables the network to progressively shape noise into structured visual data, facilitating the training of adversarial networks to improve image generation quality.\n\nArgs:\n    opt: An options object containing parameters such as image size, latent dimension, and number of output channels.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the generator network, transforming input noise into a synthesized image tensor. This process enables the model to learn mappings from noise distributions to image spaces, supporting training and evaluation of generative capabilities.\n\nArgs:\n    noise (torch.Tensor): Input latent vector of shape (batch_size, latent_dim), sampled from a random distribution.\n\nReturns:\n    torch.Tensor: Generated images tensor of shape (batch_size, channels, height, width).\n\"\"\"",
                    "source_code": "out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers of the Discriminator, constructing the necessary feature extraction, dimensionality reduction, and upsampling blocks for image data processing.\n\nArgs:\n    opt: An object containing configuration options, including the number of channels and image size.\n\nReturns:\n    None\n\nThis setup enables the discriminator to effectively learn representations that distinguish between real and generated images by transforming, compressing, and reconstructing the visual information.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        # Upsampling\n        self.down = nn.Sequential(nn.Conv2d(opt.channels, 64, 3, 2, 1), nn.ReLU())\n        # Fully-connected layers\n        self.down_size = opt.img_size // 2\n        down_dim = 64 * (opt.img_size // 2) ** 2\n        self.fc = nn.Sequential(\n            nn.Linear(down_dim, 32),\n            nn.BatchNorm1d(32, 0.8),\n            nn.ReLU(inplace=True),\n            nn.Linear(32, down_dim),\n            nn.BatchNorm1d(down_dim),\n            nn.ReLU(inplace=True),\n        )\n        # Upsampling\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(64, opt.channels, 3, 1, 1))"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses an input image through a sequence of encoding, feature extraction, and decoding operations to produce a transformed output. This helps assess whether an image is likely to be real or generated, enabling effective learning through adversarial feedback.\n\nArgs:\n    img (Tensor): Input image tensor to be evaluated.\n\nReturns:\n    Tensor: Output tensor representing the discriminator's assessment, typically in the form of a probability or score map.\n\"\"\"",
                    "source_code": "out = self.down(img)\n        out = self.fc(out.view(out.size(0), -1))\n        out = self.up(out.view(out.size(0), 64, self.down_size, self.down_size))\n        return out"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/bgan/bgan.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator network by constructing a sequential model composed of linear transformations and non-linear activations, designed to progressively map a latent input vector into an output matching the target image dimensions. The layered structure enables the model to learn complex hierarchical representations necessary for generating realistic outputs from random latent codes.\n\nArgs:\n    opt: An object containing configuration options, specifically the latent_dim attribute that specifies the dimensionality of the input noise vector.\n    img_shape: The shape tuple (channels, height, width) that determines the dimensions of the output image.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates images from input latent vectors using the internal generator model and reshapes the output to match the expected image dimensions.\n\nArgs:\n    z (torch.Tensor): Input latent vectors, typically sampled from a standard normal distribution.\n\nReturns:\n    torch.Tensor: Generated images with shape (batch_size, *img_shape), suitable for further processing or evaluation.\n    \nThe method performs these operations to transform a low-dimensional latent representation into a structured image, enabling downstream tasks such as training a discriminator or visualizing synthetic samples.\n\"\"\"",
                    "source_code": "img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator's neural network layers, defining the sequence of linear transformations and activation functions used to evaluate whether an input image is genuine or generated. This structure enables the model to map input images to a single probability score, assessing the likelihood of authenticity.\n\nArgs:\n    img_shape (tuple): The shape of input images, used to determine the input dimension for the first linear layer.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses an input image tensor through the discriminator network to determine its authenticity.\nArgs:\n    img (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\nReturns:\n    torch.Tensor: Discriminator's output indicating how likely each input image is to be real or fake.\nThis method reshapes the input image and passes it through the model to facilitate the training and evaluation of generative models by providing feedback on image authenticity.\n\"\"\"",
                    "source_code": "img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"\n    Boundary seeking loss.\n    Reference: https://wiseodd.github.io/techblog/2017/03/07/boundary-seeking-gan/\n    \"\"\"",
                "first_doc": "\"\"\"\nComputes the boundary seeking loss between predicted values and true values.\n\nArgs:\n    y_pred: The predicted values from the model.\n    y_true: The ground truth values. (Not used in loss calculation but included for API consistency.)\n\nReturns:\n    torch.Tensor: The computed boundary seeking loss as a tensor.\n\"\"\"",
                "method_name": "boundary_seeking_loss",
                "second_doc": "\"\"\"\nCalculates the boundary seeking loss to encourage the model's predictions to be confidently separated, enhancing the quality of the learned data distribution.\n\nArgs:\n    y_pred (torch.Tensor): The predicted values from the model.\n    y_true (torch.Tensor): The ground truth values. Provided for API consistency but not used in the calculation.\n\nReturns:\n    torch.Tensor: A tensor containing the computed boundary seeking loss value.\n\"\"\"",
                "source_code": "return 0.5 * torch.mean((torch.log(y_pred) - torch.log(1 - y_pred)) ** 2)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/bicyclegan/bicyclegan.py": [
        {
            "details": {
                "docstring": "\"\"\"Saves a generated sample from the validation set\"\"\"",
                "first_doc": "\"\"\"\nGenerates image samples from the validation dataloader and saves a grid of real and generated images.\n\nArgs:\n    batches_done: The current number of training batches completed, used to name the saved image file.\n\nReturns:\n    None: This method does not return a value. It saves a grid of images to disk for visual inspection of model outputs.\n\"\"\"",
                "method_name": "sample_images",
                "second_doc": "\"\"\"\nSamples images from the validation set, generates corresponding synthetic images using the trained model, and saves a visual grid combining both real and generated outputs. This process facilitates the ongoing qualitative assessment of the model's generative performance and progress during training.\n\nArgs:\n    batches_done (int): The current number of training batches completed; used to determine the filename for saving image grids.\n\nReturns:\n    None: The function does not return a value. It writes a grid of real and generated images to disk for qualitative evaluation.\n\"\"\"",
                "source_code": "generator.eval()\n    imgs = next(iter(val_dataloader))\n    img_samples = None\n    for img_A, img_B in zip(imgs[\"A\"], imgs[\"B\"]):\n        # Repeat input image by number of desired columns\n        real_A = img_A.view(1, *img_A.shape).repeat(opt.latent_dim, 1, 1, 1)\n        real_A = Variable(real_A.type(Tensor))\n        # Sample latent representations\n        sampled_z = Variable(Tensor(np.random.normal(0, 1, (opt.latent_dim, opt.latent_dim))))\n        # Generate samples\n        fake_B = generator(real_A, sampled_z)\n        # Concatenate samples horisontally\n        fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n        img_sample = torch.cat((img_A, fake_B), -1)\n        img_sample = img_sample.view(1, *img_sample.shape)\n        # Concatenate with previous samples vertically\n        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n    save_image(img_samples, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=8, normalize=True)\n    generator.train()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "reparameterization",
                "second_doc": "\"\"\"\nPerforms the reparameterization trick to enable backpropagation through stochastic nodes by transforming the mean and log-variance of a latent distribution into a sample from that distribution.\n\nArgs:\n    mu (torch.Tensor): Mean of the latent Gaussian distribution.\n    logvar (torch.Tensor): Log-variance of the latent Gaussian distribution.\n\nReturns:\n    torch.Tensor: A sample drawn from the latent distribution parameterized by mu and logvar.\n\"\"\"",
                "source_code": "std = torch.exp(logvar / 2)\n    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), opt.latent_dim))))\n    z = sampled_z * std + mu\n    return z"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/bicyclegan/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ImageDataset by setting up the transformation pipeline for image preprocessing and collecting sorted file paths from the specified directory. This ensures that all images are uniformly processed and accessible for subsequent loading and model training.\n\nArgs:\n    root (str): Root directory containing the dataset.\n    input_shape (tuple): Target shape for resizing images.\n    mode (str): Subdirectory or mode specifying which dataset partition to use (e.g., 'train' or 'test').\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transform = transforms.Compose(\n            [\n                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n            ]\n        )\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves a pair of correlated image segments by loading and splitting an image from the dataset, possibly applying horizontal flipping for augmentation, and transforming the segments for model input.\n\nThis facilitates training of models that learn mappings between image domains by providing paired data samples with varied augmentations.\n\nArgs:\n    index (int): Index of the image file to retrieve and process.\n\nReturns:\n    dict: A dictionary with keys \"A\" and \"B\", each containing a transformed image tensor representing the two halves of the original image.\n\"\"\"",
                    "source_code": "img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of images available in the dataset.\n\nThis enables seamless indexing and batching operations when working with data loaders.\n\nReturns:\n    int: The number of image files in the dataset.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/bicyclegan/models.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of network layers to promote stable and effective model training by setting convolutional layers' weights to a normal distribution with mean 0 and batch normalization layers' weights and biases to specific values. This helps ensure consistent model behavior and convergence during the early stages of training.\n\nArgs:\n    m (torch.nn.Module): A PyTorch layer whose weights will be initialized according to its type.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the downsampling block used in the U-Net architecture by defining a sequence of convolution, optional normalization, and non-linear activation layers. This configuration reduces spatial dimensions while extracting and refining feature representations, which is essential for effective hierarchical encoding in image-based neural networks.\n\nArgs:\n    in_size (int): Number of input channels for the convolutional layer.\n    out_size (int): Number of output channels produced by the convolutional layer.\n    normalize (bool): If True, applies batch normalization after convolution to stabilize training.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size, 0.8))\n        layers.append(nn.LeakyReLU(0.2))\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPasses the input tensor through the underlying model module for feature extraction and transformation. This is essential for encoding input data into a form suitable for subsequent stages of the architecture.\n\nArgs:\n    x (torch.Tensor): Input tensor representing an image or feature map to be processed by the module.\n\nReturns:\n    torch.Tensor: Output tensor after transformation by the model, capturing extracted features or representations.\n\"\"\"",
                    "source_code": "return self.model(x)"
                }
            ],
            "name": "UNetDown",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an upsampling and convolutional block used to increase the spatial dimensions and refine feature representations in a neural network.\n\nArgs:\n    in_size (int): Number of input feature channels.\n    out_size (int): Number of output feature channels.\n\nReturns:\n    None\n\nThis method constructs a sequential module consisting of upsampling, convolution, normalization, and activation layers, enabling the network to recover spatial information and enhance feature maps during the decoding process.\n\"\"\"",
                    "source_code": "super(UNetUp, self).__init__()\n        self.model = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_size, out_size, 3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_size, 0.8),\n            nn.ReLU(inplace=True),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms an upsampling operation followed by concatenation with a skip connection tensor, enabling the integration of feature maps from earlier layers with higher-resolution details from the encoder path.\n\nArgs:\n    x (torch.Tensor): Input tensor from the previous layer.\n    skip_input (torch.Tensor): Corresponding tensor from the encoder path to provide contextual information.\n\nReturns:\n    torch.Tensor: The concatenated tensor containing both the upsampled features and skip connection features, prepared for further processing in the decoding path.\n\"\"\"",
                    "source_code": "x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x"
                }
            ],
            "name": "UNetUp",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the components of the generator architecture, defining the sequence of downsampling and upsampling layers necessary for transforming input latent vectors and image information into generated images.\n\nArgs:\n    latent_dim (int): Dimensionality of the input latent vector.\n    img_shape (tuple): Shape of the output image as (channels, height, width).\n\nReturns:\n    None\n\nWhy:\n    This method sets up the neural network layers required to map a latent representation and image context through a deep encoder-decoder (U-Net-like) structure, enabling the generator to synthesize images from compact representations by progressively refining spatial details.\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n        channels, self.h, self.w = img_shape\n\n        self.fc = nn.Linear(latent_dim, self.h * self.w)\n\n        self.down1 = UNetDown(channels + 1, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512)\n        self.down5 = UNetDown(512, 512)\n        self.down6 = UNetDown(512, 512)\n        self.down7 = UNetDown(512, 512, normalize=False)\n        self.up1 = UNetUp(512, 512)\n        self.up2 = UNetUp(1024, 512)\n        self.up3 = UNetUp(1024, 512)\n        self.up4 = UNetUp(1024, 256)\n        self.up5 = UNetUp(512, 128)\n        self.up6 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, stride=1, padding=1), nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the generator network by conditioning on the latent vector and the input using a series of downsampling and upsampling layers, ultimately producing a generated output. This process enables the network to synthesize new data that shares desired properties with the input and supports robust feature learning through skip connections.\n\nArgs:\n    x (torch.Tensor): The input tensor, typically representing an image or data sample to be conditioned on.\n    z (torch.Tensor): The latent noise vector, used as a source of stochastic variation for generation.\n\nReturns:\n    torch.Tensor: The generated output tensor, typically representing a synthesized image or data sample.\n\"\"\"",
                    "source_code": "z = self.fc(z).view(z.size(0), 1, self.h, self.w)\n        d1 = self.down1(torch.cat((x, z), 1))\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        u1 = self.up1(d7, d6)\n        u2 = self.up2(u1, d5)\n        u3 = self.up3(u2, d4)\n        u4 = self.up4(u3, d3)\n        u5 = self.up5(u4, d2)\n        u6 = self.up6(u5, d1)\n\n        return self.final(u6)"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Encoder module by configuring a feature extractor based on a ResNet-18 backbone, followed by pooling and fully connected layers to produce parameters for a learned latent representation. This setup prepares input data for stochastic encoding, enabling efficient sampling and reconstruction in generative modeling tasks.\n\nArgs:\n    latent_dim (int): The dimensionality of the latent space to which images are encoded.\n\nReturns:\n    None\n\nWhy:\n    The method structures and processes input images into a compact, parameterized representation, which is essential for mapping complex data distributions into a tractable latent space used in generative models.\n\"\"\"",
                    "source_code": "super(Encoder, self).__init__()\n        resnet18_model = resnet18(pretrained=False)\n        self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n        self.pooling = nn.AvgPool2d(kernel_size=8, stride=8, padding=0)\n        # Output is mu and log(var) for reparameterization trick used in VAEs\n        self.fc_mu = nn.Linear(256, latent_dim)\n        self.fc_logvar = nn.Linear(256, latent_dim)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses the input image to extract compact, informative representations, outputting parameters that characterize a latent distribution for subsequent sampling or generation tasks.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be encoded.\n\nReturns:\n    Tuple[torch.Tensor, torch.Tensor]: Mean (mu) and log-variance (logvar) tensors representing the encoded latent distribution.\n\"\"\"",
                    "source_code": "out = self.feature_extractor(img)\n        out = self.pooling(out)\n        out = out.view(out.size(0), -1)\n        mu = self.fc_mu(out)\n        logvar = self.fc_logvar(out)\n        return mu, logvar"
                }
            ],
            "name": "Encoder",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes multiple discriminators and a downsampling layer, setting up the architecture to analyze input data at different scales or resolutions. This structure enhances the model's ability to distinguish between generated and real images by providing several perspectives on the input.\n\nArgs:\n    input_shape (tuple): Shape of the input images in the form (channels, height, width).\n    in_channels (int): Number of input channels for the downsampling layer.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(MultiDiscriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        channels, _, _ = input_shape\n        # Extracts discriminator models\n        self.models = nn.ModuleList()\n        for i in range(3):\n            self.models.add_module(\n                \"disc_%d\" % i,\n                nn.Sequential(\n                    *discriminator_block(channels, 64, normalize=False),\n                    *discriminator_block(64, 128),\n                    *discriminator_block(128, 256),\n                    *discriminator_block(256, 512),\n                    nn.Conv2d(512, 1, 3, padding=1)\n                ),\n            )\n\n        self.downsample = nn.AvgPool2d(in_channels, stride=2, padding=[1, 1], count_include_pad=False)"
                },
                {
                    "docstring": "\"\"\"Computes the MSE between model output and scalar gt\"\"\"",
                    "first_doc": "\"\"\"\nComputes the mean squared error (MSE) loss between model outputs and the ground truth scalar.\n\nArgs:\n    x: The input data to the model.\n    gt: The ground truth scalar value to compare against model outputs.\n\nReturns:\n    The computed mean squared error loss between model outputs and the ground truth.\n\"\"\"",
                    "method_name": "compute_loss",
                    "second_doc": "\"\"\"\nCalculates the total mean squared error loss by aggregating the differences between each discriminator output and the ground truth scalar. By summing these losses, the method provides a measure of how closely the model's predictions match the expected targets across multiple outputs. This guides the training process by penalizing large deviations, encouraging the model to produce more accurate outputs.\n\nArgs:\n    x: Input data passed through the model to obtain predictions.\n    gt: Ground truth scalar value used as the reference for evaluating prediction accuracy.\n\nReturns:\n    The total mean squared error loss computed by summing over all model outputs.\n\"\"\"",
                    "source_code": "loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n        return loss"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses the input through a sequence of discriminators, applying downsampling at each step, and collects the outputs from all discriminators in the series.\n\nThis approach enables capturing information at different feature scales by progressively downsampling the input and evaluating it with separate models.\n\nArgs:\n    x (torch.Tensor): Input tensor to be assessed by the sequence of discriminators.\n\nReturns:\n    list: A list containing the outputs of each discriminator for the corresponding input at its scale.\n\"\"\"",
                    "source_code": "outputs = []\n        for m in self.models:\n            outputs.append(m(x))\n            x = self.downsample(x)\n        return outputs"
                }
            ],
            "name": "MultiDiscriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/ccgan/ccgan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "apply_random_mask",
                "second_doc": "\"\"\"\nApplies a square mask of fixed size at a random location to each image in the input batch, replacing the masked region with a constant value. This simulates missing or occluded image regions, which can help to train models that are robust to incomplete or corrupted data.\n\nArgs:\n    imgs (torch.Tensor): A batch of images with shape (N, C, H, W), where N is the batch size.\n    opt: An object containing at least 'img_size' (int, dimensions of images) and 'mask_size' (int, dimensions of the square mask).\n\nReturns:\n    torch.Tensor: A batch of images with randomly placed masked regions for each input image.\n\"\"\"",
                "source_code": "idx = np.random.randint(0, opt.img_size - opt.mask_size, (imgs.shape[0], 2))\n\n    masked_imgs = imgs.clone()\n    for i, (y1, x1) in enumerate(idx):\n        y2, x2 = y1 + opt.mask_size, x1 + opt.mask_size\n        masked_imgs[i, :, y1:y2, x1:x2] = -1\n\n    return masked_imgs"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "save_sample",
                "second_doc": "\"\"\"\nGenerates and saves a visual comparison of masked input images, their corresponding GAN-generated outputs, and the original images to monitor the training progress and evaluate model performance.\n\nArgs:\n    generator (nn.Module): The GAN generator model used to create generated images.\n    saved_samples (dict): A dictionary containing tensors for 'masked', 'lowres', and 'imgs' images.\n    batches_done (int): The current training iteration used for naming the saved image file.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "gen_imgs = generator(saved_samples[\"masked\"], saved_samples[\"lowres\"])\n    # Save sample\n    sample = torch.cat((saved_samples[\"masked\"].data, gen_imgs.data, saved_samples[\"imgs\"].data), -2)\n    save_image(sample, \"images/%d.png\" % batches_done, nrow=5, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/ccgan/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the dataset by defining image transformation pipelines for input data and low-resolution data, and collects all image file paths in the specified directory.\n\nArgs:\n    root (str): Directory containing the image files.\n    transforms_x (list): List of transformations to apply to the input images.\n    transforms_lr (list): List of transformations to apply to the low-resolution images.\n\nReturns:\n    None\n\nWhy:  \nThis method prepares and organizes image data, ensuring it is properly preprocessed and easily accessible for subsequent tasks such as training deep learning models on image-related problems.\n\"\"\"",
                    "source_code": "self.transform_x = transforms.Compose(transforms_x)\n        self.transform_lr = transforms.Compose(transforms_lr)\n\n        self.files = sorted(glob.glob('%s/*.*' % root))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves and processes an image and its low-resolution counterpart from the dataset for model input.\n\nArgs:\n    index (int): Index of the desired image in the dataset.\n\nReturns:\n    dict: A dictionary containing the processed high-resolution image ('x') and its low-resolution version ('x_lr').\n\nThis method enables consistent retrieval and transformation of image data, preparing paired inputs for training and evaluation of models that require both original and downsampled images.\n\"\"\"",
                    "source_code": "img = Image.open(self.files[index % len(self.files)])\n\n        x = self.transform_x(img)\n        x_lr = self.transform_lr(img)\n\n        return {'x': x, 'x_lr': x_lr}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of image files available in the dataset.  \nThis allows iteration and batching utilities to determine the dataset's length.\n\nReturns:\n    int: Number of image files in the dataset.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/ccgan/models.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a downsampling block commonly used in encoder-based neural networks.\n\nThis method constructs a sequence of layers that applies convolution, optional normalization, nonlinearity, and optional dropout. The purpose is to reduce the spatial resolution of input features while increasing the number of feature channels, making it easier for subsequent network layers to learn hierarchical representations for complex tasks.\n\nArgs:\n    in_size (int): Number of input feature channels.\n    out_size (int): Number of output feature channels.\n    normalize (bool): Whether to apply batch normalization after convolution.\n    dropout (float, optional): Dropout probability. If provided, dropout regularization is applied to the output.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(UNetDown, self).__init__()\n        model = [nn.Conv2d(in_size, out_size, 4, stride=2, padding=1, bias=False)]\n        if normalize:\n            model.append(nn.BatchNorm2d(out_size, 0.8))\n        model.append(nn.LeakyReLU(0.2))\n        if dropout:\n            model.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*model)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies the downsampling model layers to the input tensor, transforming the data for subsequent processing in the network.\n\nArgs:\n    x (torch.Tensor): Input feature tensor to be downsampled.\n\nReturns:\n    torch.Tensor: The transformed tensor after passing through the model's downsampling layers.\n\"\"\"",
                    "source_code": "return self.model(x)"
                }
            ],
            "name": "UNetDown",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the upsampling block used in U-Net, constructing a sequence of layers that perform learned upsampling followed by normalization, non-linearity, and optional dropout. This design enables effective feature map size restoration while preserving important learned representations during the decoding stage of the network.\n\nArgs:\n    in_size (int): Number of input feature channels.\n    out_size (int): Number of output feature channels.\n    dropout (float, optional): Dropout probability. If provided, dropout is applied after the activation layer.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(UNetUp, self).__init__()\n        model = [\n            nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_size, 0.8),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            model.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*model)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms an upsampling step and concatenates the result with a skip connection input, enhancing feature representation for subsequent processing.\n\nArgs:\n    x (torch.Tensor): The input tensor to be upsampled by the model.\n    skip_input (torch.Tensor): The tensor from the encoder path providing detailed spatial features for concatenation.\n\nReturns:\n    torch.Tensor: The concatenated tensor combining the upsampled and skip connection features.\n\"\"\"",
                    "source_code": "x = self.model(x)\n        out = torch.cat((x, skip_input), 1)\n        return out"
                }
            ],
            "name": "UNetUp",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator network by defining a series of downsampling and upsampling layers based on the provided input shape. These layers extract features and reconstruct images through a U-Net-like architecture with skip connections, enabling effective transformation of input data to high-quality outputs.\n\nArgs:\n    input_shape (tuple): Shape of the input tensor, typically (channels, height, width), which determines the initial configuration of convolutional layers.\n\nReturns:\n    None\n\nWhy:\n    The method configures the Generator's layers to efficiently capture and repurpose spatial information at multiple scales, supporting the accurate synthesis or translation of complex images from input data.\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n        channels, _, _ = input_shape\n        self.down1 = UNetDown(channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128 + channels, 256, dropout=0.5)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 256, dropout=0.5)\n        self.up4 = UNetUp(512, 128)\n        self.up5 = UNetUp(256 + channels, 64)\n\n        final = [nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, 1, 1), nn.Tanh()]\n        self.final = nn.Sequential(*final)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms the forward pass through the network by encoding the input via a sequence of downsampling layers, concatenating features at a specified layer with an additional input, then decoding with upsampling layers and skip connections, producing the final output. This process enables the model to transform or enhance the input data while preserving important features through multi-scale representations.\n\nArgs:\n    x (torch.Tensor): The main input tensor, typically representing an image.\n    x_lr (torch.Tensor): An auxiliary input tensor concatenated with intermediate features to provide additional information during generation.\n\nReturns:\n    torch.Tensor: The output tensor produced by the generator, typically an image with desired transformations or enhancements.\n\"\"\"",
                    "source_code": "d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d2 = torch.cat((d2, x_lr), 1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        u1 = self.up1(d6, d5)\n        u2 = self.up2(u1, d4)\n        u3 = self.up3(u2, d3)\n        u4 = self.up4(u3, d2)\n        u5 = self.up5(u4, d1)\n\n        return self.final(u5)"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator model by constructing a sequential neural network composed of convolutional, normalization, and activation layers to process an input image and produce an output suitable for distinguishing real from generated content at the patch level.\n\nArgs:\n    input_shape (tuple): A tuple specifying the (channels, height, width) of the input images.\n\nReturns:\n    None\n\nWhy:\n    This method builds the model architecture necessary for evaluating the authenticity of image patches, enabling fine-grained judgment essential for adversarial training and improving the quality of generated outputs.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n        # Calculate output of image discriminator (PatchGAN)\n        patch_h, patch_w = int(height / 2 ** 3), int(width / 2 ** 3)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, stride, normalize):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = channels\n        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass of the input image through the discriminator model to assess its authenticity. This allows the network to distinguish between real and generated images, which is crucial for training adversarial models.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be evaluated by the discriminator.\n\nReturns:\n    torch.Tensor: Output tensor representing the discriminator's decision on the input image's authenticity.\n\"\"\"",
                    "source_code": "return self.model(img)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/cgan/cgan.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator network by constructing label embeddings and a sequential series of fully connected layers. These components work together to integrate noise and class information, transforming them into an output suitable for synthesizing new data instances. This setup allows the Generator to produce data conditioned on class labels, enabling controlled generation.\n\nArgs:\n    opt: An object containing configuration options, including n_classes (number of classes), latent_dim (dimension of noise input), and img_shape (desired output image shape).\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim + opt.n_classes, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates an image conditioned on input noise and label information by combining label embeddings with noise and processing it through the generator model.\n\nArgs:\n    noise (Tensor): Random noise tensor used for image generation, typically of shape (batch_size, latent_dim).\n    labels (Tensor): Tensor of class labels, typically of shape (batch_size,) or (batch_size, 1).\n\nReturns:\n    Tensor: Generated images reshaped to the specified output image shape.\n    \nWhy:\n    This method combines noise with label information to enable the generation of diverse and label-specific synthetic images, facilitating tasks that require conditional image creation.\n\"\"\"",
                    "source_code": "gen_input = torch.cat((self.label_emb(labels), noise), -1)\n        img = self.model(gen_input)\n        img = img.view(img.size(0), *img_shape)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator network, constructing label embeddings and a multi-layer neural architecture for distinguishing between real and generated data conditioned on class information.\n\nArgs:\n    opt: An object containing configuration parameters, including 'n_classes' for the number of classes and 'img_shape' for the shape of input images.\n\nReturns:\n    None\n\nWhy:\n    The method builds the discriminator's structure to effectively evaluate the authenticity of input samples in conjunction with their class labels, enabling conditional adversarial training for higher-quality and class-consistent generated outputs.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 512),\n            nn.Dropout(0.4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 1),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses the input image and its corresponding class labels by concatenating their representations before assessing their authenticity through the model.\n\nArgs:\n    img (torch.Tensor): Input batch of images.\n    labels (torch.Tensor): Class labels associated with each image in the batch.\n\nReturns:\n    torch.Tensor: Authenticity scores indicating the likelihood that each input image is real or generated.\n\"\"\"",
                    "source_code": "d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n        validity = self.model(d_in)\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"",
                "first_doc": "\"\"\"\nGenerates and saves a grid of sample images using the generator model.\n\nArgs:\n    n_row: The number of rows and columns in the grid of generated images.\n    batches_done: The current number of training batches processed, used for naming the output image file.\n\nReturns:\n    None: This method does not return a value; it saves an image file containing the generated samples.\n\"\"\"",
                "method_name": "sample_image",
                "second_doc": "\"\"\"\nCreates and saves a grid of images produced by the generator model to visually track generative performance and progression during network training.\n\nArgs:\n    n_row (int): The number of images per row and column in the output grid, controlling the grid layout of generated samples.\n    batches_done (int): Identifier for the number of training batches completed, used to name the output image file for easy chronological tracking.\n\nReturns:\n    None: Saves a grid image of generated samples to file; does not return a value.\n\"\"\"",
                "source_code": "# Sample noise\n    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n    # Get labels ranging from 0 to n_classes for n rows\n    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n    labels = Variable(LongTensor(labels))\n    gen_imgs = generator(z, labels)\n    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/cluster_gan/clustergan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "sample_z",
                "second_doc": "\"\"\"\nGenerates a batch of latent vectors and corresponding conditional class encodings for use as input to a generative model, enabling controlled or random class sampling depending on the specified parameters.\n\nArgs:\n    shape (int): Number of samples to generate in the batch.\n    latent_dim (int): Dimensionality of the continuous latent noise vector.\n    n_c (int): Number of classes for conditional code.\n    req_grad (bool): If True, enables gradient tracking on the output variables.\n    fix_class (int): If set to -1, samples class labels randomly; otherwise, uses the specified class index for all samples.\n\nReturns:\n    zn (torch.autograd.Variable): Continuous latent noise vectors sampled from a scaled normal distribution.\n    zc (torch.autograd.Variable): One-hot encoded class vectors, with grad option set according to req_grad.\n    zc_idx (torch.Tensor): Tensor of integer class indices corresponding to each sample in the batch.\n\"\"\"",
                "source_code": "assert (fix_class == -1 or (fix_class >= 0 and fix_class < n_c) ), \"Requested class %i outside bounds.\"%fix_class\n\n    Tensor = torch.cuda.FloatTensor\n    \n    # Sample noise as generator input, zn\n    zn = Variable(Tensor(0.75*np.random.normal(0, 1, (shape, latent_dim))), requires_grad=req_grad)\n\n    ######### zc, zc_idx variables with grads, and zc to one-hot vector\n    # Pure one-hot vector generation\n    zc_FT = Tensor(shape, n_c).fill_(0)\n    zc_idx = torch.empty(shape, dtype=torch.long)\n\n    if (fix_class == -1):\n        zc_idx = zc_idx.random_(n_c).cuda()\n        zc_FT = zc_FT.scatter_(1, zc_idx.unsqueeze(1), 1.)\n    else:\n        zc_idx[:] = fix_class\n        zc_FT[:, fix_class] = 1\n\n        zc_idx = zc_idx.cuda()\n        zc_FT = zc_FT.cuda()\n\n    zc = Variable(zc_FT, requires_grad=req_grad)\n\n    # Return components of latent space variable\n    return zn, zc, zc_idx"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "calc_gradient_penalty",
                "second_doc": "\"\"\"\nCalculates the gradient penalty term used to enforce the Lipschitz constraint during adversarial training. By interpolating between real and generated samples and measuring the norm of the discriminator's gradients with respect to these interpolations, the method helps stabilize the discriminator's behavior, preventing overfitting and improving the quality of generated outputs.\n\nArgs:\n    netD (nn.Module): Discriminator network used to distinguish between real and generated data.\n    real_data (torch.Tensor): Batch of real input data.\n    generated_data (torch.Tensor): Batch of data produced by the generator.\n\nReturns:\n    torch.Tensor: Scalar tensor representing the gradient penalty to be added to the discriminator loss.\n\"\"\"",
                "source_code": "LAMBDA = 10\n\n    b_size = real_data.size()[0]\n\n    # Calculate interpolation\n    alpha = torch.rand(b_size, 1, 1, 1)\n    alpha = alpha.expand_as(real_data)\n    alpha = alpha.cuda()\n    \n    interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n    interpolated = Variable(interpolated, requires_grad=True)\n    interpolated = interpolated.cuda()\n\n    # Calculate probability of interpolated examples\n    prob_interpolated = netD(interpolated)\n\n    # Calculate gradients of probabilities with respect to examples\n    gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n                           grad_outputs=torch.ones(prob_interpolated.size()).cuda(),\n                           create_graph=True, retain_graph=True)[0]\n\n    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n    # so flatten to easily take norm per example in batch\n    gradients = gradients.view(b_size, -1)\n\n    # Derivatives of the gradient close to 0 can cause problems because of\n    # the square root, so manually calculate norm and add epsilon\n    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n\n    # Return gradient penalty\n    return LAMBDA * ((gradients_norm - 1) ** 2).mean()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "initialize_weights",
                "second_doc": "\"\"\"\nInitializes the weights and biases of all convolutional and linear layers in the given network using a normal distribution for weights (mean=0, std=0.02) and zeros for biases.\n\nThis approach helps stabilize and standardize the training process of neural networks, which is especially important in settings where training dynamics can be sensitive to initial parameter values.\n\nArgs:\n    net (torch.nn.Module): The neural network whose weights and biases are to be initialized.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.ConvTranspose2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "softmax",
                "second_doc": "\"\"\"\nApplies the softmax activation function to the input tensor along the specified dimension to convert raw model outputs into probabilities. This is essential for interpreting outputs as normalized likelihoods in tasks such as classification, decision making, or generating attention distributions.\n\nArgs:\n    x (torch.Tensor): Input tensor representing unnormalized model outputs (logits).\n\nReturns:\n    torch.Tensor: Tensor with the same shape as input, where elements along the specified dimension are probabilities that sum to 1.\n\"\"\"",
                "source_code": "return F.softmax(x, dim=1)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Reshape class by setting the desired output shape for input tensors. This setup enables flexible reshaping of data within a deep learning model, facilitating compatibility between different layers.\n\nArgs:\n    shape (tuple or int): The target shape to which the input tensor will be reshaped.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Reshape, self).__init__()\n        self.shape = shape"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nReshapes the input tensor to a specified shape while preserving the batch size. This enables flexible transformation of data dimensions to match the requirements of subsequent neural network layers.\n\nArgs:\n    x (torch.Tensor): Input tensor of arbitrary shape, where the first dimension is expected to be the batch size.\n\nReturns:\n    torch.Tensor: Tensor reshaped to have its first dimension as the original batch size and the remaining dimensions as defined by self.shape.\n\"\"\"",
                    "source_code": "return x.view(x.size(0), *self.shape)"
                },
                {
                    "docstring": null,
                    "method_name": "extra_repr",
                    "second_doc": "\"\"\"\nProvides a concise string representation of the target shape for reshaping operations, which aids in clearly communicating the transformation that the layer will apply. This helps users and developers understand the structural changes imposed by the layer when inspecting or debugging the model.\n\nReturns:\n    str: A string indicating the new shape specified for the reshaping operation.\n\"\"\"",
                    "source_code": "return 'shape={}'.format(\n                self.shape\n            )"
                }
            ],
            "name": "Reshape",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator_CNN network architecture with layers designed to transform latent representations and class information into structured image outputs using sequential linear and convolutional operations.\n\nArgs:\n    latent_dim (int): Dimensionality of the input latent noise vector.\n    n_c (int): Number of class labels or conditional inputs.\n    x_shape (tuple): Shape of the desired output image (channels, height, width).\n    verbose (bool): If True, prints the model architecture for inspection.\n\nReturns:\n    None\n\nWhy:\n    This method sets up the generator's neural network structure to allow transformation of random noise and conditional inputs into realistic image data, enabling controlled sampling and learning in generative frameworks.\n\"\"\"",
                    "source_code": "super(Generator_CNN, self).__init__()\n\n        self.name = 'generator'\n        self.latent_dim = latent_dim\n        self.n_c = n_c\n        self.x_shape = x_shape\n        self.ishape = (128, 7, 7)\n        self.iels = int(np.prod(self.ishape))\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            # Fully connected layers\n            torch.nn.Linear(self.latent_dim + self.n_c, 1024),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, self.iels),\n            nn.BatchNorm1d(self.iels),\n            nn.LeakyReLU(0.2, inplace=True),\n        \n            # Reshape to 128 x (7x7)\n            Reshape(self.ishape),\n\n            # Upconvolution layers\n            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=True),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=True),\n            nn.Sigmoid()\n        )\n\n        initialize_weights(self)\n\n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates output data by processing the concatenation of noise and conditional input vectors through the generator network.\n\nArgs:\n    zn (torch.Tensor): Latent noise vector.\n    zc (torch.Tensor): Conditional input vector.\n\nReturns:\n    torch.Tensor: Generated output, reshaped to the expected data format.\n    \nThe method combines the latent and conditional inputs to synthesize new data samples that resemble the target distribution, enabling the generator to learn and produce realistic outputs.\n\"\"\"",
                    "source_code": "z = torch.cat((zn, zc), 1)\n        x_gen = self.model(z)\n        # Reshape for output\n        x_gen = x_gen.view(x_gen.size(0), *self.x_shape)\n        return x_gen"
                }
            ],
            "name": "Generator_CNN",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the encoder component by constructing a convolutional neural network that extracts features from input data and compresses them into a compact latent representation.\n\nArgs:\n    latent_dim (int): Dimensionality of the latent space for encoding.\n    n_c (int): Additional output features, typically for class labels or auxiliary variables.\n    verbose (bool): If True, prints debugging information about the model architecture.\n\nReturns:\n    None\n\nWhy:\n    Compressing high-dimensional input data into a lower-dimensional latent representation enables efficient downstream processing, such as reconstruction or generation within adversarial or generative models.\n\"\"\"",
                    "source_code": "super(Encoder_CNN, self).__init__()\n\n        self.name = 'encoder'\n        self.channels = 1\n        self.latent_dim = latent_dim\n        self.n_c = n_c\n        self.cshape = (128, 5, 5)\n        self.iels = int(np.prod(self.cshape))\n        self.lshape = (self.iels,)\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            # Convolutional layers\n            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Flatten\n            Reshape(self.lshape),\n            \n            # Fully connected layers\n            torch.nn.Linear(self.iels, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, latent_dim + n_c)\n        )\n\n        initialize_weights(self)\n        \n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses input features through the encoder network to extract and separate continuous latent vectors and categorical representations, enabling flexible manipulation and structured encoding of input data.\n\nArgs:\n    in_feat (Tensor): Input feature tensor, typically representing images.\n\nReturns:\n    tuple: A tuple containing:\n        - zn (Tensor): Continuous latent vector extracted from the input.\n        - zc (Tensor): Categorical latent vector probabilities obtained via softmax.\n        - zc_logits (Tensor): Raw logits of the categorical component before softmax.\n\"\"\"",
                    "source_code": "z_img = self.model(in_feat)\n        # Reshape for output\n        z = z_img.view(z_img.shape[0], -1)\n        # Separate continuous and one-hot components\n        zn = z[:, 0:self.latent_dim]\n        zc_logits = z[:, self.latent_dim:]\n        # Softmax on zc component\n        zc = softmax(zc_logits)\n        return zn, zc, zc_logits"
                }
            ],
            "name": "Encoder_CNN",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the discriminator network architecture with convolutional and fully connected layers to enable effective differentiation between authentic and generated inputs. The design is configured based on standard settings to suit diverse adversarial learning scenarios, allowing flexibility for different evaluation metrics.\n\nArgs:\n    wass_metric (bool): Specifies whether to use the Wasserstein loss metric. If False, applies a Sigmoid activation to the final layer.\n    verbose (bool): If True, prints details of the model being initialized.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator_CNN, self).__init__()\n        \n        self.name = 'discriminator'\n        self.channels = 1\n        self.cshape = (128, 5, 5)\n        self.iels = int(np.prod(self.cshape))\n        self.lshape = (self.iels,)\n        self.wass = wass_metric\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            # Convolutional layers\n            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Flatten\n            Reshape(self.lshape),\n            \n            # Fully connected layers\n            torch.nn.Linear(self.iels, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, 1),\n        )\n        \n        # If NOT using Wasserstein metric, final Sigmoid\n        if (not self.wass):\n            self.model = nn.Sequential(self.model, torch.nn.Sigmoid())\n\n        initialize_weights(self)\n\n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nFeeds an input image through the discriminator network to obtain its validity score, indicating how likely it is to be a real sample. This process supports the training of generative models by providing feedback on generated images.\n\nArgs:\n    img (Tensor): Input image tensor to be evaluated by the discriminator.\n\nReturns:\n    Tensor: Validity score(s) representing the discriminator's assessment of the input image(s).\n\"\"\"",
                    "source_code": "validity = self.model(img)\n        return validity"
                }
            ],
            "name": "Discriminator_CNN",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/cogan/cogan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of neural network layers to promote stable training and convergence.\n\nArgs:\n    m (nn.Module): The neural network module to initialize. Typically used as a callback within model initialization.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Linear\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the architecture for two coupled generator networks with shared and individual layers. By constructing a shared upsampling and convolutional path before branching into two separate generator sequences, this design enables the model to generate related but distinct output images from common latent representations. This facilitates the mapping of random noise to coherent outputs that retain both shared and unique features.\n\nArgs:\n    opt: Namespace object containing configuration parameters such as image size, latent dimension, and number of output channels.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(CoupledGenerators, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.fc = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.shared_conv = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n        )\n        self.G1 = nn.Sequential(\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n        self.G2 = nn.Sequential(\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses input noise through fully connected and convolutional layers to generate two distinct images sharing a common representation. This approach enables the production of related but individual outputs from the same latent vector.\n\nArgs:\n    noise (Tensor): Input noise tensor, typically drawn from a standard normal distribution, used as the seed for image generation.\n\nReturns:\n    Tuple[Tensor, Tensor]: A tuple containing two generated images, each represented as a tensor.\n\"\"\"",
                    "source_code": "out = self.fc(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img_emb = self.shared_conv(out)\n        img1 = self.G1(img_emb)\n        img2 = self.G2(img_emb)\n        return img1, img2"
                }
            ],
            "name": "CoupledGenerators",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the CoupledDiscriminators module by setting up a shared convolutional feature extractor and two parallel linear classifiers.\n\nArgs:\n    opt: An object containing configuration options, including the number of image channels and the target image size.\n\nReturns:\n    None\n\nThis setup allows the model to learn feature representations shared between domains or tasks, while enabling independent discrimination through separate output heads.\n\"\"\"",
                    "source_code": "super(CoupledDiscriminators, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            block.extend([nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)])\n            return block\n\n        self.shared_conv = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.D1 = nn.Linear(128 * ds_size ** 2, 1)\n        self.D2 = nn.Linear(128 * ds_size ** 2, 1)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through two coupled discriminators, evaluating the authenticity of two input images via shared convolutional layers. This approach allows both discriminators to extract common representations before making separate validity predictions for each image, enabling comparison or domain adaptation scenarios where structural features are shared.\n\nArgs:\n    img1 (torch.Tensor): The first input image tensor.\n    img2 (torch.Tensor): The second input image tensor.\n\nReturns:\n    Tuple[torch.Tensor, torch.Tensor]: The predicted validity scores for img1 and img2, as determined by the respective discriminators.\n\"\"\"",
                    "source_code": "out = self.shared_conv(img1)\n        out = out.view(out.shape[0], -1)\n        validity1 = self.D1(out)\n        # Determine validity of second image\n        out = self.shared_conv(img2)\n        out = out.view(out.shape[0], -1)\n        validity2 = self.D2(out)\n\n        return validity1, validity2"
                }
            ],
            "name": "CoupledDiscriminators",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/cogan/mnistm.py": [
        {
            "methods": [
                {
                    "docstring": "\"\"\"Init MNIST-M dataset.\"\"\"",
                    "first_doc": "\"\"\"\nInitializes the MNIST-M dataset by setting up required paths and configurations.\n\nArgs:\n    self: The instance of the MNISTM class.\n    root: Path to the directory where the MNIST-M dataset is or will be stored.\n\nAttributes:\n    root: The expanded path to the directory where the MNIST-M dataset is stored.\n    mnist_root: The expanded path to the directory where the original MNIST dataset is stored.\n    transform: The transformation function or pipeline applied to the input images.\n    target_transform: The transformation function or pipeline applied to the target labels.\n    train: Boolean value indicating whether the dataset corresponds to the training set (if True) or test set (if False).\n    train_data: The image data for training (set only if train is True).\n    train_labels: The labels for the training data (set only if train is True).\n    test_data: The image data for testing (set only if train is False).\n    test_labels: The labels for the testing data (set only if train is False).\n\nReturns:\n    None. This is a class constructor and does not return a value.\n\"\"\"",
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nPrepares the MNIST-M dataset by configuring file paths, handling optional downloading, and loading processed data for training or testing.\n\nArgs:\n    self: Instance of the MNISTM class.\n    root (str): Directory path where the MNIST-M dataset is or will be stored.\n    mnist_root (str): Directory path for the original MNIST dataset.\n    transform (callable, optional): Function to apply to the input images.\n    target_transform (callable, optional): Function to apply to the target labels.\n    train (bool, optional): If True, selects training set; otherwise, selects test set.\n    download (bool, optional): If True, downloads the dataset if it is not found.\n\nReturns:\n    None\n\nWhy:\n    This method ensures reliable access to needed data by validating dataset presence, managing storage location, and supporting automated download and postprocessing as needed. This streamlines downstream workflows that rely on consistent and ready-to-use image datasets.\n\"\"\"",
                    "source_code": "super(MNISTM, self).__init__()\n        self.root = os.path.expanduser(root)\n        self.mnist_root = os.path.expanduser(mnist_root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train  # training set or test set\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError(\"Dataset not found.\" + \" You can use download=True to download it\")\n\n        if self.train:\n            self.train_data, self.train_labels = torch.load(\n                os.path.join(self.root, self.processed_folder, self.training_file)\n            )\n        else:\n            self.test_data, self.test_labels = torch.load(\n                os.path.join(self.root, self.processed_folder, self.test_file)\n            )"
                },
                {
                    "docstring": "\"\"\"Get images and target for data loader.\n\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"",
                    "first_doc": "\"\"\"\nRetrieves an image and its corresponding target given an index, for use with a data loader.\n\nArgs:\n    index: The index of the desired sample in the dataset.\n\nReturns:\n    tuple: A tuple (image, target), where image is a PIL Image (optionally transformed), and target is the label or target class (optionally transformed).\n\"\"\"",
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nFetches and processes an image and its label at the specified index, ensuring the image is converted to a consistent format and optional transforms are applied for use in downstream deep learning workflows.\n\nArgs:\n    index (int): The position of the desired sample in the dataset.\n\nReturns:\n    tuple: (image, target), where image is a PIL Image (optionally transformed), and target is the corresponding label or class (optionally transformed).\n    \nWhy:\n    This method standardizes data retrieval and preprocessing, simplifying integration with data loaders and machine learning models that rely on uniform input formats.\n\"\"\"",
                    "source_code": "if self.train:\n            img, target = self.train_data[index], self.train_labels[index]\n        else:\n            img, target = self.test_data[index], self.test_labels[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(img.squeeze().numpy(), mode=\"RGB\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target"
                },
                {
                    "docstring": "\"\"\"Return size of dataset.\"\"\"",
                    "first_doc": "\"\"\"\nReturns the number of samples in the dataset.\n\nIf the instance is in training mode, returns the size of the training dataset; otherwise, returns the size of the test dataset.\n\nArgs:\n    self: The instance of the dataset class.\n\nReturns:\n    int: The number of samples in the current mode's dataset (training or test).\n\"\"\"",
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nDetermines the number of data samples available based on the current mode (training or testing), enabling proper batching and data loading during model development and evaluation.\n\nArgs:\n    self: Instance of the dataset class, containing data and mode information.\n\nReturns:\n    int: The count of samples in either the training or test set, depending on the mode.\n\"\"\"",
                    "source_code": "if self.train:\n            return len(self.train_data)\n        else:\n            return len(self.test_data)"
                },
                {
                    "docstring": null,
                    "method_name": "_check_exists",
                    "second_doc": "\"\"\"\nChecks whether the necessary processed training and test files exist in the designated folder to ensure the dataset is ready for use.\n\nArgs:\n    None\n\nReturns:\n    bool: True if both the processed training and test files are found; False otherwise.\n\nWhy:\n    Verifying the existence of these files prevents unnecessary reprocessing and ensures that data loading operations can proceed smoothly without interruptions, optimizing workflow efficiency.\n\"\"\"",
                    "source_code": "return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and os.path.exists(\n            os.path.join(self.root, self.processed_folder, self.test_file)\n        )"
                },
                {
                    "docstring": "\"\"\"Download the MNIST data.\"\"\"",
                    "first_doc": "\"\"\"\nDownloads and processes the MNIST-M dataset, saving preprocessed files locally for future use.\n\nArgs:\n    self: The instance of the class.\n\nClass Fields Initialized:\n    root: The base directory path where dataset folders will be created and data files stored.\n    raw_folder: The directory name under root where raw (downloaded) files are placed.\n    processed_folder: The directory name under root where processed data will be saved.\n    url: The URL from which to download the dataset.\n    mnist_root: The root directory for original MNIST data, needed for fetching labels.\n    training_file: The filename for the processed training data file.\n    test_file: The filename for the processed test data file.\n\nReturns:\n    None: This method does not return a value. It performs data download and file I/O operations.\n\"\"\"",
                    "method_name": "download",
                    "second_doc": "\"\"\"\nPrepares the MNIST-M dataset by downloading, extracting, and saving it in a preprocessed format for local use. This enables efficient and consistent access to the dataset for subsequent model training and evaluation tasks.\n\nArgs:\n    self: Instance of the MNISTM class, containing configuration and file path information.\n\nReturns:\n    None: The method performs file downloads, extraction, preprocessing, and saves the resulting data, but does not return a value.\n\nWhy:\n    Ensuring the dataset is available locally in a ready-to-use format streamlines the workflow for machine learning experiments and eliminates redundant downloads and data processing steps for future runs.\n\"\"\"",
                    "source_code": "# import essential packages\n        from six.moves import urllib\n        import gzip\n        import pickle\n        from torchvision import datasets\n\n        # check if dataset already exists\n        if self._check_exists():\n            return\n\n        # make data dirs\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        # download pkl files\n        print(\"Downloading \" + self.url)\n        filename = self.url.rpartition(\"/\")[2]\n        file_path = os.path.join(self.root, self.raw_folder, filename)\n        if not os.path.exists(file_path.replace(\".gz\", \"\")):\n            data = urllib.request.urlopen(self.url)\n            with open(file_path, \"wb\") as f:\n                f.write(data.read())\n            with open(file_path.replace(\".gz\", \"\"), \"wb\") as out_f, gzip.GzipFile(file_path) as zip_f:\n                out_f.write(zip_f.read())\n            os.unlink(file_path)\n\n        # process and save as torch files\n        print(\"Processing...\")\n\n        # load MNIST-M images from pkl file\n        with open(file_path.replace(\".gz\", \"\"), \"rb\") as f:\n            mnist_m_data = pickle.load(f, encoding=\"bytes\")\n        mnist_m_train_data = torch.ByteTensor(mnist_m_data[b\"train\"])\n        mnist_m_test_data = torch.ByteTensor(mnist_m_data[b\"test\"])\n\n        # get MNIST labels\n        mnist_train_labels = datasets.MNIST(root=self.mnist_root, train=True, download=True).train_labels\n        mnist_test_labels = datasets.MNIST(root=self.mnist_root, train=False, download=True).test_labels\n\n        # save MNIST-M dataset\n        training_set = (mnist_m_train_data, mnist_train_labels)\n        test_set = (mnist_m_test_data, mnist_test_labels)\n        with open(os.path.join(self.root, self.processed_folder, self.training_file), \"wb\") as f:\n            torch.save(training_set, f)\n        with open(os.path.join(self.root, self.processed_folder, self.test_file), \"wb\") as f:\n            torch.save(test_set, f)\n\n        print(\"Done!\")"
                }
            ],
            "name": "MNISTM",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/context_encoder/context_encoder.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional and batch normalization layers with specific normal distributions to promote stable and effective neural network training.\n\nArgs:\n    m (torch.nn.Module): The module whose weights are to be initialized.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "save_sample",
                "second_doc": "\"\"\"\nGenerates and saves a set of sample images that visualize the effect of image inpainting by combining the original, masked, and generated images. This helps in monitoring the progress and quality of the model's image completion during training.\n\nArgs:\n    test_dataloader (DataLoader): DataLoader providing masked and original image samples for evaluation.\n    generator (nn.Module): The GAN-based generator model used to fill in masked regions of images.\n    Tensor (torch.dtype): Data type specification for input tensors.\n    opt (object): Configuration object that contains mask parameters like mask size.\n    batches_done (int): Counter that indicates how many batches have been processed, used for naming the saved image.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "samples, masked_samples, i = next(iter(test_dataloader))\n    samples = Variable(samples.type(Tensor))\n    masked_samples = Variable(masked_samples.type(Tensor))\n    i = i[0].item()  # Upper-left coordinate of mask\n    # Generate inpainted image\n    gen_mask = generator(masked_samples)\n    filled_samples = masked_samples.clone()\n    filled_samples[:, :, i : i + opt.mask_size, i : i + opt.mask_size] = gen_mask\n    # Save sample\n    sample = torch.cat((masked_samples.data, filled_samples.data, samples.data), -2)\n    save_image(sample, \"images/%d.png\" % batches_done, nrow=6, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/context_encoder/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ImageDataset object by setting transformation pipeline, image and mask sizes, dataset mode, and collecting image file paths.  \nThis configuration ensures the dataset provides correctly transformed and appropriately partitioned images for model training and evaluation.\n\nArgs:\n    root (str): Path to the directory containing image files.\n    transforms_ (list): List of transformation functions to apply to each image.\n    img_size (int): The size to which images will be resized.\n    mask_size (int): The size for the mask to be applied to images.\n    mode (str): Dataset mode, typically \"train\" or another value to select a subset of images.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transform = transforms.Compose(transforms_)\n        self.img_size = img_size\n        self.mask_size = mask_size\n        self.mode = mode\n        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n        self.files = self.files[:-4000] if mode == \"train\" else self.files[-4000:]"
                },
                {
                    "docstring": "\"\"\"Randomly masks image\"\"\"",
                    "first_doc": "\"\"\"\nApplies a random square mask to the input image.\n\nRandomly selects a region within the image of size defined by the instance's configuration and replaces that area with ones, effectively masking a portion of the image. Also returns the original content of the masked region.\n\nArgs:\n    self: The instance of the class. Expects the class to have 'img_size' (the height/width of the image) and 'mask_size' (the height/width of the mask region).\n    img: The input image tensor to which the mask will be applied.\n\nReturns:\n    tuple: A tuple containing:\n        - masked_img: The image after a random region has been masked.\n        - masked_part: The original content from the region that was masked.\n\"\"\"",
                    "method_name": "apply_random_mask",
                    "second_doc": "\"\"\"\nRandomly masks a square region within the input image and returns both the masked image and the original content of that region.\n\nThis method introduces controlled occlusion to the image, which can be useful in training models to perform tasks such as image completion or robust feature learning by requiring them to reconstruct or work with missing information.\n\nArgs:\n    img (torch.Tensor): Input image tensor of shape (C, H, W) to apply the random mask to.\n\nReturns:\n    tuple:\n        masked_img (torch.Tensor): Copy of the input image with a randomly chosen square region replaced by ones.\n        masked_part (torch.Tensor): The original pixel values of the region that was masked out.\n\"\"\"",
                    "source_code": "y1, x1 = np.random.randint(0, self.img_size - self.mask_size, 2)\n        y2, x2 = y1 + self.mask_size, x1 + self.mask_size\n        masked_part = img[:, y1:y2, x1:x2]\n        masked_img = img.clone()\n        masked_img[:, y1:y2, x1:x2] = 1\n\n        return masked_img, masked_part"
                },
                {
                    "docstring": "\"\"\"Mask center part of image\"\"\"",
                    "first_doc": "\"\"\"\nMasks the center part of an image by filling a square region with ones.\n\nArgs:\n    self: The object instance. Expects that the instance contains img_size (image dimension) and mask_size (size of the central mask) attributes.\n    img: The image to be masked.\n\nReturns:\n    tuple: A tuple containing the masked image and the upper-left pixel coordinate (i) of the center mask.\n\"\"\"",
                    "method_name": "apply_center_mask",
                    "second_doc": "\"\"\"\nCreates a masked version of the input image by overwriting a central square region with ones, facilitating tasks that require the network to infer or generate missing image content.\n\nArgs:\n    img (torch.Tensor): The input image tensor to be masked. Expects shape (channels, height, width).\n\nReturns:\n    tuple: A tuple containing:\n        - masked_img (torch.Tensor): The image tensor with the central square region masked out.\n        - i (int): The top-left coordinate (row and column) of the central mask region.\n\"\"\"",
                    "source_code": "# Get upper-left pixel coordinate\n        i = (self.img_size - self.mask_size) // 2\n        masked_img = img.clone()\n        masked_img[:, i : i + self.mask_size, i : i + self.mask_size] = 1\n\n        return masked_img, i"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves and processes an image sample, applying a masking transformation tailored to the current dataset mode to facilitate image inpainting tasks.\n\nArgs:\n    index (int): Index of the image to retrieve from the dataset.\n\nReturns:\n    tuple: A tuple containing:\n        - img (Tensor): The original, transformed image.\n        - masked_img (Tensor): The image with an applied mask (random or center based on mode).\n        - aux (Tensor or Any): Additional information from the masking operation, useful for model supervision.\n\nWhy:\n    This method prepares input-target pairs by masking parts of the images in different ways for training and evaluation, supporting models that learn to reconstruct missing or occluded regions.\n\"\"\"",
                    "source_code": "img = Image.open(self.files[index % len(self.files)])\n        img = self.transform(img)\n        if self.mode == \"train\":\n            # For training data perform random mask\n            masked_img, aux = self.apply_random_mask(img)\n        else:\n            # For test data mask the center of the image\n            masked_img, aux = self.apply_center_mask(img)\n\n        return img, masked_img, aux"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of image files available in the dataset.\n\nThis allows for seamless integration with data loaders and batching routines, enabling efficient management of training and evaluation workflows.\n\nReturns:\n    int: The number of image files contained in the dataset.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/context_encoder/models.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers of the Generator model by constructing a sequence of downsampling and upsampling operations to transform an input image through a deep feature hierarchy and reconstruct it in the output domain.\n\nThis setup is necessary to enable the Generator to learn complex feature representations required for mapping input images to output images during training.\n\nArgs:\n    channels (int): Number of input and output channels for the images handled by the Generator (e.g., 3 for RGB images).\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        def downsample(in_feat, out_feat, normalize=True):\n            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2))\n            return layers\n\n        def upsample(in_feat, out_feat, normalize=True):\n            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n            layers.append(nn.ReLU())\n            return layers\n\n        self.model = nn.Sequential(\n            *downsample(channels, 64, normalize=False),\n            *downsample(64, 64),\n            *downsample(64, 128),\n            *downsample(128, 256),\n            *downsample(256, 512),\n            nn.Conv2d(512, 4000, 1),\n            *upsample(4000, 512),\n            *upsample(512, 256),\n            *upsample(256, 128),\n            *upsample(128, 64),\n            nn.Conv2d(64, channels, 3, 1, 1),\n            nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPasses the input tensor through the generator's neural network model to produce an output, typically representing synthesized data.\n\nArgs:\n    x (torch.Tensor): Input tensor, usually a batch of images or noise vectors to be transformed by the generator.\n\nReturns:\n    torch.Tensor: Output tensor generated by the model, representing the generator's attempt at producing data similar to the target distribution.\n\nWhy:\n    This method enables the generator network to transform inputs into outputs, a crucial step for training and evaluating its ability to mimic the target data distribution.\n\"\"\"",
                    "source_code": "return self.model(x)"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the discriminator model by constructing a sequence of convolutional, normalization, and activation layers to progressively extract features and distinguish between real and generated images.\n\nArgs:\n    channels (int): Number of input image channels.\n\nReturns:\n    None\n\nThis design enables the network to effectively learn hierarchical visual patterns necessary for evaluating image authenticity.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, stride, normalize):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = channels\n        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the model to evaluate an input image and produce a prediction score indicating how likely the image is to be authentic according to the model's learned parameters.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be evaluated.\n\nReturns:\n    torch.Tensor: Model output representing the authenticity assessment of the input image.\n\"\"\"",
                    "source_code": "return self.model(img)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/cyclegan/cyclegan.py": [
        {
            "details": {
                "docstring": "\"\"\"Saves a generated sample from the test set\"\"\"",
                "first_doc": "\"\"\"\nGenerates and saves a sample grid of real and translated images from the validation set.\n\nThis method takes a batch of images from the validation dataloader and uses the generator networks to perform image-to-image translation. The real and generated images are arranged into a grid for visual comparison and saved to disk.\n\nArgs:\n    batches_done: The current number of batches processed. Used for naming the saved image file.\n\nReturns:\n    None. The generated image grid is saved to disk.\n\"\"\"",
                "method_name": "sample_images",
                "second_doc": "\"\"\"\nCreates and saves a grid of real and generated images from the validation set to visually track the model's image translation quality over time.\n\nBy comparing real images to their translated counterparts in a single grid, users can regularly monitor and assess the progression of the generator networks during training.\n\nArgs:\n    batches_done (int): Number of batches processed so far, used for naming the saved file.\n\nReturns:\n    None: The function saves the generated image grid to disk for inspection.\n\"\"\"",
                "source_code": "imgs = next(iter(val_dataloader))\n    G_AB.eval()\n    G_BA.eval()\n    real_A = Variable(imgs[\"A\"].type(Tensor))\n    fake_B = G_AB(real_A)\n    real_B = Variable(imgs[\"B\"].type(Tensor))\n    fake_A = G_BA(real_B)\n    # Arange images along x-axis\n    real_A = make_grid(real_A, nrow=5, normalize=True)\n    real_B = make_grid(real_B, nrow=5, normalize=True)\n    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n    # Arange images along y-axis\n    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n    save_image(image_grid, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), normalize=False)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/cyclegan/datasets.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "to_rgb",
                "second_doc": "\"\"\"\nConverts an input image to RGB mode to ensure compatibility with image processing functions and models that expect 3-channel color images.\n\nArgs:\n    image (PIL.Image.Image): The input image to be converted to RGB.\n\nReturns:\n    PIL.Image.Image: The image in RGB mode.\n\"\"\"",
                "source_code": "rgb_image = Image.new(\"RGB\", image.size)\n    rgb_image.paste(image)\n    return rgb_image"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ImageDataset instance by setting up image transformation pipelines and loading file paths for domain A and domain B image sets. This setup enables efficient pairing or unpairing of images from different domains during data loading to support tasks such as image translation.\n\nArgs:\n    root (str): The root directory containing the dataset.\n    transforms_ (list): A list of image transformation operations to be applied to each image.\n    unaligned (bool): Whether to pair images from domains A and B in an unaligned fashion.\n    mode (str): Specifies the subdirectory (e.g., 'train' or 'test') to load data from.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transform = transforms.Compose(transforms_)\n        self.unaligned = unaligned\n\n        self.files_A = sorted(glob.glob(os.path.join(root, \"%s/A\" % mode) + \"/*.*\"))\n        self.files_B = sorted(glob.glob(os.path.join(root, \"%s/B\" % mode) + \"/*.*\"))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves a pair of images from two datasets at the given index, applying necessary preprocessing and transformations to ensure both images meet required input specifications. \nThis method enables the construction of aligned or unaligned image pairs, which is essential for tasks that require comparing or learning mappings between two visual domains.\n\nArgs:\n    index (int): Index of the dataset item to retrieve.\n\nReturns:\n    dict: A dictionary containing the processed image tensors with keys \"A\" and \"B\", corresponding to transformed images from the respective datasets.\n\"\"\"",
                    "source_code": "image_A = Image.open(self.files_A[index % len(self.files_A)])\n\n        if self.unaligned:\n            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n        else:\n            image_B = Image.open(self.files_B[index % len(self.files_B)])\n\n        # Convert grayscale images to rgb\n        if image_A.mode != \"RGB\":\n            image_A = to_rgb(image_A)\n        if image_B.mode != \"RGB\":\n            image_B = to_rgb(image_B)\n\n        item_A = self.transform(image_A)\n        item_B = self.transform(image_B)\n        return {\"A\": item_A, \"B\": item_B}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the effective length of the dataset as the maximum length between two image domains. \n\nThis approach ensures that during training or inference, the dataset covers the entirety of both domains, allowing for balanced processing even when domains differ in size.\n\nReturns:\n    int: The maximum number of images available between the two domains.\n\"\"\"",
                    "source_code": "return max(len(self.files_A), len(self.files_B))"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/cyclegan/models.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional and batch normalization layers in a neural network module using a normal distribution.\n\nThis method ensures that convolutional layers have weights drawn from a normal distribution with mean 0 and standard deviation 0.02, and biases initialized to zero if present. Batch normalization layers receive weights drawn from a normal distribution with mean 1 and standard deviation 0.02, and biases set to zero. Such initialization stabilizes and accelerates network training by providing suitable starting values for key layers.\n\nArgs:\n    m (torch.nn.Module): A neural network module whose weights are to be initialized.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        if hasattr(m, \"bias\") and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0.0)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ResidualBlock by constructing a sequence of convolutional, normalization, and activation layers with reflection padding. This design enables the effective propagation of information across the neural network, helping to preserve input details while allowing the model to learn complex feature transformations. \n\nArgs:\n    in_features (int): Number of feature channels for the convolutional layers in the residual block.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the residual block, adding the input tensor to the output of the internal block to facilitate stable and efficient gradient flow during training.\n\nArgs:\n    x (Tensor): The input tensor to the residual block.\n\nReturns:\n    Tensor: The output tensor, which is the sum of the input and the processed input from the internal block.\n\"\"\"",
                    "source_code": "return x + self.block(x)"
                }
            ],
            "name": "ResidualBlock",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the GeneratorResNet network architecture by assembling a sequence of convolutional, normalization, activation, residual, and upsampling layers. This design enables effective transformation of input images through feature extraction, encoding, and decoding processes, allowing the network to capture complex image representations while preserving key structural information.\n\nArgs:\n    input_shape (tuple): Shape of the input images, provided as (channels, height, width).\n    num_residual_blocks (int): Number of residual blocks to include in the middle of the network for improved feature learning.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(GeneratorResNet, self).__init__()\n\n        channels = input_shape[0]\n\n        # Initial convolution block\n        out_features = 64\n        model = [\n            nn.ReflectionPad2d(channels),\n            nn.Conv2d(channels, out_features, 7),\n            nn.InstanceNorm2d(out_features),\n            nn.ReLU(inplace=True),\n        ]\n        in_features = out_features\n\n        # Downsampling\n        for _ in range(2):\n            out_features *= 2\n            model += [\n                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Residual blocks\n        for _ in range(num_residual_blocks):\n            model += [ResidualBlock(out_features)]\n\n        # Upsampling\n        for _ in range(2):\n            out_features //= 2\n            model += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Output layer\n        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n\n        self.model = nn.Sequential(*model)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nComputes the forward pass by applying the generator model to the input tensor, enabling transformation or synthesis of new data based on learned patterns.\n\nArgs:\n    x (torch.Tensor): Input tensor representing data to be processed by the generator network.\n\nReturns:\n    torch.Tensor: Output tensor produced by the generator model, representing the transformed or generated data.\n\"\"\"",
                    "source_code": "return self.model(x)"
                }
            ],
            "name": "GeneratorResNet",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator network layers designed to differentiate between real and generated images by progressively downsampling the input and extracting hierarchical features.\n\nArgs:\n    input_shape (tuple): A tuple specifying the number of channels, height, and width of the input images.\n\nReturns:\n    None\n\nWhy:  \nThis method sets up the architectural structure necessary for the Discriminator to analyze images at multiple scales, enabling it to effectively judge authenticity by learning complex spatial patterns through convolutional blocks.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n\n        # Calculate output shape of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1)\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPasses the input image through the discriminator network to obtain the prediction of its authenticity.\n\nArgs:\n    img (Tensor): Input image tensor to be evaluated by the discriminator.\n\nReturns:\n    Tensor: Output from the discriminator indicating the assessed likelihood that the input is real or generated.\n\"\"\"",
                    "source_code": "return self.model(img)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/cyclegan/utils.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the replay buffer with a specified maximum capacity to store experience data for future use.\n\nArgs:\n    max_size (int): The maximum number of items the buffer can hold. Must be greater than zero.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If max_size is less than or equal to zero.\n\nThis method ensures that the buffer is appropriately sized to store and retrieve information needed during model training processes where revisiting past data is essential for enhancing learning stability and diversity.\n\"\"\"",
                    "source_code": "assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n        self.max_size = max_size\n        self.data = []"
                },
                {
                    "docstring": null,
                    "method_name": "push_and_pop",
                    "second_doc": "\"\"\"\nProcesses a batch of data by selectively storing new elements or replacing old ones with probability. This encourages diversity in stored samples and prevents overfitting to recent data by maintaining a dynamic buffer.\n\nArgs:\n    data (torch.Tensor or object with attribute 'data'): Batch of data to be added to the buffer.\n\nReturns:\n    torch.autograd.Variable: A concatenated tensor of elements returned for further processing, which consists of a mix of newly added and previously stored elements.\n\"\"\"",
                    "source_code": "to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return Variable(torch.cat(to_return))"
                }
            ],
            "name": "ReplayBuffer",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the LambdaLR scheduler with the total number of epochs, the starting offset, and the epoch at which learning rate decay should begin. This setup ensures that the decay in learning rate is applied at the correct training stage, allowing for controlled training dynamics.\n\nArgs:\n    n_epochs (int): Total number of training epochs.\n    offset (int): Starting epoch offset.\n    decay_start_epoch (int): Epoch at which learning rate decay begins.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch"
                },
                {
                    "docstring": null,
                    "method_name": "step",
                    "second_doc": "\"\"\"\nCalculates a multiplicative factor for learning rate scheduling based on the current epoch, implementing a linear decay after a specified point in training to gradually reduce the learning rate. This helps stabilize training and improve convergence over time.\n\nArgs:\n    epoch (int): The current epoch number.\n\nReturns:\n    float: The scaling factor for the learning rate at the given epoch.\n\"\"\"",
                    "source_code": "return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
                }
            ],
            "name": "LambdaLR",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/dcgan/dcgan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional and batch normalization layers using a normal distribution for improved model convergence and stability during training.\n\nArgs:\n    m (nn.Module): A neural network layer whose weights and biases will be initialized.\n\nReturns:\n    None: This function modifies the weights and biases of the given layer in-place.\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator network with layers that transform a latent noise vector into an image-like output through a series of linear, upsampling, convolutional, and activation operations.\n\nThis design enables effective learning of complex mappings from latent space to visually coherent outputs, which is essential for producing high-quality generated samples.\n\nArgs:\n    opt: An object containing configuration options, including 'img_size' (int), 'latent_dim' (int), and 'channels' (int), which determine the output image size, the dimensionality of the input latent vector, and the number of output image channels, respectively.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates an image tensor from a latent noise vector by transforming the input through a series of learned layers and convolutional blocks. This process allows the model to map random noise to structured image outputs.\n\nArgs:\n    z (torch.Tensor): Input latent vector of shape (batch_size, latent_dim) representing noise samples.\n\nReturns:\n    torch.Tensor: Generated image tensor of shape (batch_size, num_channels, image_height, image_width).\n\"\"\"",
                    "source_code": "out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator network by configuring a series of convolutional, activation, dropout, and normalization layers designed to progressively reduce the spatial dimensions of an input image and extract informative features. The final fully-connected layer outputs a probability representing the likelihood that the input is real, supporting the network's ability to distinguish authentic samples from generated ones.\n\nArgs:\n    opt: An options object containing model hyperparameters such as the image size and the number of input channels.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the discriminator network, processing the input image to assess its authenticity. This involves extracting features via the model and passing them through an adversarial layer to produce a validity score, which helps the model differentiate between real and generated data.\n\nArgs:\n    img (torch.Tensor): Batch of images to be evaluated by the discriminator.\n\nReturns:\n    torch.Tensor: Validity scores indicating how likely each input image is to be real.\n\"\"\"",
                    "source_code": "out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/discogan/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the dataset by applying the specified data transformations and collecting all image file paths in the target directory. This enables consistent preprocessing and systematic loading of images for downstream tasks.\n\nArgs:\n    root (str): The root directory containing image data.\n    transforms_ (list): A list of transformation functions to preprocess the images.\n    mode (str): Specifies the data subfolder (e.g., 'train' or 'test') to load images from.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + '/*.*'))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves a pair of related images by splitting a source image in half and applying optional random horizontal flipping and transformations. This process is often used to create corresponding input-output pairs for training models that learn image mappings or translations.\n\nArgs:\n    index (int): Index of the source image file in the dataset.\n\nReturns:\n    dict: A dictionary with two transformed images, where 'A' contains the left half and 'B' contains the right half of the split image, both possibly flipped and processed.\n\"\"\"",
                    "source_code": "img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w/2, h))\n        img_B = img.crop((w/2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], 'RGB')\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], 'RGB')\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {'A': img_A, 'B': img_B}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of image files in the dataset.\n\nThis method allows seamless integration with data loading utilities by providing the dataset length, which is essential for batching and iteration.\n\nReturns:\n    int: The number of image files contained in the dataset.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/discogan/discogan.py": [
        {
            "details": {
                "docstring": "\"\"\"Saves a generated sample from the validation set\"\"\"",
                "first_doc": "\"\"\"\nGenerates and saves sample images demonstrating the progress of image translation during validation.\n\nArgs:\n    batches_done: The number of batches processed so far. This is used to name the output image files for tracking training progress.\n\nReturns:\n    None: This method does not return any value, but saves a concatenated image grid to disk.\n\"\"\"",
                "method_name": "sample_images",
                "second_doc": "\"\"\"\nPeriodically compiles and saves a grid of real and generated images to visually monitor the effectiveness of the model's image translation between two domains during training.\n\nArgs:\n    batches_done (int): Number of batches processed so far, used to name the saved image files for chronological tracking of progress.\n\nReturns:\n    None: The method saves a composed image grid to disk and produces no return value.\n\"\"\"",
                "source_code": "imgs = next(iter(val_dataloader))\n    G_AB.eval()\n    G_BA.eval()\n    real_A = Variable(imgs[\"A\"].type(Tensor))\n    fake_B = G_AB(real_A)\n    real_B = Variable(imgs[\"B\"].type(Tensor))\n    fake_A = G_BA(real_B)\n    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data, fake_A.data), 0)\n    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=8, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/discogan/models.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional and batch normalization layers to suitable values for stable neural network training.\n\nArgs:\n    m (nn.Module): The layer of the neural network to be initialized.\n\nReturns:\n    None\n\nThis method assigns initial values to the weights and biases of specific layer types to promote effective convergence during training.\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the downsampling block for the U-Net architecture, enabling progressive reduction of spatial dimensions while extracting features from input images.\n\nArgs:\n    in_size (int): Number of input channels.\n    out_size (int): Number of output channels.\n    normalize (bool): Whether to apply instance normalization after convolution.\n    dropout (float, optional): Dropout probability. If provided, applies dropout for regularization.\n\nReturns:\n    None\n\nWhy: Downsampling blocks such as this are essential for compressing and abstracting image information, allowing subsequent layers to focus on higher-level features and facilitating efficient and stable learning in deep neural networks for image transformation tasks.\n\"\"\"",
                    "source_code": "super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass of the input tensor through the sequence of layers defined in the UNetDown module, transforming the input representation to extract hierarchical features.\n\nArgs:\n    x (torch.Tensor): Input tensor to be processed by the module.\n\nReturns:\n    torch.Tensor: Output tensor resulting from the transformations applied by the model.\n\"\"\"",
                    "source_code": "return self.model(x)"
                }
            ],
            "name": "UNetDown",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an upsampling block consisting of a transposed convolution followed by normalization and activation, with optional dropout, to progressively increase spatial resolution and refine features in the network's decoding path.\n\nArgs:\n    in_size (int): Number of input channels to the transposed convolution layer.\n    out_size (int): Number of output channels from the transposed convolution layer.\n    dropout (float, optional): Dropout probability. If provided, applies dropout regularization after activation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(UNetUp, self).__init__()\n        layers = [nn.ConvTranspose2d(in_size, out_size, 4, 2, 1), nn.InstanceNorm2d(out_size), nn.ReLU(inplace=True)]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms an upsampling operation followed by concatenation with a skip connection, enabling the network to combine higher-resolution features from earlier layers with upsampled representations for improved feature propagation during decoding.\n\nArgs:\n    x (torch.Tensor): The input tensor to the upsampling block.\n    skip_input (torch.Tensor): The tensor from an earlier layer to be concatenated as a skip connection.\n\nReturns:\n    torch.Tensor: The concatenated tensor combining processed input and skip connection features.\n\"\"\"",
                    "source_code": "x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x"
                }
            ],
            "name": "UNetUp",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers of a U-Net based generator architecture, organizing sequential downsampling and upsampling components to enable the model to learn complex transformations between input and output images.\n\nArgs:\n    input_shape (tuple): The shape of the input tensor as (channels, height, width).\n\nReturns:\n    None\n\nThe method structures the network in a way that preserves spatial information through skip connections while allowing feature extraction and reconstruction at multiple scales, facilitating high-quality image generation and transformation tasks.\n\"\"\"",
                    "source_code": "super(GeneratorUNet, self).__init__()\n        channels, _, _ = input_shape\n        self.down1 = UNetDown(channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256, dropout=0.5)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5, normalize=False)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 256, dropout=0.5)\n        self.up4 = UNetUp(512, 128)\n        self.up5 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2), nn.ZeroPad2d((1, 0, 1, 0)), nn.Conv2d(128, channels, 4, padding=1), nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the U-Net based generator network, applying a series of downsampling and upsampling layers with skip connections to transform the input tensor. This layered structure enables effective encoding and decoding of input data, allowing the generation of complex outputs from input representations.\n\nArgs:\n    x (torch.Tensor): The input tensor representing an image or feature map.\n\nReturns:\n    torch.Tensor: The transformed output tensor produced by the network.\n\"\"\"",
                    "source_code": "d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        u1 = self.up1(d6, d5)\n        u2 = self.up2(u1, d4)\n        u3 = self.up3(u2, d3)\n        u4 = self.up4(u3, d2)\n        u5 = self.up5(u4, d1)\n\n        return self.final(u5)"
                }
            ],
            "name": "GeneratorUNet",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator model architecture by constructing a series of downsampling convolutional blocks and preparing the spatial output shape for patch-level image evaluation.\n\nArgs:\n    input_shape (tuple): Tuple containing the number of channels, height, and width of the input images.\n\nReturns:\n    None\n\nWhy:\n    This setup enables the model to effectively assess image authenticity at the patch level and facilitates stable adversarial training by progressively extracting hierarchical features from input images.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n        # Calculate output of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 3, width // 2 ** 3)\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(256, 1, 4, padding=1)\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nFeeds an input image through the network to yield a prediction, enabling the evaluation of whether the input resembles real or generated data.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be assessed by the discriminator.\n\nReturns:\n    torch.Tensor: The discriminator's output indicating the likelihood of the input being real or fake.\n\"\"\"",
                    "source_code": "return self.model(img)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/dragan/dragan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional and batch normalization layers to follow a normal distribution, promoting stable and efficient training by ensuring consistent starting conditions across model parameters.\n\nArgs:\n    m (torch.nn.Module): The module to have its weights initialized. Typically used as an argument in model.apply().\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator network by defining its architecture, including the layers that transform a latent vector into an output image of the desired shape. The structure combines fully connected and convolutional layers with upsampling, normalization, and activation functions to enable the generation of visually coherent images from noise.\n\nArgs:\n    opt: An options object containing configuration parameters such as 'img_size', 'latent_dim', and 'channels'.\n\nReturns:\n    None\n\nWhy:\n    This method sets up the necessary layers and transformations so the Generator can learn to map random latent vectors into realistic images, which is essential for training adversarial models that can produce new, synthetic data samples.\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the generator network, transforming input noise into a synthesized image tensor via learned upsampling and convolutional blocks.\n\nArgs:\n    noise (torch.Tensor): Input random noise tensor of shape (batch_size, latent_dim).\n\nReturns:\n    torch.Tensor: Generated image tensor with shape determined by the network architecture.\n    \nWhy:\n    This method enables the transformation of random noise into structured visual outputs, which is fundamental for generative modeling tasks such as image synthesis and data augmentation.\n\"\"\"",
                    "source_code": "out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator network by stacking convolutional, activation, dropout, and normalization layers to progressively extract features and reduce spatial dimensionality from input images. Also constructs the final classification layer to output a probability indicating whether the input is real or generated.\n\nArgs:\n    opt: An object containing configuration parameters such as the input image size and number of channels.\n\nReturns:\n    None. The method sets up the internal layers of the Discriminator for subsequent forward operations.\n\nWhy:\n    The method sets up the network to effectively distinguish between real and generated images by learning hierarchical feature representations and making a binary decision about the authenticity of each input.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the Discriminator to assess the 'realness' of input images by processing them through the model and an adversarial output layer.\n\nArgs:\n    img (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n\nReturns:\n    torch.Tensor: Discriminator scores indicating the likelihood that each input image is real or fake.\n    \nWhy:\n    The method evaluates images to provide feedback to the adversarial training process, guiding the generator to produce more realistic outputs.\n\"\"\"",
                    "source_code": "out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"Calculates the gradient penalty loss for DRAGAN\"\"\"",
                "first_doc": "\"\"\"\nCalculates the gradient penalty for a batch of interpolated samples as used in Wasserstein GAN with Gradient Penalty (WGAN-GP).\n\nArgs:\n    D: The discriminator (critic) function that maps input samples to a scalar output.\n    X: Batch of real data samples for computing the interpolated samples and penalties.\n\nReturns:\n    The computed gradient penalty value as a tensor, representing the average squared 2-norm of the gradients, penalized to enforce the Lipschitz constraint.\n\"\"\"",
                "method_name": "compute_gradient_penalty",
                "second_doc": "\"\"\"\nComputes a regularization term that penalizes deviations in gradient norm for a batch of interpolated data samples, promoting more stable adversarial training dynamics.\n\nArgs:\n    D: Callable discriminator or critic function that outputs a scalar value for each input sample.\n    X: Tensor containing a batch of real data samples used to generate interpolated samples.\n\nReturns:\n    torch.Tensor: The average penalty value as a tensor, quantifying how much the gradients' norms deviate from 1 on interpolated samples. This helps stabilize training by encouraging smooth changes in model output.\n\"\"\"",
                "source_code": "# Random weight term for interpolation\n    alpha = Tensor(np.random.random(size=X.shape))\n\n    interpolates = alpha * X + ((1 - alpha) * (X + 0.5 * X.std() * torch.rand(X.size())))\n    interpolates = Variable(interpolates, requires_grad=True)\n\n    d_interpolates = D(interpolates)\n\n    fake = Variable(Tensor(X.shape[0], 1).fill_(1.0), requires_grad=False)\n\n    # Get gradient w.r.t. interpolates\n    gradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n\n    gradient_penalty = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/dualgan/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ImageDataset by defining the image transformation pipeline and collecting paths to images from the specified directory and mode. This setup enables efficient and consistent preprocessing of image data for training and evaluation.\n\nArgs:\n    root (str): Root directory containing the dataset.\n    transforms_ (list): List of torchvision transforms to apply to each image.\n    mode (str): Subdirectory or mode (such as 'train' or 'test') specifying which images to load.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves a pair of related images from the dataset, applies optional horizontal flip augmentation, transforms them, and returns them as a dictionary.\n\nThis method is designed to facilitate training and evaluation processes that require paired images with corresponding augmentations, supporting tasks where learning the relationship or mapping between two images is crucial.\n\nArgs:\n    index (int): Index of the image pair to retrieve.\n\nReturns:\n    dict: A dictionary containing two transformed images with keys \"A\" and \"B\".\n\"\"\"",
                    "source_code": "img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of image files available in the dataset.\n\nThis method enables data loaders and other dataset utilities to determine the dataset size for indexing, batching, and epoch management.\n\nReturns:\n    int: The number of image files contained in the dataset instance.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/dualgan/dualgan.py": [
        {
            "details": {
                "docstring": "\"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"",
                "first_doc": "\"\"\"\nComputes the gradient penalty for Wasserstein GANs with gradient penalty (WGAN-GP).\n\nInterpolates between real and fake samples, computes the discriminator output, and then calculates the gradient norm penalty as used in WGAN-GP to enforce the Lipschitz constraint.\n\nArgs:\n    D: The discriminator function or model used to evaluate the interpolated samples.\n    real_samples: Batch of real data samples used for interpolation.\n    fake_samples: Batch of generated (fake) samples used for interpolation.\n\nReturns:\n    The gradient penalty scalar tensor, representing the mean squared deviation of gradient norms from 1.\n\"\"\"",
                "method_name": "compute_gradient_penalty",
                "second_doc": "\"\"\"\nCalculates a penalty term by measuring how much the gradients of the discriminator with respect to random convex combinations of real and generated samples deviate from the desired norm. This regularization helps stabilize training and encourages the discriminator to behave smoothly, thereby addressing common pitfalls in adversarial learning.\n\nArgs:\n    D: Discriminator function or model applied to the interpolated samples for gradient computation.\n    real_samples: Batch of authentic data samples used as one endpoint for interpolation.\n    fake_samples: Batch of generated data samples used as the other endpoint for interpolation.\n\nReturns:\n    torch.Tensor: Scalar tensor representing the mean squared difference between the gradient norms and 1, serving as a regularization metric.\n\"\"\"",
                "source_code": "# Random weight term for interpolation between real and fake samples\n    alpha = FloatTensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n    # Get random interpolation between real and fake samples\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    validity = D(interpolates)\n    fake = Variable(FloatTensor(np.ones(validity.shape)), requires_grad=False)\n    # Get gradient w.r.t. interpolates\n    gradients = autograd.grad(\n        outputs=validity,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Saves a generated sample from the test set\"\"\"",
                "first_doc": "\"\"\"\nSaves a generated sample of images during the validation phase of training.\n\nThis method retrieves a batch of images from the validation dataloader, generates fake images using the respective generators, concatenates the results for visual comparison, and saves the resulting sample image grid to disk.\n\nArgs:\n    batches_done: The number of batches processed so far, used to name the saved image and indicate training progress.\n\nReturns:\n    None: This method does not return a value.\n\"\"\"",
                "method_name": "sample_images",
                "second_doc": "\"\"\"\nGenerates and saves visual comparisons of real and generated images from the validation set during training.\n\nBy assembling grids of real and corresponding fake images from both translation directions, this method allows for ongoing visual evaluation of the model\u2019s progress, helping to identify strengths and weaknesses in image translation performance at intermediate checkpoints.\n\nArgs:\n    batches_done (int): The current number of batches processed, used for naming the saved image file to track progression during training.\n\nReturns:\n    None: The function performs file I/O only and does not return a value.\n\"\"\"",
                "source_code": "imgs = next(iter(val_dataloader))\n    real_A = Variable(imgs[\"A\"].type(FloatTensor))\n    fake_B = G_AB(real_A)\n    AB = torch.cat((real_A.data, fake_B.data), -2)\n    real_B = Variable(imgs[\"B\"].type(FloatTensor))\n    fake_A = G_BA(real_B)\n    BA = torch.cat((real_B.data, fake_A.data), -2)\n    img_sample = torch.cat((AB, BA), 0)\n    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=8, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/dualgan/models.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of neural network layers for improved and stable training. Convolutional layers are initialized with a normal distribution centered at 0, while batch normalization layers are initialized to have means of 1 (weights) and 0 (biases). \nThis helps the model start from suitable parameters, encouraging more consistent and reliable training outcomes.\n\nArgs:\n    m (torch.nn.Module): A layer in the neural network whose weights and biases are to be initialized.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a downsampling block consisting of convolutional, normalization, activation, and optional dropout layers. This structure enables the progressive reduction of spatial dimensions while extracting feature representations necessary for tasks such as image translation and synthesis.\n\nArgs:\n    in_size (int): Number of input channels.\n    out_size (int): Number of output channels.\n    normalize (bool): If True, applies instance normalization after convolution.\n    dropout (float or None): If set, adds a dropout layer with the specified probability after activation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size, affine=True))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPasses the input tensor through the downsampling block of the network to extract hierarchical feature representations for subsequent processing.\n\nArgs:\n    x (torch.Tensor): Input tensor representing an image or feature map.\n\nReturns:\n    torch.Tensor: Output tensor after applying the downsampling operations defined in the model.\n\"\"\"",
                    "source_code": "return self.model(x)"
                }
            ],
            "name": "UNetDown",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes an upsampling module that expands feature maps and refines representations through a sequence of transpose convolution, normalization, and activation, with optional dropout for regularization.\n\nArgs:\n    in_size (int): Number of input channels.\n    out_size (int): Number of output channels.\n    dropout (float, optional): Dropout probability. If specified, applies dropout for regularization.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, bias=False),\n            nn.InstanceNorm2d(out_size, affine=True),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms an upsampling operation followed by concatenation with a corresponding feature map from the encoder. This allows the network to leverage both spatial details from early layers and abstract features from deeper layers, which aids in reconstructing high-fidelity outputs.\n\nArgs:\n    x (torch.Tensor): Input tensor from the previous layer.\n    skip_input (torch.Tensor): Corresponding feature map from the encoder for skip connection.\n\nReturns:\n    torch.Tensor: Output tensor after upsampling and concatenation, preserving important spatial information for further decoding.\n\"\"\"",
                    "source_code": "x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x"
                }
            ],
            "name": "UNetUp",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator network by constructing a series of downsampling and upsampling layers following the U-Net architecture, enabling the transformation and reconstruction of input data with high fidelity.\n\nArgs:\n    channels (int): Number of input and output channels for the generator.\n\nWhy:\n    Building the generator in this way allows the model to efficiently learn complex mappings between inputs and outputs, supporting tasks such as detailed image generation and transformation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.down1 = UNetDown(channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5, normalize=False)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 256)\n        self.up5 = UNetUp(512, 128)\n        self.up6 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(nn.ConvTranspose2d(128, channels, 4, stride=2, padding=1), nn.Tanh())"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the generator network, encoding the input image via multiple downsampling layers before reconstructing it using a series of upsampling layers. This process enables the model to transform and generate new images with desired characteristics by capturing relevant features and recombining them.\n\nArgs:\n    x (torch.Tensor): Input image tensor.\n\nReturns:\n    torch.Tensor: Output tensor representing the generated or transformed image.\n\"\"\"",
                    "source_code": "d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        u1 = self.up1(d7, d6)\n        u2 = self.up2(u1, d5)\n        u3 = self.up3(u2, d4)\n        u4 = self.up4(u3, d3)\n        u5 = self.up5(u4, d2)\n        u6 = self.up6(u5, d1)\n\n        return self.final(u6)"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator network by constructing a sequence of convolutional layers designed to progressively extract hierarchical features and reduce spatial dimensions from input images. This architecture enables the model to distinguish between real and generated data by capturing increasingly complex representations at each stage.\n\nArgs:\n    in_channels (int): Number of channels in the input images, determining the input dimensionality for the first convolutional layer.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discrimintor_block(in_features, out_features, normalize=True):\n            \"\"\"Discriminator block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.BatchNorm2d(out_features, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discrimintor_block(in_channels, 64, normalize=False),\n            *discrimintor_block(64, 128),\n            *discrimintor_block(128, 256),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(256, 1, kernel_size=4)\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses an input image through the discriminator network to evaluate its authenticity.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be assessed.\n\nReturns:\n    torch.Tensor: Discriminator's output, indicating the likelihood that the input image is real or generated.\n    \nThe method enables the network to distinguish between real and generated samples, supporting adversarial training dynamics.\n\"\"\"",
                    "source_code": "return self.model(img)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/ebgan/ebgan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of neural network layers for improved training stability and convergence. Convolutional layers are initialized with a normal distribution centered at 0 with a small standard deviation, while batch normalization layers receive a normalization around 1 with biases set to 0. This helps encourage effective gradient flow and consistent model behavior during early training.\n\nArgs:\n    m (nn.Module): The module whose weights and biases will be initialized.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator network by defining its architecture, which progressively transforms a latent vector into an image through fully connected and convolutional layers. This setup enables the model to learn how to map random noise to structured images as part of the generative modeling process.\n\nArgs:\n    opt: An object containing model options, including 'img_size', 'latent_dim', and 'channels', which specify the final image size, the dimension of the latent input, and the number of output image channels, respectively.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the generator network to transform input noise into a synthesized image. This process enables the creation of new image data by mapping random noise vectors to the image space, facilitating the training and evaluation of generative models.\n\nArgs:\n    noise (torch.Tensor): Input tensor of random noise, typically of shape (batch_size, latent_dim), which serves as the seed for image generation.\n\nReturns:\n    torch.Tensor: Output tensor representing the generated images, with shape (batch_size, channels, height, width).\n\"\"\"",
                    "source_code": "out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator module by setting up convolutional layers for feature extraction, fully connected layers for feature embedding and transformation, and upsampling layers to process input images. These components allow the network to effectively learn and distinguish complex data patterns by projecting images into lower-dimensional representations and reconstructing them, supporting its role in adversarial learning.\n\nArgs:\n    opt: An object containing configuration parameters including the number of image channels and the input image size.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        # Upsampling\n        self.down = nn.Sequential(nn.Conv2d(opt.channels, 64, 3, 2, 1), nn.ReLU())\n        # Fully-connected layers\n        self.down_size = opt.img_size // 2\n        down_dim = 64 * (opt.img_size // 2) ** 2\n\n        self.embedding = nn.Linear(down_dim, 32)\n\n        self.fc = nn.Sequential(\n            nn.BatchNorm1d(32, 0.8),\n            nn.ReLU(inplace=True),\n            nn.Linear(32, down_dim),\n            nn.BatchNorm1d(down_dim),\n            nn.ReLU(inplace=True),\n        )\n        # Upsampling\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(64, opt.channels, 3, 1, 1))"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses the input image through a sequence of downsampling, embedding, fully connected, and upsampling layers to extract feature representations and produce the discriminator's output. This structure enables the model to learn meaningful features for distinguishing real from generated images.\n\nArgs:\n    img (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n\nReturns:\n    Tuple[torch.Tensor, torch.Tensor]: A tuple containing the final output tensor and the intermediate embedding tensor representing learned features.\n\"\"\"",
                    "source_code": "out = self.down(img)\n        embedding = self.embedding(out.view(out.size(0), -1))\n        out = self.fc(embedding)\n        out = self.up(out.view(out.size(0), 64, self.down_size, self.down_size))\n        return out, embedding"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "pullaway_loss",
                "second_doc": "\"\"\"\nComputes the pull-away loss to encourage the embeddings in the batch to be distinct from each other by penalizing high similarity between normalized embedding vectors.\n\nArgs:\n    embeddings (torch.Tensor): A tensor of shape (batch_size, embedding_dim) containing the embeddings to be regularized.\n\nReturns:\n    torch.Tensor: A scalar tensor representing the pull-away loss value for the batch.\n\"\"\"",
                "source_code": "norm = torch.sqrt(torch.sum(embeddings ** 2, -1, keepdim=True))\n    normalized_emb = embeddings / norm\n    similarity = torch.matmul(normalized_emb, normalized_emb.transpose(1, 0))\n    batch_size = embeddings.size(0)\n    loss_pt = (torch.sum(similarity) - batch_size) / (batch_size * (batch_size - 1))\n    return loss_pt"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/esrgan/datasets.py": [
        {
            "details": {
                "docstring": "\"\"\" Denormalizes image tensors using mean and std \"\"\"",
                "first_doc": "\"\"\"\nReverses the normalization of an image tensor and clamps the values between 0 and 255.\n\nArgs:\n    tensors: The image tensor to be denormalized. Each channel of the tensor will be multiplied by the standard deviation and added to the mean.\n\nReturns:\n    The denormalized tensor with values clamped between 0 and 255.\n\"\"\"",
                "method_name": "denormalize",
                "second_doc": "\"\"\"\nReverses the normalization process on an image tensor by applying channel-wise scaling and shifting, then clamps the pixel values to the valid display range.\n\nArgs:\n    tensors (torch.Tensor): Image tensor to be denormalized, typically with shape (N, C, H, W), where each channel will be individually processed.\n    mean (list or torch.Tensor): Sequence of means used for prior normalization, applied per channel.\n    std (list or torch.Tensor): Sequence of standard deviations used for prior normalization, applied per channel.\n\nReturns:\n    torch.Tensor: The denormalized image tensor with values clipped between 0 and 255.\n\nWhy:\n    This ensures that images transformed during preprocessing can be accurately converted back for visualization or further processing, preserving their displayable pixel value range.\n\"\"\"",
                "source_code": "for c in range(3):\n        tensors[:, c].mul_(std[c]).add_(mean[c])\n    return torch.clamp(tensors, 0, 255)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes image transformation pipelines for low-resolution and high-resolution images, and collects file paths from the specified dataset directory.\n\nArgs:\n    root (str): Path to the directory containing image files.\n    hr_shape (tuple of int): Desired height and width for high-resolution images.\n    mean (tuple of float): Mean values for image normalization.\n    std (tuple of float): Standard deviation values for image normalization.\n\nReturns:\n    None\n\nWhy:\n    This setup enables consistent preprocessing and transformation of both low- and high-resolution images, which is essential for training models that learn mappings between different image qualities.\n\"\"\"",
                    "source_code": "hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n\n        self.files = sorted(glob.glob(root + \"/*.*\"))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves a paired sample consisting of a low-resolution and a high-resolution version of an image at the given index. This supports model training workflows requiring both input and target images with different levels of detail.\n\nArgs:\n    index (int): Index of the image to retrieve from the dataset.\n\nReturns:\n    dict: A dictionary containing 'lr' (the transformed low-resolution image) and 'hr' (the transformed high-resolution image).\n\"\"\"",
                    "source_code": "img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of image files available in the dataset.\n\nThis allows iteration and batching over the dataset to be handled correctly by data loaders or other utilities that require the dataset's length.\n\nReturns:\n    int: The number of image files contained within the dataset.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/esrgan/esrgan.py": [],
    "PyTorch-GAN/implementations/esrgan/models.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the FeatureExtractor by loading a pretrained VGG19 network and truncating its feature layers up to the 35th layer. This setup allows the extraction of high-level feature representations from input images, which are useful for evaluating perceptual similarity or guiding neural network training with feature-based losses.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.vgg19_54 = nn.Sequential(*list(vgg19_model.features.children())[:35])"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nExtracts high-level feature representations from the input image using a specific layer of a pre-trained VGG19 network.\n\nThis method enables the computation of perceptual similarity or loss by leveraging features from a deep convolutional neural network.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be processed.\n\nReturns:\n    torch.Tensor: Feature map output produced by the VGG19 5_4 layer for the input image.\n\"\"\"",
                    "source_code": "return self.vgg19_54(img)"
                }
            ],
            "name": "FeatureExtractor",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the dense residual block by constructing a series of convolutional and activation layer sequences with progressively increasing input feature dimensions.\n\nArgs:\n    filters (int): Number of feature channels for each convolutional layer.\n    res_scale (float): Scaling factor for the residual output.\n\nWhy:\n    By stacking and connecting multiple convolutional blocks with increasing input channels, the module is designed to promote rich feature propagation and reuse, ultimately supporting more effective transformation of input representations during training.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(DenseResidualBlock, self).__init__()\n        self.res_scale = res_scale\n\n        def block(in_features, non_linearity=True):\n            layers = [nn.Conv2d(in_features, filters, 3, 1, 1, bias=True)]\n            if non_linearity:\n                layers += [nn.LeakyReLU()]\n            return nn.Sequential(*layers)\n\n        self.b1 = block(in_features=1 * filters)\n        self.b2 = block(in_features=2 * filters)\n        self.b3 = block(in_features=3 * filters)\n        self.b4 = block(in_features=4 * filters)\n        self.b5 = block(in_features=5 * filters, non_linearity=False)\n        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies a sequence of residual blocks to the input tensor, progressively concatenating outputs to enrich feature representation, and combines the processed result with the original input using a scaled residual connection.\n\nArgs:\n    x (torch.Tensor): Input tensor representing image features or activations.\n\nReturns:\n    torch.Tensor: Output tensor after processing through dense residual connections and applying scaled residual addition. This structure helps to improve network training stability and feature reuse.\n\"\"\"",
                    "source_code": "inputs = x\n        for block in self.blocks:\n            out = block(inputs)\n            inputs = torch.cat([inputs, out], 1)\n        return out.mul(self.res_scale) + x"
                }
            ],
            "name": "DenseResidualBlock",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a residual-in-residual dense block composed of multiple dense residual sub-blocks and a residual scaling factor.\n\nArgs:\n    filters (int): The number of filters or channels for each dense residual block.\n    res_scale (float): Scaling factor applied to the residual output for stability.\n\nReturns:\n    None\n\nWhy:\n    This structure enables the creation of deep neural modules with enhanced feature learning and gradient flow, making it easier to model complex relationships in high-resolution or generative tasks.\n\"\"\"",
                    "source_code": "super(ResidualInResidualDenseBlock, self).__init__()\n        self.res_scale = res_scale\n        self.dense_blocks = nn.Sequential(\n            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies a series of dense residual blocks to the input tensor and combines the result with the original input, scaled by a predefined factor. This structure encourages stable learning and effective flow of information, which helps overcome degradation problems in deep networks.\n\nArgs:\n    x (torch.Tensor): Input tensor to be processed by the residual-in-residual dense blocks.\n\nReturns:\n    torch.Tensor: Output tensor obtained by adding the scaled dense block transformation back to the input tensor.\n\"\"\"",
                    "source_code": "return self.dense_blocks(x).mul(self.res_scale) + x"
                }
            ],
            "name": "ResidualInResidualDenseBlock",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the GeneratorRRDB network by assembling a sequence of convolutional, residual, and upsampling layers essential for transforming input images into high-quality, enhanced outputs. The organization of these layers is designed to effectively capture multi-scale features, facilitate stable training, and enable significant upscaling of image resolution.\n\nArgs:\n    channels (int): Number of channels in the input and output images.\n    filters (int): Number of feature maps at each convolutional layer.\n    num_res_blocks (int): Number of residual-in-residual dense blocks to include in the network.\n    num_upsample (int): Number of upsampling stages to apply in the network.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(GeneratorRRDB, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Conv2d(channels, filters, kernel_size=3, stride=1, padding=1)\n        # Residual blocks\n        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1)\n        # Upsampling layers\n        upsample_layers = []\n        for _ in range(num_upsample):\n            upsample_layers += [\n                nn.Conv2d(filters, filters * 4, kernel_size=3, stride=1, padding=1),\n                nn.LeakyReLU(),\n                nn.PixelShuffle(upscale_factor=2),\n            ]\n        self.upsampling = nn.Sequential(*upsample_layers)\n        # Final output block\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(filters, filters, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(filters, channels, kernel_size=3, stride=1, padding=1),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a series of convolutional, residual, and upsampling operations to transform the input tensor, enabling the generation of high-resolution outputs from lower-resolution inputs.\n\nArgs:\n    x (torch.Tensor): Input tensor representing a low-resolution image.\n\nReturns:\n    torch.Tensor: Output tensor representing the enhanced or super-resolved image.\n\"\"\"",
                    "source_code": "out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out"
                }
            ],
            "name": "GeneratorRRDB",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator neural network by constructing a sequence of convolutional and normalization layers that process input images and produce lower-resolution output patches indicating the likelihood of authenticity for different image regions.\n\nArgs:\n    input_shape (tuple): Dimensions of the input image tensor as (channels, height, width).\n\nReturns:\n    None\n\nWhy:\n    Structuring the network in this way enables the model to progressively extract hierarchical features from images, allowing it to distinguish between real and generated image patches. This approach supports effective adversarial learning by providing localized assessments of image authenticity.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPasses an input image through the discriminator's model to obtain a prediction about its authenticity.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be evaluated by the discriminator.\n\nReturns:\n    torch.Tensor: Output tensor representing the discriminator's assessment of the input image.\n\"\"\"",
                    "source_code": "return self.model(img)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/esrgan/test_on_image.py": [],
    "PyTorch-GAN/implementations/gan/gan.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator model architecture by defining a sequence of fully connected and activation layers that progressively transform a latent vector into an output matching the desired image shape. This modular structure enables the generator to efficiently map random noise into data-like outputs, supporting the training dynamics of adversarial models.\n\nArgs:\n    opt: Configuration or options object with attributes, including `latent_dim`, specifying the size of the input noise vector.\n    img_shape: Tuple or list representing the shape of the output image.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates and reshapes an output image tensor from a given input noise vector by passing it through the generator network.\n\nArgs:\n    z (Tensor): Input noise tensor of shape (batch_size, latent_dim).\n\nReturns:\n    Tensor: Generated image tensor of shape (batch_size, *img_shape).\n\nWhy:\n    This method transforms input noise vectors into structured image data, enabling the generation of new images that mimic the distribution of real data.\n\"\"\"",
                    "source_code": "img = self.model(z)\n        img = img.view(img.size(0), *img_shape)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers of the Discriminator network, constructing a sequence of fully connected layers with non-linear activations and a final sigmoid output to process and evaluate input images.\n\nArgs:\n    img_shape (tuple): The shape of the input images, used to determine the input size for the first linear layer.\n\nReturns:\n    None\n\nWhy:\n    This method establishes the network architecture needed to enable the model to effectively distinguish between real and generated data by learning representative features from input images.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nEvaluates the authenticity of input images by transforming them into a flat vector and processing them through a neural network to produce a validity score. This helps guide adversarial training by providing feedback on how real or fake an input appears.\n\nArgs:\n    img (torch.Tensor): Batch of input images, typically with shape (batch_size, channels, height, width).\n\nReturns:\n    torch.Tensor: Validity scores for each input image, indicating the discriminator's assessment of authenticity.\n\"\"\"",
                    "source_code": "img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/infogan/infogan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of neural network layers to suitable starting values to promote stable and effective model training.\n\nArgs:\n    m (nn.Module): A neural network module whose weights are to be initialized.\n\nReturns:\n    None: This function modifies the weights of the provided module in place.\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Returns one-hot encoded Variable\"\"\"",
                "first_doc": "\"\"\"\nConverts a vector of class indices into a one-hot encoded tensor.\n\nArgs:\n    y: An array-like object containing class indices for each sample.\n    num_columns: The total number of classes; defines the width of the one-hot encoded output.\n\nReturns:\n    Variable: A tensor containing the one-hot encoded representation of the input class indices.\n\"\"\"",
                "method_name": "to_categorical",
                "second_doc": "\"\"\"\nTransforms an array of class indices into a one-hot encoded tensor to enable seamless class conditioning or label representation within learning models.\n\nArgs:\n    y (array-like): Input containing class indices corresponding to each sample.\n    num_columns (int): The number of distinct classes, determining the width of the one-hot encoded output.\n\nReturns:\n    Variable: A tensor of shape (number of samples, number of classes) representing the one-hot encoded version of the input indices.\n\"\"\"",
                "source_code": "y_cat = np.zeros((y.shape[0], num_columns))\n    y_cat[range(y.shape[0]), y] = 1.0\n\n    return Variable(FloatTensor(y_cat))"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the architecture for the generator network, defining layers to transform a combined latent vector, class information, and code variables into a realistic image output through upsampling and convolutional operations.\n\nArgs:\n    opt: An object containing generator configuration options, including latent dimension size, number of classes, code dimension, image size, and number of output channels.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n        input_dim = opt.latent_dim + opt.n_classes + opt.code_dim\n\n        self.init_size = opt.img_size // 4  # Initial size before upsampling\n        self.l1 = nn.Sequential(nn.Linear(input_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the generator network, producing an image tensor from concatenated noise, label, and code vectors. By transforming these latent representations, the method enables the synthesis of new image samples conditioned on multiple input factors.\n\nArgs:\n    noise (Tensor): Random noise tensor to seed the image generation process.\n    labels (Tensor): Tensor representing categorical information or class labels.\n    code (Tensor): Additional latent code tensor for providing structured variation or control.\n\nReturns:\n    Tensor: Generated image tensor resulting from the transformation of the input representations.\n\"\"\"",
                    "source_code": "gen_input = torch.cat((noise, labels, code), -1)\n        out = self.l1(gen_input)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers of the Discriminator network, defining convolutional blocks for hierarchical feature extraction and dense layers for producing multiple outputs: adversarial, auxiliary, and latent representations.\n\nArgs:\n    opt: An object containing configuration options, including img_size, channels, n_classes, and code_dim.\n\nReturns:\n    None\n\nWhy:\n    These initializations allow the Discriminator to process input images into compact feature representations and simultaneously assess image authenticity, predict class labels, and generate latent codes, supporting diverse training objectives and improved model capacity.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.conv_blocks = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n\n        # Output layers\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))\n        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n        self.latent_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.code_dim))"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses an input image through a series of convolutional and fully connected layers to estimate its authenticity, predict its class label, and infer a latent representation. This allows the model to simultaneously evaluate if the image is real or fake, perform auxiliary classification, and capture underlying features relevant for generative modeling.\n\nArgs:\n    img (torch.Tensor): Batch of input images with shape (batch_size, channels, height, width).\n\nReturns:\n    tuple:\n        - validity (torch.Tensor): Discriminator's prediction for whether each image is real or fake.\n        - label (torch.Tensor): Predicted class labels for the images.\n        - latent_code (torch.Tensor): Inferred latent representations of the images.\n\"\"\"",
                    "source_code": "out = self.conv_blocks(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        label = self.aux_layer(out)\n        latent_code = self.latent_layer(out)\n\n        return validity, label, latent_code"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"",
                "first_doc": "\"\"\"\nGenerates and saves sample images to visualize generator outputs, including static samples and samples with varied latent codes.\n\nArgs:\n    n_row: Number of rows (and columns) of images in the grid; determines the batch size for sampling.\n    batches_done: The current training batch index, used to name output image files.\n\nReturns:\n    None: The function saves generated images to disk and does not return any value.\n\"\"\"",
                "method_name": "sample_image",
                "second_doc": "\"\"\"\nGenerates and saves sample images by producing outputs from the generator with both fixed and systematically varied latent codes. This process enables visualization and assessment of the generator's output diversity and its sensitivity to changes in the latent representation.\n\nArgs:\n    n_row (int): Number of rows (and columns) for the image grid, determining the number of images to generate per batch.\n    batches_done (int): Training step index used for naming the output image files.\n\nReturns:\n    None: The method saves generated sample images to disk for later inspection and does not return any value.\n\"\"\"",
                "source_code": "# Static sample\n    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n    static_sample = generator(z, static_label, static_code)\n    save_image(static_sample.data, \"images/static/%d.png\" % batches_done, nrow=n_row, normalize=True)\n\n    # Get varied c1 and c2\n    zeros = np.zeros((n_row ** 2, 1))\n    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n    sample1 = generator(static_z, static_label, c1)\n    sample2 = generator(static_z, static_label, c2)\n    save_image(sample1.data, \"images/varying_c1/%d.png\" % batches_done, nrow=n_row, normalize=True)\n    save_image(sample2.data, \"images/varying_c2/%d.png\" % batches_done, nrow=n_row, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/lsgan/lsgan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional and batch normalization layers in a neural network module to promote stable and efficient learning dynamics during model training.\n\nArgs:\n    m (torch.nn.Module): The neural network module whose weights are to be initialized.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator network by setting up fully connected and convolutional layers to progressively transform a latent vector into a structured image tensor. This design allows the network to learn a mapping from noise distributions to realistic image outputs, supporting training and experimentation in generative modeling.\n\nArgs:\n    opt: An options object containing hyperparameters such as latent_dim (dimension of input noise), img_size (output image size), and channels (number of output image channels).\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates an image from a given input noise vector by transforming it through a sequence of neural network layers.\n\nArgs:\n    z (torch.Tensor): Input latent noise tensor of shape (batch_size, latent_dim).\n\nReturns:\n    torch.Tensor: Generated image tensor.\n    \nWhy:\n    This method transforms random noise into structured image data, enabling the creation of synthetic images with learned properties.\n\"\"\"",
                    "source_code": "out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers and architecture of the Discriminator network to process and evaluate input images. Constructs a sequence of convolutional blocks with increasing feature channels and downsampling, followed by a fully connected output layer. This setup enables the network to extract hierarchical features from images and assess their authenticity for further adversarial processing.\n\nArgs:\n    opt (namespace): Configuration options containing image channel count and image size used to define input and intermediate layer dimensions.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Linear(128 * ds_size ** 2, 1)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the discriminator network to evaluate the realism of input images. The input image is processed by the underlying model and transformed into an appropriate representation, which is then scored for authenticity using an adversarial layer. This is crucial for distinguishing between real and generated samples during adversarial training.\n\nArgs:\n    img (Tensor): Input batch of images to be assessed by the discriminator.\n\nReturns:\n    Tensor: Validity scores indicating the likelihood of each input image being real.\n\"\"\"",
                    "source_code": "out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/munit/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ImageDataset instance by applying a sequence of image transformations and collecting image file paths from the specified dataset directory. During training mode, images from both the training and test directories are included to expand the dataset and encourage more robust model learning.\n\nArgs:\n    root (str): The root directory of the dataset.\n    transforms_ (list): A list of image transformation operations to be composed and applied.\n    mode (str): The dataset mode, typically either 'train' or another relevant phase.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n        if mode == \"train\":\n            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves a paired sample of two related image segments from the dataset, applying random horizontal flipping and necessary transformations.\n\nThe method enables varied and augmented image samples to be generated on-the-fly, which is essential for improving the robustness and generalizability of machine learning models trained on paired image data.\n\nArgs:\n    index (int): Index of the sample to retrieve.\n\nReturns:\n    dict: A dictionary containing two transformed image tensors, {\"A\": img_A, \"B\": img_B}, representing the paired segments.\n\"\"\"",
                    "source_code": "img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the number of images available in the dataset.\n\nThis allows external components to determine the dataset size, which is particularly useful for batching, iteration, and validation when working with image data in deep learning workflows.\n\nReturns:\n    int: The total number of image files in the dataset.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/munit/models.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of model layers to promote stable and effective training. \n\nFor convolutional layers, weights are initialized from a normal distribution with mean 0 and standard deviation 0.02. For batch normalization layers, weights are initialized with a mean of 1 and standard deviation 0.02, and biases are set to zero. These choices encourage convergence and reduce training instabilities.\n\nArgs:\n    m (torch.nn.Module): A layer from the model whose type will determine the initialization scheme.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the LambdaLR scheduler by setting the total training epochs, the starting offset for epochs, and the epoch at which learning rate decay begins. This setup ensures a proper schedule for adapting the learning rate during model training, which is critical for effective convergence and stability in deep learning optimization.\n\nArgs:\n    n_epochs (int): Total number of training epochs.\n    offset (int): Initial epoch count offset for flexible scheduling.\n    decay_start_epoch (int): Epoch number at which learning rate decay is triggered.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch"
                },
                {
                    "docstring": null,
                    "method_name": "step",
                    "second_doc": "\"\"\"\nCalculates a scaling factor for the learning rate at a given training epoch, implementing a linear decay schedule after a specified epoch.\n\nArgs:\n    epoch (int): The current epoch number during training.\n\nReturns:\n    float: The multiplier to be applied to the learning rate for the given epoch.\n    \nThis approach helps manage the progression of learning by gradually reducing the learning rate, which can improve stability and convergence of the training process over time.\n\"\"\"",
                    "source_code": "return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
                }
            ],
            "name": "LambdaLR",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Encoder by constructing separate modules for encoding content and style information from input data, enabling effective disentanglement of these aspects for downstream transformations.\n\nArgs:\n    in_channels (int): Number of input channels for the image data.\n    dim (int): Base number of feature map channels for the encoders.\n    n_residual (int): Number of residual blocks in the content encoder.\n    n_downsample (int): Number of downsampling layers in both encoders.\n    style_dim (int): Dimensionality of the style representation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Encoder, self).__init__()\n        self.content_encoder = ContentEncoder(in_channels, dim, n_residual, n_downsample)\n        self.style_encoder = StyleEncoder(in_channels, dim, n_downsample, style_dim)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nEncodes the input tensor into separate content and style representations by passing it through dedicated encoder modules. This enables subsequent processing steps to manipulate or combine content and style information independently, facilitating flexible image transformation or generation tasks.\n\nArgs:\n    x (Tensor): Input data tensor, typically representing an image.\n\nReturns:\n    Tuple[Tensor, Tensor]: A tuple containing the content code and style code tensors extracted from the input.\n\"\"\"",
                    "source_code": "content_code = self.content_encoder(x)\n        style_code = self.style_encoder(x)\n        return content_code, style_code"
                }
            ],
            "name": "Encoder",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Decoder module by constructing a sequence of residual, upsampling, normalization, and activation layers, followed by a final output transformation. The method also prepares a multi-layer perceptron to generate adaptive normalization parameters from a style vector. This architecture enables the model to reconstruct or synthesize images from encoded feature representations while adapting content based on style information.\n\nArgs:\n    dim (int): Base number of feature channels for the upsampling layers.\n    out_channels (int): Number of channels in the output image.\n    n_residual (int): Number of residual blocks to insert at the bottleneck.\n    n_upsample (int): Number of upsampling (resolution increase) steps.\n    style_dim (int): Dimensionality of the style code used to modulate normalization.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Decoder, self).__init__()\n\n        layers = []\n        dim = dim * 2 ** n_upsample\n        # Residual blocks\n        for _ in range(n_residual):\n            layers += [ResidualBlock(dim, norm=\"adain\")]\n\n        # Upsampling\n        for _ in range(n_upsample):\n            layers += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(dim, dim // 2, 5, stride=1, padding=2),\n                LayerNorm(dim // 2),\n                nn.ReLU(inplace=True),\n            ]\n            dim = dim // 2\n\n        # Output layer\n        layers += [nn.ReflectionPad2d(3), nn.Conv2d(dim, out_channels, 7), nn.Tanh()]\n\n        self.model = nn.Sequential(*layers)\n\n        # Initiate mlp (predicts AdaIN parameters)\n        num_adain_params = self.get_num_adain_params()\n        self.mlp = MLP(style_dim, num_adain_params)"
                },
                {
                    "docstring": "\"\"\"Return the number of AdaIN parameters needed by the model\"\"\"",
                    "first_doc": "\"\"\"\nReturns the total number of AdaIN (Adaptive Instance Normalization) parameters required by the model.\n\nArgs:\n    self: The instance of the model class, used to access the model's modules.\n\nReturns:\n    int: The total number of AdaIN parameters needed, calculated as twice the number of features for each AdaptiveInstanceNorm2d layer in the model.\n\"\"\"",
                    "method_name": "get_num_adain_params",
                    "second_doc": "\"\"\"\nCalculates the total number of parameters associated with all AdaptiveInstanceNorm2d layers in the model. This is essential for configuring and managing the learnable parameters that control feature modulation during image generation and translation.\n\nArgs:\n    self: Instance of the Decoder model, allowing access to its submodules and layers.\n\nReturns:\n    int: Total count of AdaIN parameters, computed as twice the number of features for each AdaptiveInstanceNorm2d layer in the model. This accounts for both the scale and bias parameters required for adaptive normalization.\n\"\"\"",
                    "source_code": "num_adain_params = 0\n        for m in self.modules():\n            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n                num_adain_params += 2 * m.num_features\n        return num_adain_params"
                },
                {
                    "docstring": "\"\"\"Assign the adain_params to the AdaIN layers in model\"\"\"",
                    "first_doc": "\"\"\"\nAssigns the given AdaIN parameters to the AdaptiveInstanceNorm2d layers in the model.\n\nThis method iterates through all modules in the model and, for each AdaptiveInstanceNorm2d layer, splits and assigns the provided AdaIN parameters as bias and weight values.\n\nArgs:\n    self: The model instance containing AdaptiveInstanceNorm2d layers.\n    adain_params: A tensor containing the concatenated mean and standard deviation parameters to be assigned to each AdaIN layer in the model.\n\nReturns:\n    None: This method does not return a value; it updates the state of AdaIN layers within the model.\n\"\"\"",
                    "method_name": "assign_adain_params",
                    "second_doc": "\"\"\"\nDistributes the AdaIN (Adaptive Instance Normalization) parameters as biases and weights to each AdaptiveInstanceNorm2d layer within the model during the decoding process.\n\nThis function walks through the model's modules to match and update AdaptiveInstanceNorm2d layers with the provided normalization parameters, ensuring each layer adopts the correct style transformation statistics for subsequent computations. This assignment allows the model to dynamically adjust the image feature normalization, which is essential for producing outputs with desired characteristics.\n\nArgs:\n    self: The decoder model instance that contains one or more AdaptiveInstanceNorm2d layers.\n    adain_params (torch.Tensor): Tensor of shape (batch_size, total_params) holding concatenated mean and standard deviation values for all AdaptiveInstanceNorm2d layers to update.\n\nReturns:\n    None: The method updates layer parameters in-place; no value is returned.\n\"\"\"",
                    "source_code": "for m in self.modules():\n            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n                # Extract mean and std predictions\n                mean = adain_params[:, : m.num_features]\n                std = adain_params[:, m.num_features : 2 * m.num_features]\n                # Update bias and weight\n                m.bias = mean.contiguous().view(-1)\n                m.weight = std.contiguous().view(-1)\n                # Move pointer\n                if adain_params.size(1) > 2 * m.num_features:\n                    adain_params = adain_params[:, 2 * m.num_features :]"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates an output image by adapting the content representation according to the provided style code.\n\nThis method adjusts the normalization parameters of the decoder using style information to guide the image synthesis process, enabling controlled and diverse image generation.\n\nArgs:\n    content_code (Tensor): Feature representation encoding the structural content to be decoded.\n    style_code (Tensor): Feature representation encoding the desired style to be applied.\n\nReturns:\n    Tensor: The generated image after combining content and style information.\n\"\"\"",
                    "source_code": "self.assign_adain_params(self.mlp(style_code))\n        img = self.model(content_code)\n        return img"
                }
            ],
            "name": "Decoder",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ContentEncoder module by building a sequential network that extracts and transforms meaningful feature representations from input images. The architecture applies an initial convolution block for early feature capture, followed by multiple downsampling layers to reduce spatial dimensions and increase feature depth, and concludes with a series of residual blocks to further refine features while maintaining important information. This approach enables downstream tasks to leverage rich and compact content features for advanced image manipulation and generation.\n\nArgs:\n    in_channels (int): Number of channels in the input images.\n    dim (int): Number of channels in the first convolution layer, controlling feature dimensionality.\n    n_downsample (int): Number of downsampling layers to progressively reduce spatial size and increase depth.\n    n_residual (int): Number of residual blocks to enhance feature learning and preserve important content.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ContentEncoder, self).__init__()\n\n        # Initial convolution block\n        layers = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels, dim, 7),\n            nn.InstanceNorm2d(dim),\n            nn.ReLU(inplace=True),\n        ]\n\n        # Downsampling\n        for _ in range(n_downsample):\n            layers += [\n                nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1),\n                nn.InstanceNorm2d(dim * 2),\n                nn.ReLU(inplace=True),\n            ]\n            dim *= 2\n\n        # Residual blocks\n        for _ in range(n_residual):\n            layers += [ResidualBlock(dim, norm=\"in\")]\n\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses the input through the encoder network and returns the transformed feature representation.\n\nArgs:\n    x (torch.Tensor): Input tensor, typically representing an image or batch of images.\n\nReturns:\n    torch.Tensor: Encoded feature tensor obtained by applying the encoder model to the input.\n    \nWhy:\n    Encoding the input into a compact feature representation enables subsequent generative or discriminative tasks, such as image synthesis or translation, by providing meaningful high-level information extracted from the raw input.\n\"\"\"",
                    "source_code": "return self.model(x)"
                }
            ],
            "name": "ContentEncoder",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the StyleEncoder neural network by defining a sequence of convolutional, activation, and pooling layers to progressively extract and compress image features into a compact latent representation. This process allows the model to efficiently encode input images into a vector suitable for downstream tasks such as style extraction or image translation.\n\nArgs:\n    in_channels (int): Number of channels in the input images.\n    dim (int): Base number of feature maps for the initial convolution.\n    n_downsample (int): Number of times to downsample the input, reducing spatial resolution.\n    style_dim (int): Dimension of the output style vector.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(StyleEncoder, self).__init__()\n\n        # Initial conv block\n        layers = [nn.ReflectionPad2d(3), nn.Conv2d(in_channels, dim, 7), nn.ReLU(inplace=True)]\n\n        # Downsampling\n        for _ in range(2):\n            layers += [nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n            dim *= 2\n\n        # Downsampling with constant depth\n        for _ in range(n_downsample - 2):\n            layers += [nn.Conv2d(dim, dim, 4, stride=2, padding=1), nn.ReLU(inplace=True)]\n\n        # Average pool and output layer\n        layers += [nn.AdaptiveAvgPool2d(1), nn.Conv2d(dim, style_dim, 1, 1, 0)]\n\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the underlying model, transforming the input tensor into a latent representation that captures the essential style features necessary for subsequent generative tasks.\n\nArgs:\n    x (torch.Tensor): Input tensor containing image data to be encoded.\n\nReturns:\n    torch.Tensor: Output tensor representing the encoded style features.\n\"\"\"",
                    "source_code": "return self.model(x)"
                }
            ],
            "name": "StyleEncoder",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a multilayer perceptron (MLP) with a configurable number of linear and activation layers to process input features and map them to a target output space. This structured approach allows for flexible construction of deep, non-linear transformations, which are critical for learning complex data representations.\n\nArgs:\n    input_dim (int): Number of input features to the network.\n    dim (int): Size of the hidden layers.\n    output_dim (int): Size of the network output.\n    n_blk (int): Total number of linear layers in the MLP.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(MLP, self).__init__()\n        layers = [nn.Linear(input_dim, dim), nn.ReLU(inplace=True)]\n        for _ in range(n_blk - 2):\n            layers += [nn.Linear(dim, dim), nn.ReLU(inplace=True)]\n        layers += [nn.Linear(dim, output_dim)]\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nFeeds the input tensor through the multi-layer perceptron (MLP) after flattening non-batch dimensions. This step ensures that the input is appropriately formatted for processing by the linear layers, enabling consistent feature extraction or transformation for further tasks.\n\nArgs:\n    x (torch.Tensor): Input tensor of arbitrary shape, typically containing image data.\n\nReturns:\n    torch.Tensor: Output tensor resulting from passing the flattened input through the MLP.\n\"\"\"",
                    "source_code": "return self.model(x.view(x.size(0), -1))"
                }
            ],
            "name": "MLP",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the MultiDiscriminator by constructing multiple discriminator networks, each composed of sequential convolutional layers and normalization steps for hierarchical feature extraction. This encourages the model to effectively differentiate between generated and real images at multiple scales.\n\nArgs:\n    in_channels (int): Number of input channels for the discriminators.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(MultiDiscriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        # Extracts three discriminator models\n        self.models = nn.ModuleList()\n        for i in range(3):\n            self.models.add_module(\n                \"disc_%d\" % i,\n                nn.Sequential(\n                    *discriminator_block(in_channels, 64, normalize=False),\n                    *discriminator_block(64, 128),\n                    *discriminator_block(128, 256),\n                    *discriminator_block(256, 512),\n                    nn.Conv2d(512, 1, 3, padding=1)\n                ),\n            )\n\n        self.downsample = nn.AvgPool2d(in_channels, stride=2, padding=[1, 1], count_include_pad=False)"
                },
                {
                    "docstring": "\"\"\"Computes the MSE between model output and scalar gt\"\"\"",
                    "first_doc": "\"\"\"\nCalculates the mean squared error (MSE) loss between the model's predictions and a ground truth scalar value.\n\nArgs:\n    x: Input data that will be passed through the model's forward pass.\n    gt: The scalar ground truth value to compare against model outputs.\n\nReturns:\n    The computed MSE loss between the model outputs and the ground truth value.\n\"\"\"",
                    "method_name": "compute_loss",
                    "second_doc": "\"\"\"\nComputes the mean squared error (MSE) loss between the outputs produced by multiple discriminators and a given scalar ground truth, enabling the training process to adjust model parameters based on prediction accuracy.\n\nArgs:\n    x: Input data to be evaluated by the set of discriminators.\n    gt: Scalar ground truth value for comparison with discriminator outputs.\n\nReturns:\n    The sum of MSE losses for each discriminator output compared to the ground truth value.\n\"\"\"",
                    "source_code": "loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n        return loss"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies a sequence of discriminators to progressively downsampled inputs, collecting the output from each discriminator at every stage.\n\nThis approach enables multi-scale evaluation of the input, which can help in capturing both fine and coarse details during adversarial training.\n\nArgs:\n    x (torch.Tensor): The input tensor to be processed by the multi-scale discriminators.\n\nReturns:\n    List[torch.Tensor]: A list of outputs from each discriminator, corresponding to different input resolutions.\n\"\"\"",
                    "source_code": "outputs = []\n        for m in self.models:\n            outputs.append(m(x))\n            x = self.downsample(x)\n        return outputs"
                }
            ],
            "name": "MultiDiscriminator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a residual block consisting of two convolutional layers, normalization layers (either adaptive instance normalization or instance normalization), and ReLU activation, to enhance feature transformation while preserving input information through residual connections.\n\nArgs:\n    features (int): Number of feature channels in the input and output of the block.\n    norm (str): Type of normalization to apply (\"adain\" for adaptive instance normalization, otherwise uses standard instance normalization).\n\nReturns:\n    None\n\nWhy:\n    This design allows deeper networks to learn efficiently by maintaining stable gradients, enabling the model to capture complex patterns without degrading performance as network depth increases.\n\"\"\"",
                    "source_code": "super(ResidualBlock, self).__init__()\n\n        norm_layer = AdaptiveInstanceNorm2d if norm == \"adain\" else nn.InstanceNorm2d\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            norm_layer(features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            norm_layer(features),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass by adding the input tensor to the output of a subnetwork, enabling the learning of residual mappings. This approach helps improve gradient flow and model convergence in deep neural networks.\n\nArgs:\n    x (torch.Tensor): Input tensor to be processed by the residual block.\n\nReturns:\n    torch.Tensor: Output tensor resulting from the element-wise addition of the input and the subnetwork's output.\n\"\"\"",
                    "source_code": "return x + self.block(x)"
                }
            ],
            "name": "ResidualBlock",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the AdaptiveInstanceNorm2d layer by setting the number of features, normalization parameters, and creating non-trainable buffers for mean and variance statistics.\n\nArgs:\n    num_features (int): Number of feature channels to normalize.\n    eps (float, optional): Small epsilon value to improve numerical stability. Default is 1e-5.\n    momentum (float, optional): Value used for running mean and variance computation. Default is 0.1.\n\nWhy:\n    This setup allows the normalization statistics and affine parameters to be dynamically controlled, facilitating flexible feature normalization in generative models.\n\"\"\"",
                    "source_code": "super(AdaptiveInstanceNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        # weight and bias are dynamically assigned\n        self.weight = None\n        self.bias = None\n        # just dummy buffers, not used\n        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n        self.register_buffer(\"running_var\", torch.ones(num_features))"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies adaptive instance normalization to the input tensor, aligning its feature statistics with the provided scale (weight) and shift (bias) parameters for each batch instance. This normalization allows input features to be adapted dynamically, facilitating transformations that reflect learned style or domain characteristics.\n\nArgs:\n    x (Tensor): Input tensor of shape (batch_size, num_channels, height, width).\n\nReturns:\n    Tensor: Output tensor with the same shape as the input, normalized and affinely transformed per instance.\n\nRaises:\n    AssertionError: If the weight or bias parameters have not been assigned before calling this method.\n\"\"\"",
                    "source_code": "assert (\n            self.weight is not None and self.bias is not None\n        ), \"Please assign weight and bias before calling AdaIN!\"\n        b, c, h, w = x.size()\n        running_mean = self.running_mean.repeat(b)\n        running_var = self.running_var.repeat(b)\n\n        # Apply instance norm\n        x_reshaped = x.contiguous().view(1, b * c, h, w)\n\n        out = F.batch_norm(\n            x_reshaped, running_mean, running_var, self.weight, self.bias, True, self.momentum, self.eps\n        )\n\n        return out.view(b, c, h, w)"
                },
                {
                    "docstring": null,
                    "method_name": "__repr__",
                    "second_doc": "\"\"\"\nProvides a concise string representation of the AdaptiveInstanceNorm2d instance, showing its class name and the number of features.\n\nThis facilitates clearer debugging and logging by making it easy to identify and distinguish between different layers in complex model architectures.\n\nReturns:\n    str: A string in the format \"AdaptiveInstanceNorm2d(<num_features>)\" describing the instance.\n\"\"\"",
                    "source_code": "return self.__class__.__name__ + \"(\" + str(self.num_features) + \")\""
                }
            ],
            "name": "AdaptiveInstanceNorm2d",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the LayerNorm module, configuring parameters for feature normalization across inputs. This helps stabilize and speed up neural network training by maintaining consistent feature distributions. \n\nArgs:\n    num_features (int): Number of features in the input to be normalized.\n    affine (bool): If True, enables learnable scaling (gamma) and shifting (beta) parameters.\n    eps (float): A small value to avoid division by zero during normalization.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies layer normalization to the input tensor to stabilize and accelerate training by normalizing activations per instance. This ensures that the input to each layer maintains consistent statistics, which helps the model learn more effectively.\n\nArgs:\n    x (torch.Tensor): Input tensor to be normalized.\n\nReturns:\n    torch.Tensor: The normalized tensor, with optional affine transformation applied if enabled.\n\"\"\"",
                    "source_code": "shape = [-1] + [1] * (x.dim() - 1)\n        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n        std = x.view(x.size(0), -1).std(1).view(*shape)\n        x = (x - mean) / (std + self.eps)\n\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n        return x"
                }
            ],
            "name": "LayerNorm",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/munit/munit.py": [
        {
            "details": {
                "docstring": "\"\"\"Saves a generated sample from the validation set\"\"\"",
                "first_doc": "\"\"\"\nGenerates and saves a grid of image samples by combining images from two domains with randomly sampled style codes.\n\nArgs:\n    batches_done: The current batch index or step, used to name the saved image file.\n\nReturns:\n    None: This method saves the generated image grid to disk and does not return a value.\n\"\"\"",
                "method_name": "sample_images",
                "second_doc": "\"\"\"\nProduces and saves a composite grid of images by blending samples from two distinct datasets using randomly generated style codes to demonstrate the capacity for visual transformation and style variation.\n\nArgs:\n    batches_done (int): The current batch index or step, used as part of the filename for the saved image grid.\n\nReturns:\n    None: The generated grid of images is saved to disk for visualization or analysis; no value is returned.\n\"\"\"",
                "source_code": "imgs = next(iter(val_dataloader))\n    img_samples = None\n    for img1, img2 in zip(imgs[\"A\"], imgs[\"B\"]):\n        # Create copies of image\n        X1 = img1.unsqueeze(0).repeat(opt.style_dim, 1, 1, 1)\n        X1 = Variable(X1.type(Tensor))\n        # Get random style codes\n        s_code = np.random.uniform(-1, 1, (opt.style_dim, opt.style_dim))\n        s_code = Variable(Tensor(s_code))\n        # Generate samples\n        c_code_1, _ = Enc1(X1)\n        X12 = Dec2(c_code_1, s_code)\n        # Concatenate samples horisontally\n        X12 = torch.cat([x for x in X12.data.cpu()], -1)\n        img_sample = torch.cat((img1, X12), -1).unsqueeze(0)\n        # Concatenate with previous samples vertically\n        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n    save_image(img_samples, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/pix2pix/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the image dataset by setting up transformation pipelines and collecting file paths from the specified directories based on the provided mode. This ensures the dataset can deliver uniformly processed images for model training or evaluation.\n\nArgs:\n    root (str): Root directory of the dataset.\n    transforms_ (list): List of image transformations to be applied.\n    mode (str): Dataset mode, such as 'train' or 'test', which determines which directories are loaded.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transform = transforms.Compose(transforms_)\n\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n        if mode == \"train\":\n            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.*\")))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves and processes a pair of related images from the dataset by loading an image file, splitting it into two halves, optionally applying a random horizontal flip for data augmentation, and transforming each half. This allows the model to learn from corresponding image pairs with varied orientations.\n\nArgs:\n    index (int): Index of the item in the dataset to retrieve.\n\nReturns:\n    dict: A dictionary containing two processed images, {'A': img_A, 'B': img_B}, where each image represents one half of the original image after transformation.\n\"\"\"",
                    "source_code": "img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w / 2, h))\n        img_B = img.crop((w / 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of image files in the dataset, enabling integration with data loading utilities and facilitating batching during training and evaluation.\n\nReturns:\n    int: The number of image files contained within the dataset.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/pix2pix/models.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional and batch normalization layers in a neural network with specific normal distributions to promote stable learning dynamics and convergence during training.\n\nArgs:\n    m (torch.nn.Module): The neural network layer whose weights (and bias, if applicable) will be initialized.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a downsampling block used in U-Net architectures, applying convolution and optional normalization, nonlinear activation, and dropout to reduce spatial dimensions while preserving feature information.\n\nArgs:\n    in_size (int): Number of input channels.\n    out_size (int): Number of output channels.\n    normalize (bool): If True, applies instance normalization after convolution.\n    dropout (float, optional): Dropout rate; if specified, applies dropout after activation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies the sequence of operations defined in the model to the input tensor, enabling feature extraction and transformation for subsequent processing in the network.\n\nArgs:\n    x (torch.Tensor): Input tensor representing image or feature map data.\n\nReturns:\n    torch.Tensor: Output tensor after transformation by the model.\n\"\"\"",
                    "source_code": "return self.model(x)"
                }
            ],
            "name": "UNetDown",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the upsampling block for the network by constructing a sequence of layers that perform learned upsampling, normalization, activation, and optional dropout on input feature maps.\n\nArgs:\n    in_size (int): Number of input feature channels.\n    out_size (int): Number of output feature channels.\n    dropout (float, optional): Dropout probability. If provided, a dropout layer is added after the activation.\n\nReturns:\n    None\n\nWhy:\n    This method builds an upsampling module that facilitates information flow from lower-resolution to higher-resolution feature maps, enabling the network to reconstruct finer details during the decoding process.\n\"\"\"",
                    "source_code": "super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n            nn.InstanceNorm2d(out_size),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses input through the upsampling block, integrates features from an earlier stage via concatenation with skip connections, and returns the combined feature map. This enables the model to retain and merge information from different network depths, which is critical for precise image reconstruction in encoder-decoder architectures.\n\nArgs:\n    x (torch.Tensor): Input tensor to the upsampling block, typically output from the previous layer.\n    skip_input (torch.Tensor): Feature map from an earlier layer to be concatenated, implementing the skip connection.\n\nReturns:\n    torch.Tensor: Output tensor after upsampling and concatenation, containing merged features for subsequent processing.\n\"\"\"",
                    "source_code": "x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x"
                }
            ],
            "name": "UNetUp",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the U-Net based generator network by stacking encoder (downsampling) and decoder (upsampling) layers to efficiently capture and reconstruct multi-scale image features. This layered design promotes detailed and coherent image outputs by combining low- and high-level representations during generation.\n\nArgs:\n    in_channels (int): Number of channels in the input image.\n    out_channels (int): Number of channels in the output image.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(GeneratorUNet, self).__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(128, out_channels, 4, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nDefines the forward pass through the generator's U-Net architecture, transforming the input tensor through a sequence of downsampling and upsampling layers with skip connections to produce a synthesized output. This design enables the network to leverage both global and local context for high-quality generation.\n\nArgs:\n    x (torch.Tensor): Input image tensor to be transformed by the generator network.\n\nReturns:\n    torch.Tensor: Output tensor representing the generated image.\n\"\"\"",
                    "source_code": "d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        return self.final(u7)"
                }
            ],
            "name": "GeneratorUNet",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator network by constructing a sequence of convolutional layers that progressively downsample and process concatenated input data. The architecture is designed to capture important spatial features and enable the network to distinguish between different types of input pairs.\n\nArgs:\n    in_channels (int): Number of channels in the input images.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels * 2, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass of the Discriminator by concatenating two input images along the channel dimension and passing the combined tensor through the discriminator model. This enables the network to evaluate whether the paired images are consistent or plausible within its learned data distribution.\n\nArgs:\n    img_A (torch.Tensor): First input image tensor.\n    img_B (torch.Tensor): Second input image tensor to be concatenated with the first.\n\nReturns:\n    torch.Tensor: The Discriminator's prediction for the concatenated image pair, indicating their authenticity or consistency.\n\"\"\"",
                    "source_code": "img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/pix2pix/pix2pix.py": [
        {
            "details": {
                "docstring": "\"\"\"Saves a generated sample from the validation set\"\"\"",
                "first_doc": "\"\"\"\nSaves a generated sample image during model training.\n\nThis method fetches a batch of validation images, generates a fake image using the generator, concatenates real and fake samples, and saves the resulting image grid to disk for visualization.\n\nArgs:\n    batches_done: The current number of batches processed, used to name the saved image file.\n\nReturns:\n    None: This method does not return a value.\n\"\"\"",
                "method_name": "sample_images",
                "second_doc": "\"\"\"\nGenerates and saves a visual comparison of real and generated images during training, allowing users to monitor the model's progress and output quality.\n\nThis method retrieves a batch of validation data, uses the generator to produce corresponding synthetic samples, concatenates real and generated results for side-by-side visualization, and stores them as an image grid on disk.\n\nArgs:\n    batches_done (int): The current number of processed batches, used to name the output image for sequential tracking.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "imgs = next(iter(val_dataloader))\n    real_A = Variable(imgs[\"B\"].type(Tensor))\n    real_B = Variable(imgs[\"A\"].type(Tensor))\n    fake_B = generator(real_A)\n    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/pixelda/mnistm.py": [
        {
            "methods": [
                {
                    "docstring": "\"\"\"Init MNIST-M dataset.\"\"\"",
                    "first_doc": "\"\"\"\nInitializes the MNIST-M dataset object.\n\nArgs:\n    root: Root directory where the MNIST-M dataset is stored.\n\nClass Fields:\n    root: Expanded path to the root directory for the MNIST-M dataset.\n    mnist_root: Expanded path to the MNIST dataset directory.\n    transform: Function or transform to apply to the images.\n    target_transform: Function or transform to apply to the labels.\n    train: Boolean indicating whether the dataset represents the training set or the test set.\n    train_data: Tensor containing the training images (initialized if train=True).\n    train_labels: Tensor containing the training labels (initialized if train=True).\n    test_data: Tensor containing the test images (initialized if train=False).\n    test_labels: Tensor containing the test labels (initialized if train=False).\n\nReturns:\n    None. Initializes the MNIST-M dataset object with the provided configuration.\n\"\"\"",
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes and configures the MNIST-M dataset object, loading image and label data for training or testing. This method sets up the necessary file paths, applies user-defined transforms, and ensures the dataset exists or downloads it if specified. By preparing and validating dataset access, it enables downstream models and experiments to work with consistent, preprocessed visual data.\n\nArgs:\n    root (str): Root directory where the MNIST-M dataset is stored.\n    mnist_root (str): Directory path to the original MNIST dataset.\n    transform (callable, optional): Transformation to apply to the images.\n    target_transform (callable, optional): Transformation to apply to the labels.\n    train (bool, optional): Whether to load the training set (True) or test set (False).\n    download (bool, optional): If True, downloads the dataset if not present.\n\nReturns:\n    None. Prepares dataset fields for subsequent access, raising an error if data is unavailable and download is not enabled.\n\"\"\"",
                    "source_code": "super(MNISTM, self).__init__()\n        self.root = os.path.expanduser(root)\n        self.mnist_root = os.path.expanduser(mnist_root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.train = train  # training set or test set\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError('Dataset not found.' +\n                               ' You can use download=True to download it')\n\n        if self.train:\n            self.train_data, self.train_labels = \\\n                torch.load(os.path.join(self.root,\n                                        self.processed_folder,\n                                        self.training_file))\n        else:\n            self.test_data, self.test_labels = \\\n                torch.load(os.path.join(self.root,\n                                        self.processed_folder,\n                                        self.test_file))"
                },
                {
                    "docstring": "\"\"\"Get images and target for data loader.\n\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"",
                    "first_doc": "\"\"\"\nGet an image and its corresponding target label for a given index.\n\nArgs:\n    index: Index of the sample to retrieve.\n\nReturns:\n    tuple: A tuple (image, target) where image is a PIL Image object representing the sample at the given index, and target is the index of the target class, possibly transformed.\n\"\"\"",
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves a sample image and its corresponding label for a specified index, applying consistent formatting and optional transformations. This ensures compatibility with other datasets and seamless integration into data pipelines.\n\nArgs:\n    index (int): The index of the data sample to retrieve.\n\nReturns:\n    tuple: (image, target), where image is a PIL Image object of the sample at the specified index, and target is the (optionally transformed) label associated with that sample.\n\"\"\"",
                    "source_code": "if self.train:\n            img, target = self.train_data[index], self.train_labels[index]\n        else:\n            img, target = self.test_data[index], self.test_labels[index]\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(img.squeeze().numpy(), mode='RGB')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target"
                },
                {
                    "docstring": "\"\"\"Return size of dataset.\"\"\"",
                    "first_doc": "\"\"\"\nReturns the number of samples in the current dataset split.\n\nDepending on the mode (training or testing), this method returns the size of the corresponding dataset.\n\nArgs:\n    self: The instance of the dataset class.\n\nReturns:\n    int: The number of samples in the training or testing dataset, depending on the current mode.\n\"\"\"",
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of data samples in the active subset (training or testing), enabling dynamic adjustment of batch processing and iteration counts.\n\nArgs:\n    self: Instance of the dataset class, which tracks mode (training or testing) and corresponding data.\n\nReturns:\n    int: Count of samples in either the training or testing data according to current mode.\n\"\"\"",
                    "source_code": "if self.train:\n            return len(self.train_data)\n        else:\n            return len(self.test_data)"
                },
                {
                    "docstring": null,
                    "method_name": "_check_exists",
                    "second_doc": "\"\"\"\nChecks whether the processed training and test files exist in the expected data directory.\n\nThis check ensures that required dataset files have been correctly prepared and stored before attempting to access or process them.\n\nReturns:\n    bool: True if both processed training and test files exist, False otherwise.\n\"\"\"",
                    "source_code": "return os.path.exists(os.path.join(self.root,\n                                           self.processed_folder,\n                                           self.training_file)) and \\\n            os.path.exists(os.path.join(self.root,\n                                        self.processed_folder,\n                                        self.test_file))"
                },
                {
                    "docstring": "\"\"\"Download the MNIST data.\"\"\"",
                    "first_doc": "\"\"\"\nDownloads the MNIST-M dataset, processes it, and saves it in a structured format for further use.\n\nArgs:\n    self: The instance of the class.\n\nReturns:\n    None. This method performs downloading, extraction, processing, and saving of data files, but does not return a value.\n\nClass Fields Initialized:\n    root: Directory where MNIST-M data will be stored.\n    raw_folder: Subdirectory for raw input files.\n    processed_folder: Subdirectory for processed dataset files.\n    training_file: Filename for the processed training dataset.\n    test_file: Filename for the processed test dataset.\n    url: The source URL to download the MNIST-M data.\n    mnist_root: Directory where the original MNIST digit data is stored, used to retrieve labels.\n\"\"\"",
                    "method_name": "download",
                    "second_doc": "\"\"\"\nAutomates the retrieval, extraction, and preprocessing of the MNIST-M dataset, ensuring its availability in a format compatible with downstream image modeling and machine learning workflows. This method guarantees consistent data preparation, minimizing manual intervention and potential errors, and making it easy to integrate the dataset into data pipelines or experiments.\n\nArgs:\n    self: The instance of the MNISTM class.\n\nReturns:\n    None: This method performs the downloading, extraction, processing, and local saving of the dataset but does not return any value.\n\nClass Fields Utilized:\n    root: Directory where the MNIST-M data is stored.\n    raw_folder: Subdirectory for raw input files.\n    processed_folder: Subdirectory for processed data files.\n    training_file: Path for the processed training data file.\n    test_file: Path for the processed test data file.\n    url: The web address used to download the MNIST-M dataset.\n    mnist_root: Directory for the original MNIST digit data used to retrieve labels.\n\"\"\"",
                    "source_code": "# import essential packages\n        from six.moves import urllib\n        import gzip\n        import pickle\n        from torchvision import datasets\n\n        # check if dataset already exists\n        if self._check_exists():\n            return\n\n        # make data dirs\n        try:\n            os.makedirs(os.path.join(self.root, self.raw_folder))\n            os.makedirs(os.path.join(self.root, self.processed_folder))\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise\n\n        # download pkl files\n        print('Downloading ' + self.url)\n        filename = self.url.rpartition('/')[2]\n        file_path = os.path.join(self.root, self.raw_folder, filename)\n        if not os.path.exists(file_path.replace('.gz', '')):\n            data = urllib.request.urlopen(self.url)\n            with open(file_path, 'wb') as f:\n                f.write(data.read())\n            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n                    gzip.GzipFile(file_path) as zip_f:\n                out_f.write(zip_f.read())\n            os.unlink(file_path)\n\n        # process and save as torch files\n        print('Processing...')\n\n        # load MNIST-M images from pkl file\n        with open(file_path.replace('.gz', ''), \"rb\") as f:\n            mnist_m_data = pickle.load(f, encoding='bytes')\n        mnist_m_train_data = torch.ByteTensor(mnist_m_data[b'train'])\n        mnist_m_test_data = torch.ByteTensor(mnist_m_data[b'test'])\n\n        # get MNIST labels\n        mnist_train_labels = datasets.MNIST(root=self.mnist_root,\n                                            train=True,\n                                            download=True).train_labels\n        mnist_test_labels = datasets.MNIST(root=self.mnist_root,\n                                           train=False,\n                                           download=True).test_labels\n\n        # save MNIST-M dataset\n        training_set = (mnist_m_train_data, mnist_train_labels)\n        test_set = (mnist_m_test_data, mnist_test_labels)\n        with open(os.path.join(self.root,\n                               self.processed_folder,\n                               self.training_file), 'wb') as f:\n            torch.save(training_set, f)\n        with open(os.path.join(self.root,\n                               self.processed_folder,\n                               self.test_file), 'wb') as f:\n            torch.save(test_set, f)\n\n        print('Done!')"
                }
            ],
            "name": "MNISTM",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/pixelda/pixelda.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional and batch normalization layers of a given module using a normal distribution with carefully chosen mean and standard deviation. This approach aims to promote stable and efficient model convergence by providing suitable starting values for learning.\n\nArgs:\n    m (torch.nn.Module): The network module whose weights will be initialized.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the sequence of convolutional, batch normalization, and activation layers that form the core operations of the residual block.\n\nArgs:\n    in_features (int): Number of input and output feature channels for the block.\n\nReturns:\n    None\n\nWhy:\n    This structure allows the network to learn residual mappings, which helps stabilize and accelerate the training of deep neural architectures by preserving input information across layers.\n\"\"\"",
                    "source_code": "super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, 3, 1, 1),\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, in_features, 3, 1, 1),\n            nn.BatchNorm2d(in_features),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the residual block by applying the transformation defined by the block and adding the original input, enabling the model to learn residual mappings that ease the optimization process during training.\n\nArgs:\n    x (torch.Tensor): Input tensor to the residual block.\n\nReturns:\n    torch.Tensor: Output tensor after applying the residual connection.\n\"\"\"",
                    "source_code": "return x + self.block(x)"
                }
            ],
            "name": "ResidualBlock",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the generator network with layers designed to transform random noise vectors into synthetic images. This setup includes fully connected, convolutional, and residual layers to progressively build and refine image features from the latent input.\n\nArgs:\n    opt: An options object containing configuration parameters such as latent dimension, image size, number of channels, and number of residual blocks.\n\nReturns:\n    None. This constructor method sets up the generator's neural network layers for subsequent use in forward passes.\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        # Fully-connected layer which constructs image channel shaped output from noise\n        self.fc = nn.Linear(opt.latent_dim, opt.channels * opt.img_size ** 2)\n\n        self.l1 = nn.Sequential(nn.Conv2d(opt.channels * 2, 64, 3, 1, 1), nn.ReLU(inplace=True))\n\n        resblocks = []\n        for _ in range(opt.n_residual_blocks):\n            resblocks.append(ResidualBlock())\n        self.resblocks = nn.Sequential(*resblocks)\n\n        self.l2 = nn.Sequential(nn.Conv2d(64, opt.channels, 3, 1, 1), nn.Tanh())"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates a new image by conditioning on an input image and a latent vector.\n\nArgs:\n    img (torch.Tensor): The input image tensor serving as a conditional context.\n    z (torch.Tensor): The latent noise vector used to introduce variability to the generated output.\n\nReturns:\n    torch.Tensor: The generated image tensor transformed by the network.\n    \nThis method combines learned features from the input image and a noise vector to enable flexible synthesis of outputs, enhancing the diversity and fidelity of generated samples.\n\"\"\"",
                    "source_code": "gen_input = torch.cat((img, self.fc(z).view(*img.shape)), 1)\n        out = self.l1(gen_input)\n        out = self.resblocks(out)\n        img_ = self.l2(out)\n\n        return img_"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the architecture for the Discriminator network, constructing a sequence of convolutional layers designed to progressively extract hierarchical features and evaluate input authenticity.\n\nArgs:\n    opt (object): An options object containing model hyperparameters, including the number of input channels.\n\nReturns:\n    None\n\nThis setup is used to enable the network to distinguish between real and generated data by learning relevant image representations through stacked convolutional and normalization operations.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def block(in_features, out_features, normalization=True):\n            \"\"\"Discriminator block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_features))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.channels, 64, normalization=False),\n            *block(64, 128),\n            *block(128, 256),\n            *block(256, 512),\n            nn.Conv2d(512, 1, 3, 1, 1)\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPasses an input image through the discriminator model to produce a validity score, which assesses how likely the image is to be real or generated.  \nArgs:\n    img (Tensor): Input image tensor to be evaluated by the discriminator.\n\nReturns:\n    Tensor: The validity score output by the model, indicating the discriminator's assessment of the input image.\n\"\"\"",
                    "source_code": "validity = self.model(img)\n\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Classifier module by constructing a hierarchical sequence of convolutional, activation, and normalization layers followed by a fully connected classification head.\n\nThis setup transforms input image features into class probabilities, thereby enabling the model to make predictions based on extracted representations from images.\n\nArgs:\n    opt: An object containing configuration options, including the number of image channels (opt.channels), target image size (opt.img_size), and the number of output classes (opt.n_classes).\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Classifier, self).__init__()\n\n        def block(in_features, out_features, normalization=True):\n            \"\"\"Classifier block\"\"\"\n            layers = [nn.Conv2d(in_features, out_features, 3, stride=2, padding=1), nn.LeakyReLU(0.2, inplace=True)]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_features))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.channels, 64, normalization=False), *block(64, 128), *block(128, 256), *block(256, 512)\n        )\n\n        input_size = opt.img_size // 2 ** 4\n        self.output_layer = nn.Sequential(nn.Linear(512 * input_size ** 2, opt.n_classes), nn.Softmax())"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nComputes output class labels for the given input images by first extracting feature representations with the internal model and then passing these features through an output layer. This process enables the classifier to evaluate the distinguishing features present in the images.\n\nArgs:\n    img (torch.Tensor): Batch of input images, typically of shape (batch_size, channels, height, width).\n\nReturns:\n    torch.Tensor: Predicted class labels or logits corresponding to the input images.\n\"\"\"",
                    "source_code": "feature_repr = self.model(img)\n        feature_repr = feature_repr.view(feature_repr.size(0), -1)\n        label = self.output_layer(feature_repr)\n        return label"
                }
            ],
            "name": "Classifier",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/relativistic_gan/relativistic_gan.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator model's architecture, defining the layers and upsampling operations needed to transform random latent vectors into realistic images.\n\nArgs:\n    opt: Namespace containing configuration options such as 'img_size', 'latent_dim', and 'channels'.\n\nReturns:\n    None\n\nWhy:\n    This initialization sets up the neural network structure that allows the Generator to progressively map input latent representations into plausible images through learned non-linear transformations, upsampling layers, and feature normalization steps. This is essential for enabling the model to synthesize images from random noise inputs.\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.init_size = opt.img_size // 4\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the generator network, transforming an input noise vector into an image tensor. This transformation enables the synthesis of new samples that can be evaluated or visualized.\n\nArgs:\n    z (Tensor): Input latent noise vector of shape (batch_size, latent_dim).\n\nReturns:\n    Tensor: Generated image tensor of shape (batch_size, num_channels, height, width).\n\"\"\"",
                    "source_code": "out = self.l1(z)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers for the discriminator neural network, which progressively downsample and transform input images to produce a single scalar output indicating authenticity. This layered structure allows the network to extract hierarchical features at different spatial resolutions, making it effective at distinguishing between real and generated images.\n\nArgs:\n    opt: Namespace object containing configuration parameters, including 'channels' (number of input image channels) and 'img_size' (height/width of input images).\n\nReturns:\n    None. The method sets up the internal layers for later use in forward passes.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.model = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1))"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the Discriminator network to evaluate whether input images are real or generated by condensing image features and producing a validity score. This helps guide the learning process by distinguishing between authentic and synthetic images.\n\nArgs:\n    img (torch.Tensor): Batch of images to be assessed for authenticity, typically of shape (batch_size, channels, height, width).\n\nReturns:\n    torch.Tensor: Validity scores for each input image, indicating the discriminator's confidence in their authenticity.\n\"\"\"",
                    "source_code": "out = self.model(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/sgan/sgan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional and batch normalization layers in a neural network using a normal distribution.\n\nThis method standardizes the weight initialization process for different types of layers to ensure better training stability and convergence. Proper parameter initialization helps prevent issues such as vanishing or exploding gradients, making it easier for neural networks to learn complex data representations.\n\nArgs:\n    m (torch.nn.Module): A module from the neural network, typically passed during model creation to apply custom weight initialization.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator's neural network layers to enable the transformation of random noise and class labels into synthetic images.\n\nArgs:\n    opt: An object containing configuration options such as num_classes (number of target classes), latent_dim (dimension of the noise vector), img_size (output image size), and channels (number of image channels).\n\nReturns:\n    None\n\nWhy:\n    The setup allows the model to take both latent noise and class information as input, embedding them effectively and progressively mapping them through fully connected and convolutional layers to produce high-resolution images. This layered structure provides the capacity needed for generating visually realistic samples.\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.label_emb = nn.Embedding(opt.num_classes, opt.latent_dim)\n\n        self.init_size = opt.img_size // 4  # Initial size before upsampling\n        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            nn.BatchNorm2d(128, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n            nn.BatchNorm2d(64, 0.8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n            nn.Tanh(),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass of the generator network, transforming input noise into a synthetic image through successive layers. This process enables the generation of diverse and realistic output samples from random latent vectors.\n\nArgs:\n    noise (torch.Tensor): Input noise tensor, typically sampled from a prior distribution such as a standard normal, shaped as (batch_size, latent_dim).\n\nReturns:\n    torch.Tensor: Generated image tensor with shape (batch_size, channels, height, width).\n\"\"\"",
                    "source_code": "out = self.l1(noise)\n        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n        img = self.conv_blocks(out)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers and structure of the Discriminator neural network, which processes input images to classify them as real or fake and to predict their associated class labels. This setup is essential for enabling adversarial training and multi-class discrimination in downstream generative modeling tasks.\n\nArgs:\n    opt: Namespace or object containing configuration options such as image size, number of classes, and input channels.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, bn=True):\n            \"\"\"Returns layers of each discriminator block\"\"\"\n            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n            if bn:\n                block.append(nn.BatchNorm2d(out_filters, 0.8))\n            return block\n\n        self.conv_blocks = nn.Sequential(\n            *discriminator_block(opt.channels, 16, bn=False),\n            *discriminator_block(16, 32),\n            *discriminator_block(32, 64),\n            *discriminator_block(64, 128),\n        )\n\n        # The height and width of downsampled image\n        ds_size = opt.img_size // 2 ** 4\n\n        # Output layers\n        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.num_classes + 1), nn.Softmax())"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses an input image tensor through the discriminator network to produce scores representing both the probability of the input being real or fake and its predicted class label. This dual output enables the model to distinguish between genuine and generated data while also performing auxiliary classification.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be evaluated by the discriminator network.\n\nReturns:\n    Tuple[torch.Tensor, torch.Tensor]: A tuple containing the validity score (real or fake probability) and the predicted class label tensor for the input image.\n\"\"\"",
                    "source_code": "out = self.conv_blocks(img)\n        out = out.view(out.shape[0], -1)\n        validity = self.adv_layer(out)\n        label = self.aux_layer(out)\n\n        return validity, label"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/softmax_gan/softmax_gan.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator by constructing a multi-layer perceptron network that transforms an input latent vector into an output matching the target image shape. The architecture progressively increases feature dimensionality and applies normalization and activation functions to enable learning of complex data distributions, ultimately allowing for the generation of realistic samples.\n\nArgs:\n    opt: Configuration options containing model hyperparameters, including 'latent_dim' specifying the size of the input noise vector, and 'img_shape' defining the shape of the generated output.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates and reshapes images from input latent vectors using the generator model.\n\nArgs:\n    z (Tensor): Input latent vector of shape (batch_size, latent_dim) representing the random noise samples.\n\nReturns:\n    Tensor: Generated images reshaped to the specified image dimensions, suitable for evaluation or visualization.\n\nThe method transforms latent vectors into output images as part of the generative process, enabling exploration and training of the model's image synthesis capabilities.\n\"\"\"",
                    "source_code": "img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator network as a feedforward neural model composed of linear and LeakyReLU layers, designed to distinguish between real and generated images by outputting a single prediction value.\n\nArgs:\n    opt: An object containing model configuration options, specifically the image size (img_size) for determining input dimensions.\n\nReturns:\n    None\n\nWhy:\n    This initialization constructs the necessary architecture for the Discriminator to effectively evaluate image authenticity, which is essential for training adversarial networks where the goal is to improve the generative model's capability.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(opt.img_size ** 2, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses an input image tensor by flattening it and passing it through the network to determine its validity, enabling the model to distinguish between real and generated images.\n\nArgs:\n    img (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n\nReturns:\n    torch.Tensor: The output validity prediction(s) for the input image(s).\n\"\"\"",
                    "source_code": "img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "log",
                "second_doc": "\"\"\"\nComputes the element-wise natural logarithm of the input tensor after shifting by a small constant to ensure numerical stability.\n\nArgs:\n    x (torch.Tensor): Input tensor for which to compute the logarithm.\n\nReturns:\n    torch.Tensor: Tensor containing the natural logarithm of the input values, safely computed to avoid issues with zero or negative inputs.\n\nWhy:\n    Adding a small constant before applying the logarithm prevents undefined or infinite values, which helps maintain robust and stable learning during model training and evaluation.\n\"\"\"",
                "source_code": "return torch.log(x + 1e-8)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/srgan/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes image transformation pipelines for preparing low- and high-resolution image pairs, enabling consistent preprocessing for downstream models.\n\nArgs:\n    hr_shape (tuple): Height and width of the high-resolution images as a tuple.\n    mean (tuple or list): Mean values for normalization.\n    std (tuple or list): Standard deviation values for normalization.\n    root (str): Root directory containing the dataset images.\n\nReturns:\n    None\n\nWhy:\nThis method prepares image data in standardized forms, facilitating model training or evaluation that requires paired high- and low-resolution inputs.\n\"\"\"",
                    "source_code": "hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n\n        self.files = sorted(glob.glob(root + \"/*.*\"))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves and processes an image from the dataset at the specified index, applying both low-resolution and high-resolution transformations.\n\nThis method is used to provide paired low- and high-resolution versions of images, enabling tasks such as super-resolution or image quality assessment during model training and evaluation.\n\nArgs:\n    index (int): Index of the image to retrieve from the dataset.\n\nReturns:\n    dict: A dictionary containing the transformed images with keys 'lr' for the low-resolution image and 'hr' for the high-resolution image.\n\"\"\"",
                    "source_code": "img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of image files available in the dataset.\n\nThis enables compatibility with data-loading utilities that require knowledge of dataset size for batching and iteration.\n\nReturns:\n    int: The number of image files contained in the dataset.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/srgan/models.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the FeatureExtractor by loading a pre-trained VGG19 model and extracting its early convolutional layers up to the 18th layer. This configuration enables efficient extraction of high-level image features, which is crucial for tasks that require perceptual similarity or advanced image understanding.\n\nArgs:\n    None\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses the input image tensor through the underlying feature extractor to obtain high-level feature representations. This approach supports downstream tasks that rely on learned feature embeddings, such as evaluating generated images or guiding model training.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be processed.\n\nReturns:\n    torch.Tensor: Feature representations extracted from the input image.\n\"\"\"",
                    "source_code": "return self.feature_extractor(img)"
                }
            ],
            "name": "FeatureExtractor",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes a residual block consisting of sequential convolutional, batch normalization, and PReLU layers. By stacking these layers, the block enables the learning of residual mappings that help stabilize deep network training and facilitate richer feature extraction for downstream processing.\n\nArgs:\n    in_features (int): Number of input and output feature channels for the convolutional layers.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n            nn.PReLU(),\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies a residual connection by adding the input tensor to the output of a convolutional block, allowing the network to learn modifications to the input while preserving original features.\n\nArgs:\n    x (torch.Tensor): Input tensor to the residual block.\n\nReturns:\n    torch.Tensor: Output tensor after applying the residual connection.\n\"\"\"",
                    "source_code": "return x + self.conv_block(x)"
                }
            ],
            "name": "ResidualBlock",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the GeneratorResNet by defining its architecture, which consists of an initial convolutional layer, multiple residual blocks, additional convolutional and batch normalization layers, upsampling stages, and an output layer. This design enables the model to effectively learn complex feature hierarchies and produce high-quality image outputs with enhanced details and resolution.\n\nArgs:\n    in_channels (int): Number of channels in the input image.\n    out_channels (int): Number of channels in the output image.\n    n_residual_blocks (int): Number of residual blocks used to build the network's deeper layers.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(GeneratorResNet, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n\n        # Residual blocks\n        res_blocks = []\n        for _ in range(n_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n\n        # Upsampling layers\n        upsampling = []\n        for out_features in range(2):\n            upsampling += [\n                # nn.Upsample(scale_factor=2),\n                nn.Conv2d(64, 256, 3, 1, 1),\n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),\n            ]\n        self.upsampling = nn.Sequential(*upsampling)\n\n        # Final output layer\n        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies a series of convolutional, residual, and upsampling operations to the input tensor, transforming features for further image synthesis or enhancement tasks.\n\nArgs:\n    x (torch.Tensor): Input tensor representing an image or feature map.\n\nReturns:\n    torch.Tensor: Output tensor with modified spatial and feature properties, suitable for high-quality image generation or refinement.\n\"\"\"",
                    "source_code": "out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out"
                }
            ],
            "name": "GeneratorResNet",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator module by constructing a series of convolutional and normalization layers designed to effectively distinguish between real and generated images at multiple spatial resolutions.\n\nArgs:\n    input_shape (tuple): A tuple representing the shape of an input image, defined as (channels, height, width).\n\nReturns:\n    None\n\nWhy:\n    By progressively downsampling image features while increasing the number of filters, the discriminator builds a rich hierarchical representation, enabling it to better capture visual cues and textures that differentiate authentic inputs from synthetic ones.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass of the input image through the network to compute its authenticity prediction. This step enables the model to evaluate and distinguish between real and generated images, facilitating its learning process.\n\nArgs:\n    img (torch.Tensor): Input image tensor to be evaluated by the discriminator.\n\nReturns:\n    torch.Tensor: Output tensor indicating the discriminator's assessment of the input image's authenticity.\n\"\"\"",
                    "source_code": "return self.model(img)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/srgan/srgan.py": [],
    "PyTorch-GAN/implementations/stargan/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the dataset by setting up image transformations, loading image paths, selecting specified attributes, and parsing annotation labels. This setup organizes and prepares image data for efficient batch loading and conditional processing in training or evaluation.\n\nArgs:\n    root (str): Path to the root directory containing image and annotation files.\n    attributes (list): List of attribute names to be used as labels.\n    transforms_ (list): List of torchvision transformations to apply to images.\n    mode (str): Dataset mode; 'train' for the training set or otherwise for testing/validation.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transform = transforms.Compose(transforms_)\n\n        self.selected_attrs = attributes\n        self.files = sorted(glob.glob(\"%s/*.jpg\" % root))\n        self.files = self.files[:-2000] if mode == \"train\" else self.files[-2000:]\n        self.label_path = glob.glob(\"%s/*.txt\" % root)[0]\n        self.annotations = self.get_annotations()"
                },
                {
                    "docstring": "\"\"\"Extracts annotations for CelebA\"\"\"",
                    "first_doc": "\"\"\"\nExtracts and processes annotation labels from the CelebA dataset.\n\nArgs:\n    self: The instance of the class containing label_path and selected_attrs.\n\nReturns:\n    dict: A dictionary mapping image filenames to their corresponding list of selected attribute annotations, where each attribute is represented as a binary value (1 if present, 0 otherwise).\n\"\"\"",
                    "method_name": "get_annotations",
                    "second_doc": "\"\"\"\nParses attribute labels for each image in the dataset, transforming the relevant annotation values into binary indicators for subsequent use.\n\nArgs:\n    self: Instance of the class, containing paths and the list of attributes to extract.\n\nReturns:\n    dict: Maps image filenames to lists of binary values indicating the presence or absence of selected attributes, enabling efficient filtering and conditioning during data loading and model training.\n\"\"\"",
                    "source_code": "annotations = {}\n        lines = [line.rstrip() for line in open(self.label_path, \"r\")]\n        self.label_names = lines[1].split()\n        for _, line in enumerate(lines[2:]):\n            filename, *values = line.split()\n            labels = []\n            for attr in self.selected_attrs:\n                idx = self.label_names.index(attr)\n                labels.append(1 * (values[idx] == \"1\"))\n            annotations[filename] = labels\n        return annotations"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves a transformed image and its corresponding label based on the provided index, enabling batched and efficient data access for model training and evaluation.\n\nArgs:\n    index (int): Index of the sample to retrieve.\n\nReturns:\n    tuple: A tuple containing the transformed image (as a tensor) and its associated label (as a FloatTensor).\n\"\"\"",
                    "source_code": "filepath = self.files[index % len(self.files)]\n        filename = filepath.split(\"/\")[-1]\n        img = self.transform(Image.open(filepath))\n        label = self.annotations[filename]\n        label = torch.FloatTensor(np.array(label))\n\n        return img, label"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the total number of image files available in the dataset.\n\nThis allows external components, such as data loaders, to determine how many examples can be accessed for training or evaluation, which is essential for batching and progress tracking.\n\nReturns:\n    int: The number of files contained in the dataset.\n\"\"\"",
                    "source_code": "return len(self.files)"
                }
            ],
            "name": "CelebADataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/stargan/models.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of convolutional layers in the given module using a normal distribution with mean 0.0 and standard deviation 0.02.\n\nThis weight initialization helps stabilize training dynamics by setting comparable starting points for convolutional kernels, which is particularly useful in deep networks.\n\nArgs:\n    m (nn.Module): The neural network module to apply the initialization to.\n\nReturns:\n    None\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers of a residual block with convolutional and normalization operations, enabling efficient feature learning and stable training in deep neural networks.\n\nArgs:\n    in_features (int): The number of input channels for the convolutional layers.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(ResidualBlock, self).__init__()\n\n        conv_block = [\n            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, in_features, 3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(in_features, affine=True, track_running_stats=True),\n        ]\n\n        self.conv_block = nn.Sequential(*conv_block)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies a residual connection by adding the output of a convolutional block to the input tensor. This helps maintain information from the input while enabling the model to learn complex transformations efficiently.\n\nArgs:\n    x (torch.Tensor): Input tensor to the residual block.\n\nReturns:\n    torch.Tensor: Output tensor after applying the residual connection.\n\"\"\"",
                    "source_code": "return x + self.conv_block(x)"
                }
            ],
            "name": "ResidualBlock",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the generator network architecture for the image transformation model, defining a series of convolutional, normalization, activation, residual, and upsampling layers to process input images conditioned on attribute information. This layer configuration is designed to enable effective learning of complex image mappings for flexible image modification and synthesis.\n\nArgs:\n    img_shape (tuple): The shape of the input image as (channels, height, width).\n    c_dim (int): The number of conditioning attributes to incorporate.\n    res_blocks (int): Number of residual blocks to include in the model.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(GeneratorResNet, self).__init__()\n        channels, img_size, _ = img_shape\n\n        # Initial convolution block\n        model = [\n            nn.Conv2d(channels + c_dim, 64, 7, stride=1, padding=3, bias=False),\n            nn.InstanceNorm2d(64, affine=True, track_running_stats=True),\n            nn.ReLU(inplace=True),\n        ]\n\n        # Downsampling\n        curr_dim = 64\n        for _ in range(2):\n            model += [\n                nn.Conv2d(curr_dim, curr_dim * 2, 4, stride=2, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim * 2, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            ]\n            curr_dim *= 2\n\n        # Residual blocks\n        for _ in range(res_blocks):\n            model += [ResidualBlock(curr_dim)]\n\n        # Upsampling\n        for _ in range(2):\n            model += [\n                nn.ConvTranspose2d(curr_dim, curr_dim // 2, 4, stride=2, padding=1, bias=False),\n                nn.InstanceNorm2d(curr_dim // 2, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            ]\n            curr_dim = curr_dim // 2\n\n        # Output layer\n        model += [nn.Conv2d(curr_dim, channels, 7, stride=1, padding=3), nn.Tanh()]\n\n        self.model = nn.Sequential(*model)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass through the generator by concatenating input data and condition vectors, spatially aligning the conditions to match the input dimensions before feeding the combined tensor to the core network. This enables the network to incorporate external information when producing output transformations.\n\nArgs:\n    x (torch.Tensor): Input tensor, typically representing an image or feature map.\n    c (torch.Tensor): Condition tensor, usually a vector encoding additional information.\n\nReturns:\n    torch.Tensor: The output tensor produced by the generator model after conditioning.\n\"\"\"",
                    "source_code": "c = c.view(c.size(0), c.size(1), 1, 1)\n        c = c.repeat(1, 1, x.size(2), x.size(3))\n        x = torch.cat((x, c), 1)\n        return self.model(x)"
                }
            ],
            "name": "GeneratorResNet",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the discriminator network architecture for processing images and assigning authenticity and class predictions at multiple scales. Constructs a series of downsampling convolutional blocks tailored to the image shape and number of strided layers, followed by separate output heads for adversarial discrimination and class prediction.  \nArgs:  \n    img_shape (tuple): Shape of the input images as (channels, height, width).  \n    c_dim (int): Number of classes for class prediction output.  \n    n_strided (int): Number of downsampling (strided convolution) layers to use.\n\nReturns:  \n    None\n\nWhy:  \nThis method establishes the core structure of a neural network that assesses input images, enabling separate outputs for real/fake discrimination (PatchGAN) and class labeling. This dual-headed design supports training objectives in tasks like image translation or conditional image generation, where distinguishing authenticity and recognizing categories are both required.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n        channels, img_size, _ = img_shape\n\n        def discriminator_block(in_filters, out_filters):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1), nn.LeakyReLU(0.01)]\n            return layers\n\n        layers = discriminator_block(channels, 64)\n        curr_dim = 64\n        for _ in range(n_strided - 1):\n            layers.extend(discriminator_block(curr_dim, curr_dim * 2))\n            curr_dim *= 2\n\n        self.model = nn.Sequential(*layers)\n\n        # Output 1: PatchGAN\n        self.out1 = nn.Conv2d(curr_dim, 1, 3, padding=1, bias=False)\n        # Output 2: Class prediction\n        kernel_size = img_size // 2 ** n_strided\n        self.out2 = nn.Conv2d(curr_dim, c_dim, kernel_size, bias=False)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nComputes the adversarial and classification outputs for a given input image by first extracting its high-level feature representation.\n\nArgs:\n    img (Tensor): Input image tensor for which discrimination and classification are to be performed.\n\nReturns:\n    Tuple[Tensor, Tensor]: Contains the adversarial prediction and the reshaped class prediction tensors.\n\nWhy:\n    This method allows the model to simultaneously determine if the input image is real or generated and to extract relevant class information, which is essential for learning meaningful representations and enhancing the stability and effectiveness of adversarial training.\n\"\"\"",
                    "source_code": "feature_repr = self.model(img)\n        out_adv = self.out1(feature_repr)\n        out_cls = self.out2(feature_repr)\n        return out_adv, out_cls.view(out_cls.size(0), -1)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/stargan/stargan.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "criterion_cls",
                "second_doc": "\"\"\"\nComputes the binary cross-entropy loss between predicted logits and target labels, averaged over the batch.\n\nArgs:\n    logit (Tensor): Predicted unnormalized scores (logits) from the model.\n    target (Tensor): Ground truth binary labels.\n\nReturns:\n    Tensor: Scalar loss value representing the average binary classification error across the batch.\n\nThis function measures how well the model's output distinguishes between two classes, which is essential for guiding optimization during adversarial training.\n\"\"\"",
                "source_code": "return F.binary_cross_entropy_with_logits(logit, target, size_average=False) / logit.size(0)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"",
                "first_doc": "\"\"\"\nComputes the gradient penalty for the Wasserstein GAN with Gradient Penalty (WGAN-GP).\n\nThis method interpolates between real and fake samples, computes the discriminator's output for the interpolated samples, and then determines the gradients with respect to those samples. The gradient penalty encourages the gradients of the discriminator to have unit norm, which is critical for WGAN-GP's training stability.\n\nArgs:\n    D: Discriminator function or model that maps input samples to outputs. Used to compute predictions on interpolated samples.\n    real_samples: Batch of real samples (e.g., images or data points) used for interpolation.\n    fake_samples: Batch of generated samples, typically produced by a generator.\n\nReturns:\n    The gradient penalty scalar value, used as a regularizer in WGAN-GP training.\n\"\"\"",
                "method_name": "compute_gradient_penalty",
                "second_doc": "\"\"\"\nCalculates a gradient penalty term by interpolating between real and generated data samples, then assessing how much the discriminator's gradients deviate from a unit norm. Penalizing these deviations helps ensure the discriminator provides meaningful feedback to the generator, preventing training instabilities caused by overly confident or unreliable gradients.\n\nArgs:\n    D: Callable neural network model acting as a discriminator, which outputs predictions for input samples.\n    real_samples: Tensor containing batches of authentic data instances (e.g., real images).\n    fake_samples: Tensor containing batches of generated or synthetic data instances.\n\nReturns:\n    Tensor scalar representing the gradient penalty, which is intended to regularize the discriminator's behavior during training.\n\"\"\"",
                "source_code": "# Random weight term for interpolation between real and fake samples\n    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n    # Get random interpolation between real and fake samples\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates, _ = D(interpolates)\n    fake = Variable(Tensor(np.ones(d_interpolates.shape)), requires_grad=False)\n    # Get gradient w.r.t. interpolates\n    gradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": "\"\"\"Saves a generated sample of domain translations\"\"\"",
                "first_doc": "\"\"\"\nGenerates and saves a grid of sample images from the validation data using the generator model.\n\nArgs:\n    batches_done: The current number of training batches processed, used for naming the saved image file.\n\nReturns:\n    None: This method saves a generated image grid to disk but does not return any value.\n\"\"\"",
                "method_name": "sample_images",
                "second_doc": "\"\"\"\nCreates and stores a visual grid showcasing how the generator model transforms validation images under varying label conditions, enabling qualitative assessment of model outputs as training progresses.\n\nArgs:\n    batches_done (int): The number of training batches processed so far; used to uniquely identify and save each generated image grid.\n\nReturns:\n    None: The function saves the resulting image grid to disk but does not return any value.\n\"\"\"",
                "source_code": "val_imgs, val_labels = next(iter(val_dataloader))\n    val_imgs = Variable(val_imgs.type(Tensor))\n    val_labels = Variable(val_labels.type(Tensor))\n    img_samples = None\n    for i in range(10):\n        img, label = val_imgs[i], val_labels[i]\n        # Repeat for number of label changes\n        imgs = img.repeat(c_dim, 1, 1, 1)\n        labels = label.repeat(c_dim, 1)\n        # Make changes to labels\n        for sample_i, changes in enumerate(label_changes):\n            for col, val in changes:\n                labels[sample_i, col] = 1 - labels[sample_i, col] if val == -1 else val\n\n        # Generate translations\n        gen_imgs = generator(imgs, labels)\n        # Concatenate images by width\n        gen_imgs = torch.cat([x for x in gen_imgs.data], -1)\n        img_sample = torch.cat((img.data, gen_imgs), -1)\n        # Add as row to generated samples\n        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n\n    save_image(img_samples.view(1, *img_samples.shape), \"images/%s.png\" % batches_done, normalize=True)"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/unit/datasets.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the ImageDataset by setting up image file paths for two different domains (A and B) and applying specified transformations. This setup is crucial for preparing paired or unpaired datasets needed for training models that learn relationships between two distinct sets of images.\n\nArgs:\n    root (str): Path to the root directory containing the images.\n    transforms_ (list): List of transformation functions to be applied to the images.\n    unaligned (bool): Flag indicating whether the dataset should be treated as unpaired (unaligned).\n    mode (str): Specifies which image set partition to use, e.g., 'train' or 'test'.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "self.transform = transforms.Compose(transforms_)\n        self.unaligned = unaligned\n\n        self.files_A = sorted(glob.glob(os.path.join(root, \"%s/A\" % mode) + \"/*.*\"))\n        self.files_B = sorted(glob.glob(os.path.join(root, \"%s/B\" % mode) + \"/*.*\"))"
                },
                {
                    "docstring": null,
                    "method_name": "__getitem__",
                    "second_doc": "\"\"\"\nRetrieves a transformed image pair from two datasets at the given index. Optionally selects the second image randomly from the dataset to provide unaligned image pairs. This enables flexible pairing strategies crucial for training models that learn relationships or mappings between two domains under both aligned and unaligned scenarios.\n\nArgs:\n    index (int): Index of the item to retrieve from the datasets.\n\nReturns:\n    dict: A dictionary containing the transformed images:\n        - 'A': Image from the first dataset, transformed.\n        - 'B': Image from the second dataset, either at the same index or randomly chosen, and transformed.\n\"\"\"",
                    "source_code": "item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n\n        if self.unaligned:\n            item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n        else:\n            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n\n        return {\"A\": item_A, \"B\": item_B}"
                },
                {
                    "docstring": null,
                    "method_name": "__len__",
                    "second_doc": "\"\"\"\nReturns the effective length of the dataset as the maximum number of items available in the two source image sets. This ensures that both domains are sufficiently represented when iterating through the dataset, even if the collections differ in size.\n\nReturns:\n    int: The greater length of the two image file lists, representing the dataset size.\n\"\"\"",
                    "source_code": "return max(len(self.files_A), len(self.files_B))"
                }
            ],
            "name": "ImageDataset",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/unit/models.py": [
        {
            "details": {
                "docstring": null,
                "method_name": "weights_init_normal",
                "second_doc": "\"\"\"\nInitializes the weights of network layers to promote stable convergence during training.\n\nArgs:\n    m (nn.Module): A neural network layer whose weights will be initialized. Typically used as a callback for model.apply().\n\nReturns:\n    None\n\nThis method sets the initial parameter values of convolutional and batch normalization layers to specific normal distributions. Such initialization helps prevent issues related to vanishing or exploding gradients, fostering consistent learning dynamics across different layers.\n\"\"\"",
                "source_code": "classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)"
            },
            "type": "function"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the learning rate scheduler parameters and checks that the decay phase begins before training ends.\n\nArgs:\n    n_epochs (int): Total number of epochs for training.\n    offset (int): Epoch to start counting from (used for resuming training).\n    decay_start_epoch (int): Epoch at which learning rate decay should start.\n\nReturns:\n    None\n\nWhy:\n    Ensures a proper schedule for adjusting the learning rate during training, which can stabilize and improve model convergence by gradually reducing the learning rate after a specified epoch.\n\"\"\"",
                    "source_code": "assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch"
                },
                {
                    "docstring": null,
                    "method_name": "step",
                    "second_doc": "\"\"\"\nCalculates a scaling factor for the learning rate based on the current training epoch, implementing a linear decay schedule after a specified starting point.\n\nArgs:\n    epoch (int): The current epoch number during training.\n\nReturns:\n    float: The computed learning rate multiplier for the current epoch.\n    \nThis method ensures stable training dynamics by progressively reducing the learning rate, which helps models converge more effectively and avoids overshooting in the later stages of training.\n\"\"\"",
                    "source_code": "return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"
                }
            ],
            "name": "LambdaLR",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the residual block by configuring a sequence of convolutional, normalization, and activation layers designed to refine feature representations while preserving essential information through residual connections.\n\nArgs:\n    features (int): The number of feature channels for the convolutional layers in the block.\n\nReturns:\n    None\n\nWhy:\n    This setup facilitates stable and efficient learning by enabling the model to pass information forward while also allowing non-linear transformations, supporting the construction of deeper neural networks without degradation in performance.\n\"\"\"",
                    "source_code": "super(ResidualBlock, self).__init__()\n\n        conv_block = [\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            nn.InstanceNorm2d(features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(features, features, 3),\n            nn.InstanceNorm2d(features),\n        ]\n\n        self.conv_block = nn.Sequential(*conv_block)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nApplies a residual connection by adding the input tensor to the output of a convolutional block, facilitating efficient information flow and stable training within deep neural networks.\n\nArgs:\n    x (Tensor): Input tensor to the residual block.\n\nReturns:\n    Tensor: Output tensor after applying the residual connection and convolutional block.\n\"\"\"",
                    "source_code": "return x + self.conv_block(x)"
                }
            ],
            "name": "ResidualBlock",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Encoder module by constructing a sequence of convolutional, normalization, activation, and residual layers designed to hierarchically extract and condense informative features from input data. This sequential structure enables effective representation learning for downstream generative and discriminative tasks.\n\nArgs:\n    in_channels (int): Number of channels in the input data.\n    dim (int): Number of output channels for the initial convolutional layer.\n    n_downsample (int): Number of times the spatial resolution will be halved through downsampling.\n    shared_block (nn.Module, optional): A shared network block to be integrated into the Encoder.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Encoder, self).__init__()\n\n        # Initial convolution block\n        layers = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(in_channels, dim, 7),\n            nn.InstanceNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n        ]\n\n        # Downsampling\n        for _ in range(n_downsample):\n            layers += [\n                nn.Conv2d(dim, dim * 2, 4, stride=2, padding=1),\n                nn.InstanceNorm2d(dim * 2),\n                nn.ReLU(inplace=True),\n            ]\n            dim *= 2\n\n        # Residual blocks\n        for _ in range(3):\n            layers += [ResidualBlock(dim)]\n\n        self.model_blocks = nn.Sequential(*layers)\n        self.shared_block = shared_block"
                },
                {
                    "docstring": null,
                    "method_name": "reparameterization",
                    "second_doc": "\"\"\"\nGenerates a latent variable by sampling random noise and shifting it by the provided mean tensor.\n\nThis operation introduces randomness into the latent space to enhance the diversity and generalization ability of the learned representations.\n\nArgs:\n    mu (torch.Tensor): Mean tensor that determines where the sampled noise will be centered.\n\nReturns:\n    torch.Tensor: Latent tensor obtained by shifting random noise by the mean.\n\"\"\"",
                    "source_code": "Tensor = torch.cuda.FloatTensor if mu.is_cuda else torch.FloatTensor\n        z = Variable(Tensor(np.random.normal(0, 1, mu.shape)))\n        return z + mu"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses the input tensor through the encoder network to produce a latent representation and its reparameterized version. This two-step output enables flexible sampling from the latent space, which is essential for generative modeling and learning expressive data representations.\n\nArgs:\n    x (torch.Tensor): Input tensor, typically representing data such as images.\n\nReturns:\n    Tuple[torch.Tensor, torch.Tensor]: The mean latent vector (mu) and the reparameterized latent vector (z).\n\"\"\"",
                    "source_code": "x = self.model_blocks(x)\n        mu = self.shared_block(x)\n        z = self.reparameterization(mu)\n        return mu, z"
                }
            ],
            "name": "Encoder",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the generator network by constructing a sequence of residual and upsampling layers to transform latent representations into output images.\n\nArgs:\n    shared_block (nn.Module): A module potentially used for parameter sharing across networks or conditional operations.\n    dim (int): The initial dimensionality of feature maps.\n    n_upsample (int): Number of upsampling layers to progressively increase spatial resolution.\n    out_channels (int): Number of channels in the output image.\n\nReturns:\n    None\n\nWhy:\n    This method assembles the core layers necessary for converting intermediate feature representations into full-resolution images, enabling the flexible generation and refinement of synthetic imagery from compact latent spaces.\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        self.shared_block = shared_block\n\n        layers = []\n        dim = dim * 2 ** n_upsample\n        # Residual blocks\n        for _ in range(3):\n            layers += [ResidualBlock(dim)]\n\n        # Upsampling\n        for _ in range(n_upsample):\n            layers += [\n                nn.ConvTranspose2d(dim, dim // 2, 4, stride=2, padding=1),\n                nn.InstanceNorm2d(dim // 2),\n                nn.LeakyReLU(0.2, inplace=True),\n            ]\n            dim = dim // 2\n\n        # Output layer\n        layers += [nn.ReflectionPad2d(3), nn.Conv2d(dim, out_channels, 7), nn.Tanh()]\n\n        self.model_blocks = nn.Sequential(*layers)"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses input data through the generator network by sequentially passing it through shared and model-specific computational blocks. This transforms the input into an output suitable for subsequent tasks such as image synthesis or enhancement.\n\nArgs:\n    x (torch.Tensor): Input tensor to the generator network.\n\nReturns:\n    torch.Tensor: Transformed tensor after processing through the generator's internal layers.\n\"\"\"",
                    "source_code": "x = self.shared_block(x)\n        x = self.model_blocks(x)\n        return x"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator network, which processes an input image and constructs a series of convolutional layers to assess the authenticity of image patches. By progressively downsampling and extracting features through sequential convolution, normalization, and activation layers, the network enables fine-grained evaluation of images at the patch level, which is essential for distinguishing real images from generated ones.\n\nArgs:\n    input_shape (tuple): A tuple (channels, height, width) representing the shape of the input images.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n        channels, height, width = input_shape\n        # Calculate output of image discriminator (PatchGAN)\n        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.Conv2d(512, 1, 3, padding=1)\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nPerforms a forward pass of an input image tensor through the discriminator network to compute its prediction.\n\nArgs:\n    img (torch.Tensor): Input image tensor, typically representing a real or generated image.\n\nReturns:\n    torch.Tensor: Output tensor containing the discriminator's prediction for the input image.\n    \nThis operation enables the model to assess and differentiate between real and generated samples as part of the adversarial learning process.\n\"\"\"",
                    "source_code": "return self.model(img)"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/unit/unit.py": [
        {
            "details": {
                "docstring": "\"\"\"Saves a generated sample from the test set\"\"\"",
                "first_doc": "\"\"\"\nGenerates and saves a sample grid of real and translated images for visual inspection during model training.\n\nArgs:\n    batches_done: The current number of batches processed, used to determine the naming of the output image file.\n\nReturns:\n    None. The method saves a concatenated sample image grid to disk; it does not return a value.\n\"\"\"",
                "method_name": "sample_images",
                "second_doc": "\"\"\"\nCreates and saves a visual comparison grid that displays original and model-generated images from the validation set at a given training stage. This enables monitoring of how well the model is learning to generate and translate images by providing qualitative feedback as training progresses.\n\nArgs:\n    batches_done (int): The current count of batches processed during training, used to name the output image file for tracking progress over time.\n\nReturns:\n    None: The function saves the assembled image grid to disk; it does not return any value.\n\"\"\"",
                "source_code": "imgs = next(iter(val_dataloader))\n    X1 = Variable(imgs[\"A\"].type(Tensor))\n    X2 = Variable(imgs[\"B\"].type(Tensor))\n    _, Z1 = E1(X1)\n    _, Z2 = E2(X2)\n    fake_X1 = G1(Z2)\n    fake_X2 = G2(Z1)\n    img_sample = torch.cat((X1.data, fake_X2.data, X2.data, fake_X1.data), 0)\n    save_image(img_sample, \"images/%s/%s.png\" % (opt.dataset_name, batches_done), nrow=5, normalize=True)"
            },
            "type": "function"
        },
        {
            "details": {
                "docstring": null,
                "method_name": "compute_kl",
                "second_doc": "\"\"\"\nCalculates the mean squared value of the input tensor, commonly used as a regularization term to encourage the values of mu to be close to zero. This can help stabilize training and prevent excessive deviation in learned parameters.\n\nArgs:\n    mu (torch.Tensor): Input tensor containing the parameters to be regularized.\n\nReturns:\n    torch.Tensor: Scalar tensor representing the mean squared value of the input.\n\"\"\"",
                "source_code": "mu_2 = torch.pow(mu, 2)\n    loss = torch.mean(mu_2)\n    return loss"
            },
            "type": "function"
        }
    ],
    "PyTorch-GAN/implementations/wgan/wgan.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator by constructing a neural network model composed of fully connected layers with optional normalization and non-linear activations, progressively expanding the latent vector into the desired output shape. This enables the transformation of low-dimensional random input into structured data suitable for further processing or image generation.\n\nArgs:\n    opt: An object containing configuration parameters, specifically `latent_dim` for the input dimension and `img_shape` for the shape of the generated data.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates an output image from the input latent vector by passing it through the generator model, then reshapes the result to match the expected image dimensions.\n\nArgs:\n    z (torch.Tensor): Input latent vector, typically sampled from a random distribution.\n\nReturns:\n    torch.Tensor: Generated image tensor with the appropriate shape for further processing or visualization.\n\nThis method reshapes the raw output of the model to ensure compatibility with downstream functions that expect image-like tensor structures.\n\"\"\"",
                    "source_code": "img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator network architecture by constructing a sequential model that maps input image data to a single output value. This layered structure enables the network to gradually extract and condense information from the input, assisting it in evaluating inputs for authenticity.\n\nArgs:\n    img_shape (tuple): The dimensions representing the shape of the input image to be processed by the network.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses an input image tensor by flattening it and passing it through the discriminator's neural network to produce an assessment of its authenticity. This transformation allows the model to evaluate whether an input image is likely to be real or generated.\n\nArgs:\n    img (torch.Tensor): Batch of input images of shape (batch_size, channels, height, width).\n\nReturns:\n    torch.Tensor: Discriminator's output representing the likelihood of each input image being real.\n\"\"\"",
                    "source_code": "img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/wgan_div/wgan_div.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator network architecture by building a series of fully connected layers with optional normalization and activation functions, culminating in a linear transformation and output activation suitable for generating image-shaped outputs.\n\nArgs:\n    opt: An object containing model options, specifically the latent_dim parameter that specifies the size of the input noise vector, and img_shape, which defines the dimensions of the output image.\n\nReturns:\n    None\n\nWhy:\n    This method sets up the network structure required to transform random noise vectors into data representations resembling real samples, enabling effective learning of complex data distributions for generative tasks.\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates an output image tensor from the given latent vector by passing it through the model and reshaping it to the expected image dimensions.\n\nArgs:\n    z (torch.Tensor): Input latent vector tensor representing noise samples.\n\nReturns:\n    torch.Tensor: Output image tensor with dimensions matching the specified image shape.\n\nThe method performs this operation to transform abstract latent representations into visually coherent images, enabling the exploration of how meaningful data can be synthesized from random input.\n\"\"\"",
                    "source_code": "img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Discriminator neural network with a multi-layer architecture consisting of linear and LeakyReLU activation layers, enabling it to process input images and output a scalar value representing the assessment of image authenticity. This structure is designed to effectively differentiate between genuine images and those generated by a counterpart network.\n\nArgs:\n    img_shape (tuple): The dimensions of the input image, used to determine the input size for the network.\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nEvaluates the input image using the discriminator's model to determine its authenticity.\n\nArgs:\n    img (torch.Tensor): Input image tensor.\n\nReturns:\n    torch.Tensor: Discriminator's prediction of the input image's validity.\n    \nThis method reshapes the input image and processes it through the model to assess its authenticity, enabling the network to distinguish between genuine and generated data as part of adversarial learning.\n\"\"\"",
                    "source_code": "img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        }
    ],
    "PyTorch-GAN/implementations/wgan_gp/wgan_gp.py": [
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the Generator architecture by constructing a sequence of fully connected layers with optional normalization and non-linear activations, progressively transforming the latent input into an image-shaped output with values in the range [-1, 1]. This structure enables the model to learn how to map random noise vectors to realistic data-like samples.\n\nArgs:\n    opt: Namespace containing configuration options, specifically the dimensionality of the latent input (opt.latent_dim) and the target image shape (img_shape).\n\nReturns:\n    None\n\"\"\"",
                    "source_code": "super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nGenerates and reshapes images from input latent vectors using the generator model.\n\nArgs:\n    z (torch.Tensor): A batch of latent vectors with shape (batch_size, latent_dim).\n\nReturns:\n    torch.Tensor: A batch of generated images reshaped to (batch_size, *img_shape).\n\nWhy:\n    The method transforms input latent representations into image data, which is essential for learning how to synthesize realistic images from random noise distributions.\n\"\"\"",
                    "source_code": "img = self.model(z)\n        img = img.view(img.shape[0], *img_shape)\n        return img"
                }
            ],
            "name": "Generator",
            "type": "class"
        },
        {
            "methods": [
                {
                    "docstring": null,
                    "method_name": "__init__",
                    "second_doc": "\"\"\"\nInitializes the layers of the Discriminator network, building a multi-layer perceptron to distinguish input data patterns.\n\nArgs:\n    img_shape (tuple): The shape of the input image to be flattened and fed into the network.\n\nReturns:\n    None\n\nWhy:\n    This setup enables the network to effectively process and evaluate input images, learning to assign confidence scores that help differentiate between real and synthetic samples.\n\"\"\"",
                    "source_code": "super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n        )"
                },
                {
                    "docstring": null,
                    "method_name": "forward",
                    "second_doc": "\"\"\"\nProcesses an input image tensor by flattening it and passing it through the discriminator network to assess its authenticity. This helps the model learn to distinguish between real and generated images.\n\nArgs:\n    img (torch.Tensor): Input image tensor of shape (batch_size, channels, height, width).\n\nReturns:\n    torch.Tensor: A tensor containing the discriminator's assessment (validity score) for each input image in the batch.\n\"\"\"",
                    "source_code": "img_flat = img.view(img.shape[0], -1)\n        validity = self.model(img_flat)\n        return validity"
                }
            ],
            "name": "Discriminator",
            "type": "class"
        },
        {
            "details": {
                "docstring": "\"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"",
                "first_doc": "\"\"\"\nCalculates the gradient penalty used for stabilizing training in Wasserstein GANs with gradient penalty.\n\nThe method interpolates between real and fake samples, computes the discriminator output on the interpolated samples, calculates the gradients with respect to the interpolated data, and returns the mean squared deviation from unit norm as the gradient penalty.\n\nArgs:\n    D: The discriminator model to evaluate and compute gradients from.\n    real_samples: A batch of real data samples.\n    fake_samples: A batch of generated (fake) data samples.\n\nReturns:\n    A scalar tensor representing the gradient penalty regularization term.\n\"\"\"",
                "method_name": "compute_gradient_penalty",
                "second_doc": "\"\"\"\nComputes a gradient penalty term to encourage smoothness and stability during adversarial training.\n\nBy interpolating between actual and generated data, this method quantifies how much the discriminator's output varies with respect to its input, promoting more reliable training dynamics and preventing excessive growth of gradients.\n\nArgs:\n    D (torch.nn.Module): Discriminator model to evaluate gradients.\n    real_samples (torch.Tensor): Batch of real data samples.\n    fake_samples (torch.Tensor): Batch of generated (fake) data samples.\n\nReturns:\n    torch.Tensor: Scalar representing the gradient penalty regularization term, used to control the magnitude of discriminator gradients.\n\"\"\"",
                "source_code": "# Random weight term for interpolation between real and fake samples\n    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n    # Get random interpolation between real and fake samples\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = D(interpolates)\n    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n    # Get gradient w.r.t. interpolates\n    gradients = autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty"
            },
            "type": "function"
        }
    ]
}